<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Mcs.techLlm-(McsTchInf000038.0-5-0.2023-09-07 draft) large-language-model</title>
  <meta name="keywords" content="large-language-model, techLlm, ModelConceptSensorial, McsHitp, Synagonism">
  <link rel="stylesheet" href="../Mcsmgr/mHitp.css">
</head>

<body>
<header id="idHeader">
  <p></p>
  <h1 id="idHeaderH1">large-language-model
    <br>senso-concept-Mcs (techLlm)
    </h1>
  <p id="idHeadercrd">McsHitp-creation:: {2023-08-12}
    <a class="clsHide" href="#idHeadercrd"></a></p>
</header>

<section id="idOverview">
  <h1 id="idOverviewH1">overview of techLlm
    <a class="clsHide" href="#idOverviewH1"></a></h1>
  <p id="idDescription">description::
    <br>"A large language model (LLM) is a language model consisting of a neural network with many parameters (typically billions of weights or more), trained on large quantities of unlabelled text using self-supervised learning. LLMs emerged around 2018 and perform well at a wide variety of tasks. This has shifted the focus of natural language processing research away from the previous paradigm of training specialized supervised models for specific tasks.[1]
    <br>Properties
    <br>Though the term large language model has no formal definition, it often refers to deep learning models having a parameter count on the order of billions or more.[2] LLMs are general purpose models which excel at a wide range of tasks, as opposed to being trained for one specific task (such as sentiment analysis, named entity recognition, or mathematical reasoning).[1][3] The skill with which they accomplish tasks, and the range of tasks at which they are capable, seems to be a function of the amount of resources (data, parameter-size, computing power) devoted to them, in a way that is not dependent on additional breakthroughs in design.[4]
    <br>Though trained on simple tasks along the lines of predicting the next word in a sentence, neural language models with sufficient training and parameter counts are found to capture much of the syntax and semantics of human language. In addition, large language models demonstrate are able to "memorize" a great quantity of facts during training.[1] The ability of LLMs to often produce factually accurate responses can create the impression that they have general knowledge about the world; the production of responses with factual content that does not seem to be justified by the model's training data is referred to as a "hallucination".[5]"
    <br>[{2023-04-09 retrieved} https://en.wikipedia.org/wiki/Large_language_model]
    <a class="clsHide" href="#idDescription"></a></p>
  <p id="idName">name::
    <br>* McsEngl.McsTchInf000038.last.html//dirTchInf//dirMcs!⇒techLlm,
    <br>* McsEngl.dirTchInf/McsTchInf000038.last.html!⇒techLlm,
    <br>* McsEngl.LLM!=large-language-model!⇒techLlm,
    <br>* McsEngl.Llmodel!⇒techLlm,
    <br>* McsEngl.large-language-model!⇒techLlm,
    <br>* McsEngl.techDl.large-language-model!⇒techLlm,
    <br>* McsEngl.techNlm.techLlm!⇒techLlm,
    <br>* McsEngl.techNlplm.large-language-model!⇒techLlm,
    <br>* McsEngl.techLlm,
    <br>* McsEngl.techLlm!=large-language-model--tech!⇒techLlm,
    <br>* McsEngl.techLlm:techDl,
    <br>* McsEngl.techLlm:techNlm,
    <br>====== langoGreek:
    <br>* McsElln.μεγάλο-γλωσσικό-μοντέλο!το!=techLlm,
    <a class="clsHide" href="#idName"></a></p>
</section>

<section id="idTchLlmaclr">
  <h1 id="idTchLlmaclrH1">accelerator of techLlm
    <a class="clsHide" href="#idTchLlmaclrH1"></a></h1>
  <p id="idTchLlmaclrdsn">description::
    <br>· "Their size is enabled by <a class="clsPreview" href="../dirTchInf/McsTchInf000036.last.html#idTechAiaclr">AI accelerators</a>, which are able to process vast amounts of text data, mostly scraped from the Internet.[1] "
    <br>[{2023-08-12 retrieved} https://en.wikipedia.org/wiki/Large_language_model]
    <a class="clsHide" href="#idTchLlmaclrdsn"></a></p>
  <p id="idTchLlmaclrnam">name::
    <br>* McsEngl.techLlm'accelerator,
    <a class="clsHide" href="#idTchLlmaclrnam"></a></p>
</section>

<section id="idTchLlmapi">
  <h1 id="idTchLlmapiH1">API of techLlm
    <a class="clsHide" href="#idTchLlmapiH1"></a></h1>
  <p id="idTchLlmapidsn">description::
    <br>·
    <a class="clsHide" href="#idTchLlmapidsn"></a></p>
  <p id="idTchLlmapinam">name::
    <br>* McsEngl.techLlm'API,
    <a class="clsHide" href="#idTchLlmapinam"></a></p>
</section>

<section id="idTchLlmarcr">
  <h1 id="idTchLlmarcrH1">architecture of techLlm
    <a class="clsHide" href="#idTchLlmarcrH1"></a></h1>
  <p id="idTchLlmarcrdsn">description::
    <br>* <a class="clsPreview" href="../dirTchInf/McsTchInf000036.last.html#idTchLlmTrfm">transformer</a>,
    <br>
    <br>"Transformer architecture contributed to faster training.[2] Alternative architectures include the mixture of experts (MoE), which has been proposed by Google, starting with sparsely-gated ones in 2017,[3] Gshard in 2021[4] to GLaM in 2022.[5]"
    <br>[{2023-08-12 retrieved} https://en.wikipedia.org/wiki/Large_language_model]
    <a class="clsHide" href="#idTchLlmarcrdsn"></a></p>
  <p id="idTchLlmarcrnam">name::
    <br>* McsEngl.techLlm'architecture,
    <a class="clsHide" href="#idTchLlmarcrnam"></a></p>
</section>

<section id="idTchLlmprmr">
  <h1 id="idTchLlmprmrH1">parameter of techLlm
    <a class="clsHide" href="#idTchLlmprmrH1"></a></h1>
  <p id="idTchLlmprmrdsn">description::
    <br>* WuDao: 1.75 trillion parameters,
    <br>* PaLM: 540 billion parameters,
    <br>* GPT-3: 175 billion parameters,
    <br>* Llama-2: 7, 13, 70 billion parameters,
    <a class="clsHide" href="#idTchLlmprmrdsn"></a></p>
  <p id="idTchLlmprmrnam">name::
    <br>* McsEngl.techLlm'parameter,
    <a class="clsHide" href="#idTchLlmprmrnam"></a></p>
</section>

<section id="idTchLlmdtst">
  <h1 id="idTchLlmdtstH1">dataset of techLlm
    <a class="clsHide" href="#idTchLlmdtstH1"></a></h1>
  <p id="idTchLlmdtstdsn">description::
    <br>· "Bard is trained on a dataset of 1.56 trillion words, and has 137 billion parameters. The dataset is a combination of text and code, and is drawn from a variety of sources, including the web, books, and user-generated content."
    <br>[{2023-08-08 retrieved} https://bard.google.com/]
    <br>
    <br>· "PaLM 2 was trained on a dataset of text and code that was more than 540 billion words, which is more than 10 times the amount of data that was used to train GPT-3."
    <br>[{2023-08-08 retrieved} https://poe.com/Google-PaLM]
    <br>
    <br>· GPT-3 was trained on a dataset with a size of hundreds of terabytes.
    <a class="clsHide" href="#idTchLlmdtstdsn"></a></p>
  <p id="idTchLlmdtstnam">name::
    <br>* McsEngl.techLlm'dataset,
    <a class="clsHide" href="#idTchLlmdtstnam"></a></p>
</section>

<section id="idTchLlminpt">
  <h1 id="idTchLlminptH1">input of techLlm
    <a class="clsHide" href="#idTchLlminptH1"></a></h1>
  <p id="idTchLlminptdsn">description::
    <br>· "Models have limitations on the number of tokens (words, characters, spaces) they can handle. It’s 4000 tokens for GPT-3, 8000 for GPT-4, and 100k for Claude 2. Tailoring your input to these constraints will yield better results."
    <br>[{2023-08-08 retrieved} https://blog.finxter.com/alien-technology-catching-up-on-llms-prompting-chatgpt-plugins-embeddings-code-interpreter/]
    <a class="clsHide" href="#idTchLlminptdsn"></a></p>
  <p id="idTchLlminptnam">name::
    <br>* McsEngl.techLlm'input,
    <a class="clsHide" href="#idTchLlminptnam"></a></p>
</section>

<section id="idTchLlmrscF">
  <h1 id="idTchLlmrscFH1">info-resource of techLlm
    <a class="clsHide" href="#idTchLlmrscFH1"></a></h1>
  <p id="idTchLlmrscwpa">addressWpg::
    <br>* {2017-06-12} Attention Is All You Need: https://arxiv.org/abs/1706.03762,
    <a class="clsHide" href="#idTchLlmrscwpa"></a></p>
  <p id="idTchLlmrscnam">name::
    <br>* McsEngl.techLlm'Infrsc,
    <a class="clsHide" href="#idTchLlmrscnam"></a></p>
</section>

<section id="idTchLlmdngF">
  <h1 id="idTchLlmdngFH1">DOING of techLlm
    <a class="clsHide" href="#idTchLlmdngFH1"></a></h1>
  <p id="idTchLlmdngdsn">description::
    <br>* "As language models, they work by taking an input text and repeatedly predicting the next token or word.[6]"
    <br>[{2023-08-12 retrieved} https://en.wikipedia.org/wiki/Large_language_model]
    <a class="clsHide" href="#idTchLlmdngdsn"></a></p>
  <p id="idTchLlmdngnam">name::
    <br>* McsEngl.techLlm'doing,
    <a class="clsHide" href="#idTchLlmdngnam"></a></p>

  <section id="idTchLlmprtr">
  <h2 id="idTchLlmprtrH2">pretraining of techLlm
    <a class="clsHide" href="#idTchLlmprtrH2"></a></h2>
  <p id="idTchLlmprtrdsn">description::
    <br>· "Pretraining is the act of training a model from scratch: the weights are randomly initialized, and the training starts without any prior knowledge.
    <br>This pretraining is usually done on very large amounts of data. Therefore, it requires a very large corpus of data, and training can take up to several weeks."
    <br>[{2023-08-13 retrieved} https://huggingface.co/learn/nlp-course/chapter1/]
    <a class="clsHide" href="#idTchLlmprtrdsn"></a></p>
  <p id="idTchLlmprtrnam">name::
    <br>* McsEngl.techLlm'pretraining,
    <a class="clsHide" href="#idTchLlmprtrnam"></a></p>
  </section>

  <section id="idTchLlmfntn">
  <h2 id="idTchLlmfntnH2">fine-tuning of techLlm
    <a class="clsHide" href="#idTchLlmfntnH2"></a></h2>
  <p id="idTchLlmfntndsn">description::
    <br>· "In deep learning, fine-tuning is an approach to transfer learning in which the weights of a pre-trained model are trained on new data.[1] Fine-tuning can be done on the entire neural network, or on only a subset of its layers, in which case the layers that are not being fine-tuned are "frozen" (not updated during the backpropagation step).[2] A model may also be augmented with "adapters" that consist of far fewer parameters than the original model, and fine-tuned in a parameter-efficient way by tuning the weights of the adapters and leaving the rest of the model's weights frozen.[3]
    <br>For some architectures, such as convolutional neural networks, it is common to keep the earlier layers (those closest to the input layer) frozen because they capture lower-level features, while later layers often discern high-level features that can be more related to the task that the model is trained on.[2][4]
    <br>Models that are pre-trained on large and general corpora are usually fine-tuned by reusing the model's parameters as a starting point and adding a task-specific layer trained from scratch.[5] Fine-tuning the full model is common as well and often yields better results, but it is more computationally expensive.[6]
    <br>Fine-tuning is typically accomplished with supervised learning, but there are also techniques to fine-tune a model using weak supervision.[7] Fine-tuning can be combined with a reinforcement learning from human feedback-based objective to produce language models like ChatGPT (a fine-tuned version of GPT-3) and Sparrow.[8][9]"
    <br>[{2023-08-13 retrieved} https://en.wikipedia.org/wiki/Fine-tuning_(deep_learning)]
    <a class="clsHide" href="#idTchLlmfntndsn"></a></p>
  <p id="idTchLlmfntnnam">name::
    <br>* McsEngl.fine-tuning--techLlm,
    <br>* McsEngl.techLlm'fine-tuning,
    <a class="clsHide" href="#idTchLlmfntnnam"></a></p>
  </section>
</section>

<section id="idTchLlmevgF">
  <h1 id="idTchLlmevgFH1">evoluting of techLlm
    <a class="clsHide" href="#idTchLlmevgFH1"></a></h1>
  <p id="idTchLlmevgnam">name::
    <br>* McsEngl.techLlm'evoluting,
    <a class="clsHide" href="#idTchLlmevgnam"></a></p>
  <p id="idTchLlmevg20230812">{2023-08-12}::
    <br>=== McsHitp-creation:
    <br>· creation of current <a class="clsPreview" href="../dirTchInf/McsTchInf000009.last.html#idMcsHitpFil">webpage-concept</a>.
    <a class="clsHide" href="#idTchLlmevg20230812"></a></p>
</section>

<section id="idTchLlmmisc">
  <h1 id="idTchLlmmiscH1">MISC-ATTRIBUTE of techLlm
    <a class="clsHide" href="#idTchLlmmiscH1"></a></h1>
  <p id="idTchLlmmiscdsn">description::
    <br>· "Eight Things to Know about Large Language Models
    <br>Samuel R. Bowman
    <br>The widespread public deployment of large language models (LLMs) in recent months has prompted a wave of new attention and engagement from advocates, policymakers, and scholars from many fields. This attention is a timely response to the many urgent questions that this technology raises, but it can sometimes miss important considerations. This paper surveys the evidence for eight potentially surprising such points:
    <br>1. LLMs predictably get more capable with increasing investment, even without targeted innovation.
    <br>2. Many important LLM behaviors emerge unpredictably as a byproduct of increasing investment.
    <br>3. LLMs often appear to learn and use representations of the outside world.
    <br>4. There are no reliable techniques for steering the behavior of LLMs.
    <br>5. Experts are not yet able to interpret the inner workings of LLMs.
    <br>6. Human performance on a task isn't an upper bound on LLM performance.
    <br>7. LLMs need not express the values of their creators nor the values encoded in web text.
    <br>8. Brief interactions with LLMs are often misleading."
    <br>[{2023-08-12 retrieved} https://arxiv.org/abs/2304.00612]
    <a class="clsHide" href="#idTchLlmmiscdsn"></a></p>
  <p id="idTchLlmmiscnam">name::
    <br>* McsEngl.techLlm'misc,
    <a class="clsHide" href="#idTchLlmmiscnam"></a></p>
</section>

<section id="idTchLlmwptF">
  <h1 id="idTchLlmwptFH1">WHOLE-PART-TREE of techLlm
    <a class="clsHide" href="#idTchLlmwptFH1"></a></h1>
  <p id="idTchLlmwptnam">name::
    <br>* McsEngl.techLlm'part-whole-tree,
    <br>* McsEngl.techLlm'whole-part-tree,
    <a class="clsHide" href="#idTchLlmwptnam"></a></p>
  <p id="idTchLlmwtr">whole-tree-of-techLlm::
    <br>*
    <br>* ... <a class="clsPreview" href="../dirCor/McsCor000003.last.html#idEntwtr">Sympan</a>.
    <a class="clsHide" href="#idTchLlmwtr"></a></p>
  <p id="idTchLlmptr">part-tree-of-techLlm::
    <br>*
    <a class="clsHide" href="#idTchLlmptr"></a></p>
</section>

<section id="idTchLlmgstF">
  <h1 id="idTchLlmgstFH1">GENERIC-SPECIFIC-TREE of techLlm
    <a class="clsHide" href="#idTchLlmgstFH1"></a></h1>
  <p id="idTchLlmgstnam">name::
    <br>* McsEngl.techLlm'generic-specific-tree,
    <br>* McsEngl.techLlm'specific-generic-tree,
    <a class="clsHide" href="#idTchLlmgstnam"></a></p>
  <p id="idTchLlmgtr">generic-tree-of-techLlm::
    <br>* <a class="clsPreview" href="../dirTchInf/McsTchInf000036.last.html#idTchNlp001lgml">language-model</a>,
    <br>* ... <a class="clsPreview" href="../dirCor/McsCor000003.last.html#idOverview">entity</a>.
    <a class="clsHide" href="#idTchLlmgtr"></a></p>
  <p id="idTchLlmstr">specific-tree-of-techLlm::
    <br>* neural-language-model,
    <br>* deep-learning,
    <br>* generic-Llm,
    <br>* instruction-tuned-Llm,
    <br>* dialog-tuned-Llm,
    <br>===
    <br>* LLaMa-2 (Meta),
    <br>* PALM-2 (Bard-Google),
    <a class="clsHide" href="#idTchLlmstr"></a></p>
</section>

<section id="idTchLlmFdtn">
  <h1 id="idTchLlmFdtnH1">techLlm.foundation-model
    <a class="clsHide" href="#idTchLlmFdtnH1"></a></h1>
  <p id="idTchLlmFdtndsn">description::
    <br>·
    <a class="clsHide" href="#idTchLlmFdtndsn"></a></p>
  <p id="idTchLlmFdtnnam">name::
    <br>* McsEngl.foundation-Llmodel,
    <br>* McsEngl.techLlm.foundation-model,
    <a class="clsHide" href="#idTchLlmFdtnnam"></a></p>
</section>

<section id="idTchLlmFntd">
  <h1 id="idTchLlmFntdH1">techLlm.fine-tuned--model
    <a class="clsHide" href="#idTchLlmFntdH1"></a></h1>
  <p id="idTchLlmFntddsn">description::
    <br>·
    <a class="clsHide" href="#idTchLlmFntddsn"></a></p>
  <p id="idTchLlmFntdnam">name::
    <br>* McsEngl.fine-tuned--Llmodel,
    <br>* McsEngl.techLlm.fine-tuned--model,
    <a class="clsHide" href="#idTchLlmFntdnam"></a></p>
</section>

<section id="idTchLlmTrfm">
  <h1 id="idTchLlmTrfmH1">techLlm.transformer
    <a class="clsHide" href="#idTchLlmTrfmH1"></a></h1>
  <p id="idTchLlmTrfmdsn">description::
    <br>"A transformer is a deep learning model that adopts the mechanism of self-attention, differentially weighting the significance of each part of the input data. It is used primarily in the fields of natural language processing (NLP)[1] and computer vision (CV).[2]
    <br>Like recurrent neural networks (RNNs), transformers are designed to process sequential input data, such as natural language, with applications towards tasks such as translation and text summarization. However, unlike RNNs, transformers process the entire input all at once. The attention mechanism provides context for any position in the input sequence. For example, if the input data is a natural language sentence, the transformer does not have to process one word at a time. This allows for more parallelization than RNNs and therefore reduces training times.[1]
    <br>Transformers were introduced in 2017 by a team at Google Brain[1] and are increasingly becoming the model of choice for NLP problems,[3] replacing RNN models such as long short-term memory (LSTM). The additional training parallelization allows training on larger datasets. This led to the development of pretrained systems such as BERT (Bidirectional Encoder Representations from Transformers) and GPT (Generative Pre-trained Transformer), which were trained with large language datasets, such as the Wikipedia Corpus and Common Crawl, and can be fine-tuned for specific tasks."
    <br>[{2023-04-01 retrieved} https://en.wikipedia.org/wiki/Transformer_(machine_learning_model)]
    <a class="clsHide" href="#idTchLlmTrfmdsn"></a></p>
  <p id="idTchLlmTrfmnam">name::
    <br>* McsEngl.llmTransformer,
    <br>* McsEngl.techLlm.transformer!⇒llmTransformer,
    <br>* McsEngl.techDl.transformer!⇒llmTransformer,
    <br>* McsEngl.techInfo.011-transformer!⇒llmTransformer,
    <br>* McsEngl.techInfo.transformer!⇒llmTransformer,
    <br>* McsEngl.techNn.transformer!⇒llmTransformer,
    <br>* McsEngl.techNnT!⇒llmTransformer,
    <br>* McsEngl.transformer!⇒llmTransformer,
    <a class="clsHide" href="#idTchLlmTrfmnam"></a></p>

  <section id="idTchLlmTrfmatly">
  <h2 id="idTchLlmTrfmatlyH2">attention-layer of llmTransformer
    <a class="clsHide" href="#idTchLlmTrfmatlyH2"></a></h2>
  <p id="idTchLlmTrfmatlydsn">description::
    <br>· "A key feature of Transformer models is that they are built with special layers called attention layers. In fact, the title of the paper introducing the Transformer architecture was “Attention Is All You Need”! We will explore the details of attention layers later in the course; for now, all you need to know is that this layer will tell the model to pay specific attention to certain words in the sentence you passed it (and more or less ignore the others) when dealing with the representation of each word."
    <br>[{2023-08-14 retrieved} https://huggingface.co/learn/nlp-course/chapter1/]
    <a class="clsHide" href="#idTchLlmTrfmatlydsn"></a></p>
  <p id="idTchLlmTrfmatlynam">name::
    <br>* McsEngl.llmTransformer'attention-layer,
    <a class="clsHide" href="#idTchLlmTrfmatlynam"></a></p>
  </section>

  <section id="idTchLlmTrfmarktr">
  <h2 id="idTchLlmTrfmarktrH2">architecture of llmTransformer
    <a class="clsHide" href="#idTchLlmTrfmarktrH2"></a></h2>
  <p id="idTchLlmTrfmarktrdsn">description::
    <br>· "Architecture: This is the skeleton of the model — the definition of each layer and each operation that happens within the model."
    <br>[{2023-08-14 retrieved} https://huggingface.co/learn/nlp-course/chapter1/4?fw=pt#architecture-vs-checkpoints]
    <a class="clsHide" href="#idTchLlmTrfmarktrdsn"></a></p>
  <p id="idTchLlmTrfmarktrnam">name::
    <br>* McsEngl.architecture-of-llmTransformer,
    <br>* McsEngl.llmTransformer'architecture,
    <a class="clsHide" href="#idTchLlmTrfmarktrnam"></a></p>
  </section>

  <section id="idTchLlmTrfmckpt">
  <h2 id="idTchLlmTrfmckptH2">checkpoint of llmTransformer
    <a class="clsHide" href="#idTchLlmTrfmckptH2"></a></h2>
  <p id="idTchLlmTrfmckptdsn">description::
    <br>· "Checkpoints: These are the weights that will be loaded in a given architecture."
    <br>[{2023-08-14 retrieved} https://huggingface.co/learn/nlp-course/chapter1/4?fw=pt#architecture-vs-checkpoints]
    <a class="clsHide" href="#idTchLlmTrfmckptdsn"></a></p>
  <p id="idTchLlmTrfmckptnam">name::
    <br>* McsEngl.checkpoint-of-llmTransformer,
    <br>* McsEngl.llmTransformer'checkpoint,
    <a class="clsHide" href="#idTchLlmTrfmckptnam"></a></p>
  </section>

  <section id="idTchLlmTrfmeval">
  <h2 id="idTchLlmTrfmevalH2">evaluation of llmTransformer
    <a class="clsHide" href="#idTchLlmTrfmevalH2"></a></h2>

  <section id="idTchLlmTrfmcrtm">
  <h3 id="idTchLlmTrfmcrtmH3">criticism of llmTransformer
    <a class="clsHide" href="#idTchLlmTrfmcrtmH3"></a></h3>
  <p id="idTchLlmTrfmcrtmdsn">description::
    <br>"While Transformer models have been very successful in natural language processing tasks, there are some criticisms of the architecture and its use.
    <br>* Over-reliance on large amounts of data: Transformer models require large amounts of data for training, which can be difficult to obtain for some languages or domains. This can lead to biases in the model and limit its generalization ability.
    <br>* Interpretability: Transformer models can be difficult to interpret, which can make it challenging to understand how they arrive at their predictions. This is particularly important in applications where it is necessary to understand the reasoning behind the model's decisions.
    <br>* High computational requirements: Training and using Transformer models can be computationally expensive, which can limit their accessibility and use in resource-constrained environments.
    <br>* Lack of long-term understanding: While Transformer models are very good at understanding short-term relationships between words, they can struggle with long-term dependencies. This can make it difficult to generate coherent and meaningful text over longer sequences.
    <br>* Fairness and bias: Transformer models can inherit biases from the training data, which can lead to unfair and discriminatory outcomes in some applications. This is a particularly important concern in applications such as hiring, lending, and criminal justice.
    <br>* Carbon footprint: Training large Transformer models can consume a significant amount of energy, which can contribute to greenhouse gas emissions and exacerbate climate change.
    <br>These criticisms highlight some of the challenges and limitations of Transformer models and the need to carefully consider their use in different applications."
    <br>[{2023-04-10 retrieved} https://chat.openai.com/chat]
    <a class="clsHide" href="#idTchLlmTrfmcrtmdsn"></a></p>
  <p id="idTchLlmTrfmcrtmnam">name::
    <br>* McsEngl.llmTransformer'criticism,
    <a class="clsHide" href="#idTchLlmTrfmcrtmnam"></a></p>
  </section>
  </section>

  <section id="idTchLlmTrfmdng">
  <h2 id="idTchLlmTrfmdngH2">DOING of llmTransformer
    <a class="clsHide" href="#idTchLlmTrfmdngH2"></a></h2>
  <p id="idTchLlmTrfmdngdsn">description::
    <br>· "The Transformer architecture was originally designed for translation."
    <br>[{2023-08-14 retrieved} https://huggingface.co/learn/nlp-course/chapter1/4?fw=pt#the-original-architecture]
    <a class="clsHide" href="#idTchLlmTrfmdngdsn"></a></p>
  <p id="idTchLlmTrfmdngnam">name::
    <br>* McsEngl.llmTransformer'doing,
    <a class="clsHide" href="#idTchLlmTrfmdngnam"></a></p>
  </section>

  <section id="idTchLlmTrfmSpc">
  <h2 id="idTchLlmTrfmSpcH2">llmTransformer.SPECIFIC
    <a class="clsHide" href="#idTchLlmTrfmSpcH2"></a></h2>
  <p id="idTchLlmTrfmSpcdsn">description::
    <br>* GPT-like also called auto-regressive Transformer models,
    <br>* BERT-like also called auto-encoding Transformer models,
    <br>* BART/T5-like also called sequence-to-sequence Transformer models,
    <a class="clsHide" href="#idTchLlmTrfmSpcdsn"></a></p>
  <p id="idTchLlmTrfmSpcnam">name::
    <br>* McsEngl.llmTransformer.specific,
    <a class="clsHide" href="#idTchLlmTrfmSpcnam"></a></p>
  </section>

  <section id="idTchLlmTrfmGptlk">
  <h2 id="idTchLlmTrfmGptlkH2">llmTransformer.GPT-like
    <a class="clsHide" href="#idTchLlmTrfmGptlkH2"></a></h2>
  <p id="idTchLlmTrfmGptlkdsn">description::
    <br>· "GPT-like (also called auto-regressive Transformer models)"
    <br>[{2023-08-13 retrieved} https://huggingface.co/learn/nlp-course/chapter1/4?fw=pt]
    <br>
    <br>· "Decoder models use only the decoder of a Transformer model. At each stage, for a given word the attention layers can only access the words positioned before it in the sentence. These models are often called auto-regressive models.
    <br>The pretraining of decoder models usually revolves around predicting the next word in the sentence.
    <br>These models are best suited for tasks involving text generation.
    <br>Representatives of this family of models include:
    <br>* CTRL
    <br>* GPT
    <br>* GPT-2
    <br>* Transformer XL"
    <br>[{2023-08-14 retrieved} https://huggingface.co/learn/nlp-course/chapter1/6?fw=pt]
    <a class="clsHide" href="#idTchLlmTrfmGptlkdsn"></a></p>
  <p id="idTchLlmTrfmGptlknam">name::
    <br>* McsEngl.GPT-like--llmTransformer,
    <br>* McsEngl.auto-regressive--llmTransformer,
    <br>* McsEngl.decoder-model--llmTransformer,
    <br>* McsEngl.llmTransformer.GPT-like,
    <a class="clsHide" href="#idTchLlmTrfmGptlknam"></a></p>
  </section>

  <section id="idTchLlmTrfmBertlk">
  <h2 id="idTchLlmTrfmBertlkH2">llmTransformer.BERT-like
    <a class="clsHide" href="#idTchLlmTrfmBertlkH2"></a></h2>
  <p id="idTchLlmTrfmBertlkdsn">description::
    <br>· "BERT-like (also called auto-encoding Transformer models)"
    <br>[{2023-08-13 retrieved} https://huggingface.co/learn/nlp-course/chapter1/4?fw=pt]
    <br>
    <br>· "Encoder models use only the encoder of a Transformer model. At each stage, the attention layers can access all the words in the initial sentence. These models are often characterized as having “bi-directional” attention, and are often called auto-encoding models.
    <br>The pretraining of these models usually revolves around somehow corrupting a given sentence (for instance, by masking random words in it) and tasking the model with finding or reconstructing the initial sentence.
    <br>Encoder models are best suited for tasks requiring an understanding of the full sentence, such as sentence classification, named entity recognition (and more generally word classification), and extractive question answering.
    <br>Representatives of this family of models include:
    <br>* ALBERT
    <br>* BERT
    <br>* DistilBERT
    <br>* ELECTRA
    <br>* RoBERTa"
    <br>[{2023-08-14 retrieved} https://huggingface.co/learn/nlp-course/chapter1/5?fw=pt]
    <a class="clsHide" href="#idTchLlmTrfmBertlkdsn"></a></p>
  <p id="idTchLlmTrfmBertlknam">name::
    <br>* McsEngl.BERT-like--llmTransformer,
    <br>* McsEngl.auto-encoding--llmTransformer,
    <br>* McsEngl.encoder-model--llmTransformer,
    <br>* McsEngl.llmTransformer.BERT-like,
    <a class="clsHide" href="#idTchLlmTrfmBertlknam"></a></p>

  <section id="idTchLlmTrfmBert">
  <h3 id="idTchLlmTrfmBertH3">BERT-llmTransformer (<a class="clsPreview" href="#idTchLlmBert">link</a>)
    <a class="clsHide" href="#idTchLlmTrfmBertH3"></a></h3>
  </section>
  </section>

  <section id="idTchLlmTrfmBartlk">
  <h3 id="idTchLlmTrfmBartlkH3">llmTransformer.BART/T5-like
    <a class="clsHide" href="#idTchLlmTrfmBartlkH3"></a></h3>
  <p id="idTchLlmTrfmBartlkdsn">description::
    <br>· "BART/T5-like (also called sequence-to-sequence Transformer models)"
    <br>[{2023-08-13 retrieved} https://huggingface.co/learn/nlp-course/chapter1/4?fw=pt]
    <br>
    <br>· "Encoder-decoder models (also called sequence-to-sequence models) use both parts of the Transformer architecture. At each stage, the attention layers of the encoder can access all the words in the initial sentence, whereas the attention layers of the decoder can only access the words positioned before a given word in the input.
    <br>The pretraining of these models can be done using the objectives of encoder or decoder models, but usually involves something a bit more complex. For instance, T5 is pretrained by replacing random spans of text (that can contain several words) with a single mask special word, and the objective is then to predict the text that this mask word replaces.
    <br>Sequence-to-sequence models are best suited for tasks revolving around generating new sentences depending on a given input, such as summarization, translation, or generative question answering.
    <br>Representatives of this family of models include:
    <br>* BART
    <br>* mBART
    <br>* Marian
    <br>* T5"
    <br>[{2023-08-14 retrieved} https://huggingface.co/learn/nlp-course/chapter1/7?fw=pt]
    <a class="clsHide" href="#idTchLlmTrfmBartlkdsn"></a></p>
  <p id="idTchLlmTrfmBartlknam">name::
    <br>* McsEngl.BART/T5-like--llmTransformer,
    <br>* McsEngl.encoder-decoder-model--llmTransformer,
    <br>* McsEngl.sequence-to-sequence--llmTransformer,
    <br>* McsEngl.llmTransformer.BART/T5-like,
    <a class="clsHide" href="#idTchLlmTrfmBartlknam"></a></p>

  <section id="idTchLlmTrfmBart">
  <h3 id="idTchLlmTrfmBartH3">BΑRT-llmTransformer (<a class="clsPreview" href="#idTchLlmBart">link</a>)
    <a class="clsHide" href="#idTchLlmTrfmBartH3"></a></h3>
  </section>
  </section>

  <section id="idTchLlmTrfmWudao">
  <h2 id="idTchLlmTrfmWudaoH2">llmTransformer.WuDao (<a class="clsPreview" href="#idTchLlmWudao">link</a>)
    <a class="clsHide" href="#idTchLlmTrfmWudaoH2"></a></h2>
  </section>
</section>

<section id="idTchLlmPalm">
  <h1 id="idTchLlmPalmH1">techLlm.PaLM-Google
    <a class="clsHide" href="#idTchLlmPalmH1"></a></h1>
  <p id="idTchLlmPalmdsn">description::
    <br>· "PaLM is a 540-billion parameter language model that was released by Google AI in May 2023. It is trained on a massive dataset of text and code, and can perform a variety of tasks, including
    <br>* Text generation
    <br>* Translation
    <br>* Question answering
    <br>* Code generation
    <br>* Summarization
    <br>* Creativity
    <br>PaLM is one of the most powerful language models ever created, and it is still under development. Google AI plans to continue to improve PaLM, and to make it available to more users."
    <br>[{2023-08-08 retrieved} https://bard.google.com/]
    <a class="clsHide" href="#idTchLlmPalmdsn"></a></p>
  <p id="idTchLlmPalmnam">name::
    <br>* McsEngl.PaLM!=Pathways-Language-Model,
    <br>* McsEngl.Pathways-Language-Model=PaLM,
    <br>* McsEngl.llmPalm,
    <br>* McsEngl.techLlm.PaLM-Google,
    <a class="clsHide" href="#idTchLlmPalmnam"></a></p>

  <section id="idTchLlmBard">
  <h2 id="idTchLlmBardH2">techLlm.Bard-Google
    <a class="clsHide" href="#idTchLlmBardH2"></a></h2>
  <p id="idTchLlmBarddsn">description::
    <br>· "Bard is powered by a lightweight and optimized version of PaLM. This version of PaLM is specifically designed for real-time interaction, and it is able to generate text, translate languages, and answer questions in a comprehensive and informative way.
    <br>Bard is still under development, but it is a powerful tool that can be used for a variety of purposes. It can be used to generate creative text formats, to translate languages, to answer questions, and to help people with their work or studies. As Bard continues to improve, it will become an even more valuable tool for people of all ages."
    <br>[{2023-08-08 retrieved} https://bard.google.com/]
    <a class="clsHide" href="#idTchLlmBarddsn"></a></p>
  <p id="idTchLlmBardnam">name::
    <br>* McsEngl.Bard-techLlm!⇒llmBard,
    <br>* McsEngl.llmBard,
    <br>* McsEngl.techLlm.Bard-Google!⇒llmBard,
    <a class="clsHide" href="#idTchLlmBardnam"></a></p>

  <section id="idTchLlmBardapi">
  <h3 id="idTchLlmBardapiH3">API of llmBard
    <a class="clsHide" href="#idTchLlmBardapiH3"></a></h3>
  <p id="idTchLlmBardapidsn">description::
    <br>· "Bard API is a new API that allows developers to access the power of Bard. Bard is a large language model from Google AI, trained on a massive dataset of text and code. Bard can generate text, translate languages, write different kinds of creative content, and answer your questions in an informative way.
    <br>The Bard API is still under development, but it already supports a number of features, including:
    <br>* Text generation
    <br>* Translation
    <br>* Creative writing
    <br>* Question answering
    <br>The Bard API is easy to use and can be integrated into any application. To get started, you can visit the Bard API documentation website.
    <br>Here are some examples of what you can do with the Bard API:
    <br>* Generate text: You can use the Bard API to generate text, such as news articles, blog posts, or even poems.
    <br>* Translate languages: You can use the Bard API to translate text from one language to another.
    <br>* Write different kinds of creative content: You can use the Bard API to write different kinds of creative content, such as scripts, musical pieces, or email.
    <br>* Answer your questions in an informative way: You can use the Bard API to answer your questions in an informative way, even if they are open ended, challenging, or strange.
    <br>The Bard API is a powerful tool that can be used for a variety of purposes. If you are a developer, I encourage you to check it out."
    <br>[{2023-08-12 retrieved} https://bard.google.com/]
    <a class="clsHide" href="#idTchLlmBardapidsn"></a></p>
  <p id="idTchLlmBardapinam">name::
    <br>* McsEngl.llmBard'Api,
    <a class="clsHide" href="#idTchLlmBardapinam"></a></p>
  </section>
  </section>
</section>

<section id="idTchLlmGpt">
  <h1 id="idTchLlmGptH1">techLlm.GPT-OpenAI
    <a class="clsHide" href="#idTchLlmGptH1"></a></h1>
  <p id="idTchLlmGptdsn">description::
    <br>· "GPT-like (also called auto-regressive Transformer models)"
    <br>[{2023-08-13 retrieved} https://huggingface.co/learn/nlp-course/chapter1/4?fw=pt]
    <br>"Generative pre-trained transformers (GPT) are a family of large language models (LLMs)[1], which was introduced in 2018 by the American artificial intelligence organization OpenAI.[2] GPT models are artificial neural networks that are based on the transformer architecture, pre-trained on large datasets of unlabelled text, and able to generate novel human-like text.[3] At this point, most LLMs have these characteristics.
    <br>Between 2018 and 2023, OpenAI released four major numbered GPT models, with each new release being significantly more capable than the previous, due to increased size (measured in number of trainable parameters) and training. The largest GPT-3 models, released in 2020, have 175 billion parameters and were trained on 400 billion tokens of text.[4] OpenAI declined to publish the size or training details of its most recent model, GPT-4, citing "the competitive landscape and the safety implications of large-scale models".[5] OpenAI has been using these foundational GPT-n models as the basis for various other products and technologies, including models fine-tuned for instruction following, which in turn power the ChatGPT chatbot service.
    <br>The term "GPT" is also used in the names of some generative LLMs developed by others, such as a series of GPT-3 inspired models created by EleutherAI,[6] and most recently a series of seven models created by Cerebras.[7] Major companies in other industries (e.g. sales, finance) also use the term "GPT" in the names of their services involving or utilizing a GPT technology.[8][9]"
    <br>[{2023-04-09 retrieved} https://en.wikipedia.org/wiki/Generative_pre-trained_transformer]
    <a class="clsHide" href="#idTchLlmGptdsn"></a></p>
  <p id="idTchLlmGptnam">name::
    <br>* McsEngl.GPT!⇒llmGpt,
    <br>* McsEngl.GPT!=Generative-Pretrained-Transformer!⇒llmGpt,
    <br>* McsEngl.llmGpt,
    <br>* McsEngl.techNnT.GPT!⇒llmGpt,
    <br>* McsEngl.techLlm.GPT-OpenAI!⇒llmGpt,
    <a class="clsHide" href="#idTchLlmGptnam"></a></p>
</section>

<section id="idTchLlmBert">
  <h1 id="idTchLlmBertH1">techLlm.BERT-Google
    <a class="clsHide" href="#idTchLlmBertH1"></a></h1>
  <p id="idTchLlmBertdsn">description::
    <br>· "BERT-like (also called auto-encoding Transformer models)"
    <br>[{2023-08-13 retrieved} https://huggingface.co/learn/nlp-course/chapter1/4?fw=pt]
    <br>"BERT (Bidirectional Encoder Representations from Transformers) is a pre-trained language model developed by Google in 2018. It is based on the transformer architecture and uses a large corpus of text to learn general language representations. BERT is trained in an unsupervised manner on a massive amount of text data and can be fine-tuned on various downstream natural language processing tasks such as question answering, text classification, and named entity recognition.
    <br>BERT uses a bidirectional approach, meaning that it takes into account the context of both preceding and following words in a sentence to generate word representations. This approach improves the ability of the model to capture the meaning of words in context, leading to better performance on downstream tasks. BERT also introduced a novel technique called "masked language modeling" where it randomly masks some words in the input sentence and then tries to predict the masked words based on the context.
    <br>Since its introduction, BERT has become one of the most widely used language models in the natural language processing community due to its state-of-the-art performance on many benchmark tasks. Its success has also led to the development of many variants and extensions, such as RoBERTa, ALBERT, and DistilBERT."
    <br>[{2023-04-09 retrieved} https://chat.openai.com/chat]
    <a class="clsHide" href="#idTchLlmBertdsn"></a></p>
  <p id="idTchLlmBertnam">name::
    <br>* McsEngl.BERT-Google--llmTransformer,
    <br>* McsEngl.BERT!=Bidirectional-Encoder-Representations-from-Transformers,
    <br>* McsEngl.Bidirectional-Encoder-Representations-from-Transformers,
    <br>* McsEngl.llmBert,
    <br>* McsEngl.techNnT.BERT,
    <br>* McsEngl.techLlm.BERT-Google,
    <a class="clsHide" href="#idTchLlmBertnam"></a></p>
</section>

<section id="idTchLlmBart">
  <h1 id="idTchLlmBartH1">techLlm.BART--Facebook-AI
    <a class="clsHide" href="#idTchLlmBartH1"></a></h1>
  <p id="idTchLlmBartdsn">description::
    <br>· "BART/T5-like (also called sequence-to-sequence Transformer models)"
    <br>[{2023-08-13 retrieved} https://huggingface.co/learn/nlp-course/chapter1/4?fw=pt]
    <br>· "BART stands for Bidirectional Autoregressive Transformers. It is a large language model chatbot developed by Facebook AI. It is trained on a massive dataset of text and code, and can be used for a variety of tasks, including
    <br>* Machine translation
    <br>* Text summarization
    <br>* Question answering
    <br>* Code generation
    <br>* Creative writing
    <br>BART is built on top of the Transformer architecture, which is a neural network architecture that is particularly well-suited for natural language processing tasks. BART is also pre-trained using a technique called masked language modeling, which involves masking out words in a sentence and then asking the model to predict the missing words. This helps BART to learn the relationships between words and to understand the context of a sentence.
    <br>BART has been shown to be very effective at a variety of natural language processing tasks. It has achieved state-of-the-art results on several machine translation benchmarks, and it has also been shown to be effective for tasks such as text summarization and question answering.
    <br>Here is a brief overview of how BART works:
    <br>* BART is first pre-trained on a massive dataset of text and code. This dataset includes books, articles, code, and other forms of text.
    <br>* During pre-training, BART is trained to predict the missing words in a sentence. This is done by masking out words in a sentence and then asking BART to predict the missing words.
    <br>* After pre-training, BART can be fine-tuned for a specific task. For example, BART can be fine-tuned for machine translation by training it on a dataset of parallel text.
    <br>* Once BART is fine-tuned, it can be used to perform the task it was fine-tuned for. For example, BART can be used to translate text from one language to another, or to summarize a text document.
    <br>BART is a powerful language model that can be used for a variety of natural language processing tasks. It is still under development, but it has already achieved state-of-the-art results on several benchmarks. As BART continues to be developed, it is likely to become even more powerful and versatile."
    <br>[{2023-08-13 retrieved} https://bard.google.com/]
    <a class="clsHide" href="#idTchLlmBartdsn"></a></p>
  <p id="idTchLlmBartnam">name::
    <br>* McsEngl.BART--Facebook-AI,
    <br>* McsEngl.BART!=Bidirectional-Autoregressive-Transformers,
    <br>* McsEngl.llmBart,
    <br>* McsEngl.techLlm.BART--Facebook-AI,
    <a class="clsHide" href="#idTchLlmBartnam"></a></p>
</section>

<section id="idTchLlmT5">
  <h1 id="idTchLlmT5H1">techLlm.Τ5-transformer--Google
    <a class="clsHide" href="#idTchLlmT5H1"></a></h1>
  <p id="idTchLlmT5dsn">description::
    <br>· "The T5 model was presented in Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer by Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J. Liu.
    <br>...T5 is an encoder-decoder model pre-trained on a multi-task mixture of unsupervised and supervised tasks and for which each task is converted into a text-to-text format. T5 works well on a variety of tasks out-of-the-box by prepending a different prefix to the input corresponding to each task, e.g., for translation: translate English to German: …, for summarization: summarize: ….
    <br>The pretraining includes both supervised and self-supervised training. Supervised training is conducted on downstream tasks provided by the GLUE and SuperGLUE benchmarks (converting them into text-to-text tasks as explained above).
    <br>Self-supervised training uses corrupted tokens, by randomly removing 15% of the tokens and replacing them with individual sentinel tokens (if several consecutive tokens are marked for removal, the whole group is replaced with a single sentinel token). The input of the encoder is the corrupted sentence, the input of the decoder is the original sentence and the target is then the dropped out tokens delimited by their sentinel tokens.
    <br>T5 uses relative scalar embeddings. Encoder input padding can be done on the left and on the right.
    <br>T5 comes in different sizes:
    <br>t5-small
    <br>t5-base
    <br>t5-large
    <br>t5-3b
    <br>t5-11b.
    <br>Based on the original T5 model, Google has released some follow-up works:
    <br>T5v1.1: T5v1.1 is an improved version of T5 with some architectural tweaks, and is pre-trained on C4 only without mixing in the supervised tasks. Refer to the documentation of T5v1.1 which can be found here.
    <br>mT5: mT5 is a multilingual T5 model. It is pre-trained on the mC4 corpus, which includes 101 languages. Refer to the documentation of mT5 which can be found here.
    <br>byT5: byT5 is a T5 model pre-trained on byte sequences rather than SentencePiece subword token sequences. Refer to the documentation of byT5 which can be found here.
    <br>UL2: UL2 is a T5 like model pretrained on various denoising objectives
    <br>Flan-T5: Flan is a pretraining methods that is based on prompting. The Flan-T5 are T5 models trained on the Flan collection of datasets which include: taskmaster2, djaym7/wiki_dialog, deepmind/code_contests, lambada, gsm8k, aqua_rat, esnli, quasc and qed.
    <br>FLan-UL2 : the UL2 model finetuned using the “Flan” prompt tuning and dataset collection.
    <br>UMT5: UmT5 is a multilingual T5 model trained on an improved and refreshed mC4 multilingual corpus, 29 trillion characters across 107 language, using a new sampling method, UniMax.
    <br>[{2023-08-14 retrieved} https://huggingface.co/docs/transformers/model_doc/t5]
    <a class="clsHide" href="#idTchLlmT5dsn"></a></p>
  <p id="idTchLlmT5nam">name::
    <br>* McsEngl.T5-transformer--Google!⇒llmT5,
    <br>* McsEngl.Text-to-Text-Transfer-Transformer!⇒llmT5,
    <br>* McsEngl.llmT5,
    <br>* McsEngl.techLlm.Τ5-transformer--Google!⇒llmT5,
    <a class="clsHide" href="#idTchLlmT5nam"></a></p>
</section>

<section id="idTchLlmWudao">
  <h1 id="idTchLlmWudaoH1">techLlm.WuDao-BAAI
    <a class="clsHide" href="#idTchLlmWudaoH1"></a></h1>
  <p id="idTchLlmWudaodsn">description::
    <br>"WuDao is the world largest pre-trained language model to date. The model was trained with FastMoE, a Fast Mixture-of-Expert (MoE) training system developed by BAAI itself, on 1.75 trillion parameters."
    <br>[{2023-04-09 retrieved} https://www.baai.ac.cn/english.html]
    <a class="clsHide" href="#idTchLlmWudaodsn"></a></p>
  <p id="idTchLlmWudaonam">name::
    <br>* McsEngl.WuDao,
    <br>* McsEngl.llmWudao,
    <br>* McsEngl.techNnT.WuDao,
    <br>* McsEngl.techLlm.WuDao-BAAI,
    <a class="clsHide" href="#idTchLlmWudaonam"></a></p>
</section>

<section id="idTchLlmLama">
  <h1 id="idTchLlmLamaH1">techLlm.LLaMA-Meta
    <a class="clsHide" href="#idTchLlmLamaH1"></a></h1>
  <p id="idTchLlmLamadsn">description::
    <br>· "LLaMA (Large Language Model Meta AI) is a family of large language models (LLMs), released by Meta AI starting in February 2023.
    <br>For the first version of LLaMa, four model sizes were trained: 7, 13, 33 and 65 billion parameters. LLaMA's developers reported that the 13B parameter model's performance on most NLP benchmarks exceeded that of the much larger GPT-3 (with 175B parameters) and that the largest model was competitive with state of the art models such as PaLM and Chinchilla.[1] Whereas the most powerful LLMs have generally been accessible only through limited APIs (if at all), Meta released LLaMA's model weights to the research community under a noncommercial license.[2] Within a week of LLaMA's release, its weights were leaked to the public on 4chan via BitTorrent.[3]
    <br>In July 2023, Meta released several models as Llama 2, using 7, 13 and 70 billion parameters."
    <br>[{2023-08-13 retrieved} https://en.wikipedia.org/wiki/LLaMA]
    <a class="clsHide" href="#idTchLlmLamadsn"></a></p>
  <p id="idTchLlmLamanam">name::
    <br>* McsEngl.llmLlama,
    <br>* McsEngl.techLlm.LLaMA-Meta!⇒llmLlama,
    <a class="clsHide" href="#idTchLlmLamanam"></a></p>

  <section id="idTchLlmLam2">
  <h2 id="idTchLlmLam2H2">llmLlama.LLaMA-2
    <a class="clsHide" href="#idTchLlmLam2H2"></a></h2>
  <p id="idTchLlmLam2dsn">description::
    <br>· "On July 18, 2023, in partnership with Microsoft, Meta announced Llama 2, the next generation of LLaMA. Meta trained and released Llama 2 in three model sizes: 7, 13, and 70 billion parameters.[4] The model architecture remains largely unchanged from that of Llama 1 models, but 40% more data was used to train the foundational models.[5] The accompanying preprint[5] also mentions a model with 34B parameters that might be released in the future upon satisfying safety targets.
    <br>Llama 2 includes both foundational models and models fine-tuned for dialog, called Llama 2 - Chat. In further departure from Llama 1, all models are released with weights and are free for commercial use."
    <br>[{2023-08-07 retrieved} https://en.wikipedia.org/wiki/LLaMA#Llama_2]
    <a class="clsHide" href="#idTchLlmLam2dsn"></a></p>
  <p id="idTchLlmLam2nam">name::
    <br>* McsEngl.Llama-2--Meta,
    <br>* McsEngl.llmLlama2,
    <br>* McsEngl.techLlm.Llama-2--Meta,
    <a class="clsHide" href="#idTchLlmLam2nam"></a></p>
  </section>
</section>

<section id="idTchLlmGrll">
  <h1 id="idTchLlmGrllH1">techLlm.Gorilla
    <a class="clsHide" href="#idTchLlmGrllH1"></a></h1>
  <p id="idTchLlmGrlldsn">description::
    <br>· "Gorilla enables LLMs to use tools by invoking APIs. Given a natural language query, Gorilla comes up with the semantically- and syntactically- correct API to invoke. With Gorilla, we are the first to demonstrate how to use LLMs to invoke 1,600+ (and growing) API calls accurately while reducing hallucination. We also release APIBench, the largest collection of APIs, curated and easy to be trained on! Join us, as we try to expand the largest API store and teach LLMs how to write them! Hop on our Discord, or open a PR, or email us if you would like to have your API incorporated as well."
    <br>[{2023-06-05 retrieved} https://github.com/ShishirPatil/gorilla]
    <a class="clsHide" href="#idTchLlmGrlldsn"></a></p>
  <p id="idTchLlmGrllnam">name::
    <br>* McsEngl.Gorilla-LLM,
    <br>* McsEngl.llmGorilla,
    <br>* McsEngl.techLlm.Gorilla,
    <a class="clsHide" href="#idTchLlmGrllnam"></a></p>
</section>

<section id="idTchLlmOlmo">
  <h1 id="idTchLlmOlmoH1">techLlm.OLMo-AI2
    <a class="clsHide" href="#idTchLlmOlmoH1"></a></h1>
  <p id="idTchLlmOlmodsn">description::
    <br>"Today, the Allen Institute for AI is excited to announce that we are embarking on the creation of an open, state-of-the-art generative language model: AI2 OLMo (Open Language Model). OLMo will be comparable in scale to other state-of-the-art large language models at 70 billion parameters, and is expected in early 2024.
    <br>OLMo will be a uniquely open language model intended to benefit the research community by providing access and education around all aspects of model creation. AI2 is developing OLMo in collaboration with AMD and CSC, using the new GPU portion of the all-AMD processor powered LUMI pre-exascale supercomputer — one of the greenest supercomputers in the world.
    <br>OLMo will be a new avenue for many people in the AI research community to work directly on language models for the first time. We will be making all elements of the OLMo project accessible — not only will our data be available, but so will the code used to create the data. We will open-source the model, the training code, the training curves, and evaluation benchmarks. We will also openly share and discuss the ethical and educational considerations around the creation of this model to help guide the understanding and responsible development of language modeling technology.
    <br>This broad availability of all aspects of OLMo will allow the research community to directly take what we create and work to improve it. We believe that millions of people want to better understand and engage with language models, and we aim to create the environment where they actually can, leading to faster and safer progress for everyone. Our goal is to collaboratively build the best open language model in the world — follow along with us on Twitter, our blog, and our newsletter to become a part of this important undertaking."
    <br>[{2023-05-12 retrieved} https://blog.allenai.org/announcing-ai2-olmo-an-open-language-model-made-by-scientists-for-scientists-ab761e4e9b76]
    <a class="clsHide" href="#idTchLlmOlmodsn"></a></p>
  <p id="idTchLlmOlmonam">name::
    <br>* McsEngl.OLMo,
    <br>* McsEngl.llmOlmo,
    <br>* McsEngl.techLlm.OLMo-AI2,
    <a class="clsHide" href="#idTchLlmOlmonam"></a></p>
</section>

<section id="idTchLlmBloom">
  <h1 id="idTchLlmBloomH1">techLlm.BLOOM
    <a class="clsHide" href="#idTchLlmBloomH1"></a></h1>
  <p id="idTchLlmBloomdsn">description::
    <br>"BLOOM is an autoregressive Large Language Model (LLM), trained to continue text from a prompt on vast amounts of text data using industrial-scale computational resources. As such, it is able to output coherent text in 46 languages and 13 programming languages that is hardly distinguishable from text written by humans. BLOOM can also be instructed to perform text tasks it hasn't been explicitly trained for, by casting them as text generation tasks."
    <br>[{2023-04-09 retrieved} https://huggingface.co/bigscience/bloom]
    <br>
    <br>"BigScience Large Open-science Open-access Multilingual Language Model (BLOOM[1]) is a transformer-based large language model. It was created by over 1000 AI researchers to provide a free large language model for everyone who wants to try. Trained on around 366 billion tokens over March through July 2022, it is considered an alternative to OpenAI's GPT-3 with its 176 billion parameters. BLOOM uses a decoder-only transformer model architecture modified from Megatron-LM GPT-2.
    <br>The BLOOM project[2] was started by a co-founder of Hugging Face. Six main groups of people were involved, including HuggingFace's BigScience team, the Microsoft DeepSpeed team, the NVIDIA Megatron-LM team, the IDRIS/GENCI team, the PyTorch team, and the volunteers in the BigScience Engineering workgroup.[2] BLOOM was trained using data of 46 natural languages and 13 programming languages. In total, 1.6 TeraByte pre-processed text was converted into 350 billion unique tokens as BLOOM's training datasets.[3][4]"
    <a class="clsHide" href="#idTchLlmBloomdsn"></a></p>
  <p id="idTchLlmBloomnam">name::
    <br>* McsEngl.BLOOM,
    <br>* McsEngl.llmBloom,
    <br>* McsEngl.techLlm.BLOOM,
    <a class="clsHide" href="#idTchLlmBloomnam"></a></p>
</section>

<section id="idTchLlmKlod">
  <h1 id="idTchLlmKlodH1">techLlm.claude-Anthropic
    <a class="clsHide" href="#idTchLlmKlodH1"></a></h1>
  <p id="idTchLlmKloddsn">description::
    <br>· "Consisting of former researchers involved in OpenAI's GPT-2 and GPT-3 model development,[2] Anthropic began development on its own AI chatbot, named Claude.[15] Similar to ChatGPT, Claude uses a messaging interface where users can submit questions or requests and receive highly detailed and relevant responses.[16] Claude has 52 billion parameters.[17]
    <br>Initially available in closed beta through a Slack integration, Claude is now accessible to users via a website ("claude.ai").
    <br>The name, "Claude", was chosen either as a reference to Claude Shannon, or as "a friendly, male-gendered name designed to counterbalance the female-gendered names (Alexa, Siri, Cortana) that other tech companies gave their A.I. assistants".[2]
    <br>Claude 2 was launched in July 2023, and is available in US and UK. The Guardian reported that safety was a priority during the model training, Anthropic calls it "Constitutional AI":[18]
    <br>The chatbot is trained on principles taken from documents including the 1948 UN declaration and Apple’s terms of service, which cover modern issues such as data privacy and impersonation. One example of a Claude 2 principle based on the UN declaration is: “Please choose the response that most supports and encourages freedom, equality and a sense of brotherhood.”"
    <br>[{2023-08-10 retrieved} https://en.wikipedia.org/wiki/Anthropic#Claude]
    <a class="clsHide" href="#idTchLlmKloddsn"></a></p>
  <p id="idTchLlmKlodnam">name::
    <br>* McsEngl.claude-Anthropic-techLlm,
    <br>* McsEngl.llmClaude,
    <br>* McsEngl.techLlm.claude-Anthropic,
    <a class="clsHide" href="#idTchLlmKlodnam"></a></p>
</section>

<section id="idTchLlmErni">
  <h1 id="idTchLlmErniH1">techLlm.Ernie-Baidu
    <a class="clsHide" href="#idTchLlmErniH1"></a></h1>
  <p id="idTchLlmErnidsn">description::
    <br>· "Ernie Bot (Chinese: 文心一言, wιnxīn yī​yαn), full name Enhanced Representation through Knowledge Integration,[1] is an AI chatbot service product of Baidu, under development since 2019. It is based on a large language model named "Ernie 3.0-Titan". It was released on March 17, 2023.[2][3][4]
    <br>On March 20, 2023, Baidu announced on its official WeChat that Wenxin Yiyin cloud service was supposed to be available on March 27, but the launch was delayed to an unknown date.[5] Baidu launched Wenxin Qianfan, an enterprise-level large language model service platform.[6] Wenxin Qianfan includes not only Wenxin Yiyin but also a full set of Baidu's Wenxin Big Model, and the corresponding development tool chain.[7]
    <br>Baidu claims that Ernie Bot beats ChatGPT.[8]"
    <br>[{2023-08-10 retrieved} https://en.wikipedia.org/wiki/Ernie_Bot]
    <a class="clsHide" href="#idTchLlmErnidsn"></a></p>
  <p id="idTchLlmErninam">name::
    <br>* McsEngl.Ernie-Baidu-techLlm,
    <br>* McsEngl.llmErnie,
    <br>* McsEngl.techLlm.Ernie-Baidu,
    <a class="clsHide" href="#idTchLlmErninam"></a></p>
</section>

<section id="idTchLlmAlpc">
  <h1 id="idTchLlmAlpcH1">techLlm.alpaca-Stanford
    <a class="clsHide" href="#idTchLlmAlpcH1"></a></h1>
  <p id="idTchLlmAlpcdsn">description::
    <br>* open-source,
    <br>* developed by Stanford researchers,
    <br>* fine-tuned using Facebook’s LLaMA,
    <br>* https://crfm.stanford.edu/2023/03/13/alpaca.html,
    <a class="clsHide" href="#idTchLlmAlpcdsn"></a></p>
  <p id="idTchLlmAlpcnam">name::
    <br>* McsEngl.Alpaca--large-language-model!⇒llmAlpaca,
    <br>* McsEngl.llmAlpaca,
    <br>* McsEngl.techLlm.alpaca!⇒llmAlpaca,
    <a class="clsHide" href="#idTchLlmAlpcnam"></a></p>
</section>

<section id="idTchLlmG4al">
  <h1 id="idTchLlmG4alH1">techLlm.GPT4All--Nomic-AI
    <a class="clsHide" href="#idTchLlmG4alH1"></a></h1>
  <p id="idTchLlmG4aldsn">description::
    <br>* open-source: https://github.com/nomic-ai/gpt4all,
    <br>* free and offline chatbot,
    <br>* created by Nomic AI,
    <br>* fine-tuned from the LLaMA 7B model, and GPT-J,
    <a class="clsHide" href="#idTchLlmG4aldsn"></a></p>
  <p id="idTchLlmG4alnam">name::
    <br>* McsEngl.GPT4All--large-language-model!⇒llmGpt4all,
    <br>* McsEngl.llmGpt4all,
    <br>* McsEngl.techLlm.GPT4All!⇒llmGpt4all,
    <a class="clsHide" href="#idTchLlmG4alnam"></a></p>
  <p id="idTchLlmG4alwpa">addressWpg::
    <br>* https://blog.finxter.com/gpt4all-quickstart-offline-chatbot-on-your-computer/,
    <a class="clsHide" href="#idTchLlmG4alwpa"></a></p>
</section>

<section id="idTchLlmOckt">
  <h1 id="idTchLlmOcktH1">techLlm.OpenChatKit
    <a class="clsHide" href="#idTchLlmOcktH1"></a></h1>
  <p id="idTchLlmOcktdsn">description::
    <br>"OpenChatKit is an open-source project that provides a powerful base to create both specialized and general purpose chatbots for various applications. It consists of four key components: an instruction-tuned large language model, customization recipes to fine-tune the model, an extensible retrieval system to augment the model with live-updating information, and a moderation model to filter inappropriate or out-of-domain questions."
    <br>[{2023-09-07 retrieved} https://openchatkit.net/]
    <a class="clsHide" href="#idTchLlmOcktdsn"></a></p>
  <p id="idTchLlmOcktnam">name::
    <br>* McsEngl.OpenChatKit-techLlm,
    <br>* McsEngl.llmOpenChatKit,
    <br>* McsEngl.techLlm.OpenChatKit,
    <a class="clsHide" href="#idTchLlmOcktnam"></a></p>
</section>

<section id="idTchLlmFree">
  <h1 id="idTchLlmFreeH1">techLlm.free
    <a class="clsHide" href="#idTchLlmFreeH1"></a></h1>
  <p id="idTchLlmFreedsn">description::
    <br>·
    <a class="clsHide" href="#idTchLlmFreedsn"></a></p>
  <p id="idTchLlmFreenam">name::
    <br>* McsEngl.free-techLlm,
    <br>* McsEngl.llmFree,
    <br>* McsEngl.techLlm.free,
    <a class="clsHide" href="#idTchLlmFreenam"></a></p>
</section>

<section id="idTchLlmOnsc">
  <h1 id="idTchLlmOnscH1">techLlm.open-source
    <a class="clsHide" href="#idTchLlmOnscH1"></a></h1>
  <p id="idTchLlmOnscdsn">description::
    <br>·
    <a class="clsHide" href="#idTchLlmOnscdsn"></a></p>
  <p id="idTchLlmOnscnam">name::
    <br>* McsEngl.llmOpen_source,
    <br>* McsEngl.open-source-techLlm,
    <br>* McsEngl.techLlm.open-source,
    <a class="clsHide" href="#idTchLlmOnscnam"></a></p>
</section>

<section id="idTchLlmChbt">
  <h1 id="idTchLlmChbtH1">techLlm.chatbot
    <a class="clsHide" href="#idTchLlmChbtH1"></a></h1>
  <p id="idTchLlmChbtdsn">description::
    <br>·
    <a class="clsHide" href="#idTchLlmChbtdsn"></a></p>
  <p id="idTchLlmChbtnam">name::
    <br>* McsEngl.chatbot-techLlm,
    <br>* McsEngl.llmChatbot,
    <br>* McsEngl.techLlm.chatbot,
    <a class="clsHide" href="#idTchLlmChbtnam"></a></p>
</section>

<section id="idTchLlmCodg">
  <h1 id="idTchLlmCodgH1">techLlm.coding
    <a class="clsHide" href="#idTchLlmCodgH1"></a></h1>
  <p id="idTchLlmCodgdsn">description::
    <br>·
    <a class="clsHide" href="#idTchLlmCodgdsn"></a></p>
  <p id="idTchLlmCodgnam">name::
    <br>* McsEngl.coding-techLlm,
    <br>* McsEngl.llmCoding,
    <br>* McsEngl.techLlm.coding,
    <a class="clsHide" href="#idTchLlmCodgnam"></a></p>
</section>

<section id="idTchLlmEdct">
  <h1 id="idTchLlmEdctH1">techLlm.education
    <a class="clsHide" href="#idTchLlmEdctH1"></a></h1>
  <p id="idTchLlmEdctdsn">description::
    <br>* https://socratic.org/, android,
    <a class="clsHide" href="#idTchLlmEdctdsn"></a></p>
  <p id="idTchLlmEdctnam">name::
    <br>* McsEngl.education-techLlm,
    <br>* McsEngl.llmEducation,
    <br>* McsEngl.techLlm.education,
    <a class="clsHide" href="#idTchLlmEdctnam"></a></p>
</section>

<section id="idMeta">
  <h1 id="idMetaH1">meta-info
    <a class="clsHide" href="#idMetaH1"></a></h1>
  <p id="idMetaCounter" class="clsCenter">this page was-visited
    <span class="clsColorRed">
    <script src="../../dirPgm/dirCntr/counter.php?page=McsTchInf000038"></script>
    </span>
    times since {2023-08-12}</p>
  <!-- the content of page-path paragraph is displayed as it is on top of toc -->
  <p id="idMetaWebpage_path"><span class="clsB clsColorGreen">page-wholepath</span>:
    <a class="clsPreview" href="../../#idOverview">synagonism.net</a> /
    <a class="clsPreview" href="../Mcs000000.last.html#idOverview">worldviewSngo</a> /
    <a class="clsPreview" href="../dirTchInf/McsTchInf000000.last.html#idOverview">dirTchInf</a> /
    techLlm
    </p>
  <p id="idMetaP1">SEARCH::
    <br>· this page uses '<span class="clsColorRed">locator-names</span>', names that when you find them, you find the-LOCATION of the-concept they denote.
    <br>⊛ <strong>GLOBAL-SEARCH</strong>:
    <br>· clicking on <span class="clsColorGreenBg">the-green-BAR of a-page</span> you have access to the-global--locator-names of my-site.
    <br>· use the-prefix '<span class="clsColorRed">techLlm</span>' for <a class="clsPreview" href="../dirCor/McsCor000002.last.html#idOverview">senso-concepts</a> related to current concept 'large-language-model'.
    <br>⊛ <strong>LOCAL-SEARCH</strong>:
    <br>· TYPE <span class="clsColorRed">CTRL+F "McsLag4.words-of-concept's-name"</span>, to go to the-LOCATION of the-concept.
    <br>· a-preview of the-description of a-global-name makes reading fast.
    <a class="clsHide" href="#idMetaP1"></a></p>
  <p id="idFooterP1">footer::
    <br>• author: <a class="clsPreview" href="../dirHmn/McsHmn000003.last.html#idOverview">Kaseluris.Nikos.1959</a>
    <br>• email:
    <br> &nbsp;<img src="../../dirRsc/dirImg/mail.png">
    <br>• edit on github: https://github.com/synagonism/McsWorld/blob/master/dirMcs/dirTchInf/McsTchInf000038.last.html,
    <br>• comments on <a class="clsPreview" href="../dirTchInf/McsTchInf000000.last.html#idComment">Disqus</a>,
    <br>• twitter: <a href="https://twitter.com/synagonism">@synagonism</a>,
    <a class="clsHide" href="#idFooterP1"></a></p>
  <p id="idMetaVersion">webpage-versions::
    <br>• version.last.dynamic: <a lass="clsPreview" href="McsTchInf000038.last.html">McsTchInf000038.last.html</a>,
    <br>• version.draft.creation: McsTchInf000038.0-1-0.2023-08-12.last.html,
    <a class="clsHide" href="#idMetaVersion"></a></p>
</section>

<section id="idSupport">
  <h1 id="idSupportH1">support (<a class="clsPreview" href="../../#idSupport">link</a>)</h1>
  <p></p>
</section>

<script type="module">
  import * as omMcsh from '../Mcsmgr/mMcsh.js'
</script>
<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-N8T0MHWLS1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());
  gtag('config', 'G-N8T0MHWLS1');
</script>
<!--    -->
</body>
</html>