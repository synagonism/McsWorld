<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Mcs.techLlm!=large-language-model (McsTchInf000038.0-12-0.2024-02-20 draft)</title>
  <meta name="keywords" content="large-language-model, techLlm, ModelConceptSensorial, McsHitp, Synagonism">
  <link rel="stylesheet" href="../Mcsmgr/mHitp.css">
</head>

<body>
<header id="idHeader">
  <p></p>
  <h1 id="idHeaderH1">large-language-model
    <br>senso-concept-Mcs (techLlm)
    </h1>
  <p id="idHeadercrd">McsHitp-creation:: {2023-08-12}
    <a class="clsHide" href="#idHeadercrd"></a></p>
</header>

<section id="idOverview">
  <h1 id="idOverviewH1">overview of techLlm
    <a class="clsHide" href="#idOverviewH1"></a></h1>
  <p id="idDescription">description::
    <br>× generic: <a class="clsPreview" href="../dirTchInf/McsTchInf000039.last.html#idLKnlg002mngr">logo-knowledge-language--manager</a>,
    <br>
    <br>· LLM-Knlgmngr is <a class="clsPreview" href="../dirTchInf/McsTchInf000039.last.html#idLKnlgmngr">a-knowledge-manager</a> that uses an-LLM-(a-statistical-model of a-huge text) as knowledge-base.
    <br>[<a class="clsPreview" href="../dirHmn/McsHmn000003.last.html#idOverview">hmnSngo</a>.{2023-10-31}]
    <br>"A large language model (LLM) is a language model consisting of a neural network with many parameters (typically billions of weights or more), trained on large quantities of unlabelled text using self-supervised learning. LLMs emerged around 2018 and perform well at a wide variety of tasks. This has shifted the focus of natural language processing research away from the previous paradigm of training specialized supervised models for specific tasks.[1]
    <br>Properties
    <br>Though the term large language model has no formal definition, it often refers to deep learning models having a parameter count on the order of billions or more.[2] LLMs are general purpose models which excel at a wide range of tasks, as opposed to being trained for one specific task (such as sentiment analysis, named entity recognition, or mathematical reasoning).[1][3] The skill with which they accomplish tasks, and the range of tasks at which they are capable, seems to be a function of the amount of resources (data, parameter-size, computing power) devoted to them, in a way that is not dependent on additional breakthroughs in design.[4]
    <br>Though trained on simple tasks along the lines of predicting the next word in a sentence, neural language models with sufficient training and parameter counts are found to capture much of the syntax and semantics of human language. In addition, large language models demonstrate are able to "memorize" a great quantity of facts during training.[1] The ability of LLMs to often produce factually accurate responses can create the impression that they have general knowledge about the world; the production of responses with factual content that does not seem to be justified by the model's training data is referred to as a "hallucination".[5]"
    <br>[{2023-04-09 retrieved} https://en.wikipedia.org/wiki/Large_language_model]
    <a class="clsHide" href="#idDescription"></a></p>
  <p id="idName">name::
    <br>* McsEngl.McsTchInf000038.last.html//dirTchInf//dirMcs!⇒techLlm,
    <br>* McsEngl.dirTchInf/McsTchInf000038.last.html!⇒techLlm,
    <br>* McsEngl.LLM!=large-language-model!⇒techLlm,
    <br>* McsEngl.LLM-Knlgmngr!⇒techLlm,
    <br>* McsEngl.LLM-Lgklmngr!⇒techLlm,
    <br>* McsEngl.Llmodel!⇒techLlm!⇒techLlm,
    <br>* McsEngl.Knlgmngr.023-LLM!⇒techLlm,
    <br>* McsEngl.Knlgmngr.LLM-Large-Language-Model!⇒techLlm,
    <br>* McsEngl.klmrLlm!=Large-Language-Model--knowledge-language-manager!⇒techLlm,
    <br>* McsEngl.large-language-model!⇒techLlm,
    <br>* McsEngl.lklmLlm!=logo-knowledge-lanaguage-manager.large-language-model!⇒techLlm, {2023-11-06},
    <br>* McsEngl.techDl.large-language-model!⇒techLlm,
    <br>* McsEngl.techNlm.techLlm!⇒techLlm,
    <br>* McsEngl.techNlplm.large-language-model!⇒techLlm,
    <br>* McsEngl.techLlm!=large-language-model--tech,
    <br>* McsEngl.techLlm:Lgklmngr,
    <br>* McsEngl.techLlm:techDl,
    <br>* McsEngl.techLlm:techNlm,
    <br>====== langoGreek:
    <br>* McsElln.μεγάλο-γλωσσικό-μοντέλο!το!=techLlm,
    <a class="clsHide" href="#idName"></a></p>
</section>

<section id="idTchLlmknbs">
  <h1 id="idTchLlmknbsH1">knowledge-base of techLlm
    <a class="clsHide" href="#idTchLlmknbsH1"></a></h1>
  <p id="idTchLlmknbsdsn">description::
    <br>× generic: <a class="clsPreview" href="../dirTchInf/McsTchInf000036.last.html#idTchNlp001lgml">language-model</a>,
    <br>
    <br>· "the meaning" the-manager knows is a-statistical model on the-input-tex.
    <a class="clsHide" href="#idTchLlmknbsdsn"></a></p>
  <p id="idTchLlmknbsnam">name::
    <br>* McsEngl.techLlm'knowledge-base,
    <a class="clsHide" href="#idTchLlmknbsnam"></a></p>

  <section id="idTchLlmkbvlm">
  <h2 id="idTchLlmkbvlmH2">language-model-vs-knowledge-base of techLlm
    <a class="clsHide" href="#idTchLlmkbvlmH2"></a></h2>
  <p id="idTchLlmkbvlmdsn">description::
    <br>"language-model vs knowledge-base:
    <br>Language models and knowledge bases are both powerful tools for understanding and generating human language. However, they differ in several important ways.
    <br>
    <br>**Language models** are statistical methods that learn to predict the next word in a sequence given the words that have already been used. They are trained on massive amounts of text data, and they are able to learn the patterns of human language. This allows them to generate text that is both grammatically correct and natural-sounding.
    <br>
    <br>**Knowledge bases** are collections of structured data that represent facts about the world. They are typically organized by entities, such as people, places, and things. Each entity has a set of attributes, such as name, description, and relationships to other entities.
    <br>
    <br>Here is a table that summarizes the key differences between language models and knowledge bases:
    <br>| Feature | Language Models | Knowledge Bases |
    <br>| Type of data | Text | Structured data |
    <br>| Format | Statistical model | Collection of entities and attributes |
    <br>| Learning method | Training on massive amounts of text data | Human curation |
    <br>| Strengths | Generating natural-sounding text | Representing facts about the world |
    <br>| Weaknesses | Can generate incorrect or nonsensical text | Can be difficult to maintain and update |
    <br>
    <br>**Applications of language models:**
    <br>* Text generation
    <br>* Machine translation
    <br>* Speech recognition
    <br>* Natural language processing
    <br>* Chatbots
    <br>
    <br>**Applications of knowledge bases:**
    <br>* Question answering
    <br>* Semantic search
    <br>* Recommender systems
    <br>* Data visualization
    <br>* Fraud detection
    <br>
    <br>In general, language models are better suited for tasks that require understanding and generating natural language, while knowledge bases are better suited for tasks that require representing and understanding facts about the world. However, the two can often be used together to achieve powerful results. For example, a language model could be used to generate a natural-language description of an entity in a knowledge base, while a knowledge base could be used to provide information about the context of a conversation.
    <br>
    <br>The future of language models and knowledge bases is bright. As they continue to develop, they will become even more powerful tools for understanding and generating human language and for representing and understanding the world around us."
    <br>[{2023-11-06 retrieved} https://bard.google.com/chat/1e39dbadda8d0cf7?hl=en&pli=1]
    <br>
    <br>"language-model vs knowledge-base:
    <br>Language models and knowledge bases are two different approaches to storing and utilizing information, particularly in the context of natural language understanding and processing. Here are the key differences between them:
    <br>
    <br>1. Nature of Information:
    <br>   - Language Model: Language models like GPT-3 are trained on vast amounts of text data from the internet, but they do not have a structured representation of specific knowledge. They learn patterns in language and can generate text based on the data they were trained on. They do not have a predefined understanding of specific facts or structured information.
    <br>   - Knowledge Base: A knowledge base, on the other hand, is a structured repository of specific information or knowledge. It contains organized data about facts, entities, relationships, and concepts. Knowledge bases are typically designed with specific knowledge in mind, and the information is curated and structured.
    <br>
    <br>2. Querying and Retrieval:
    <br>   - Language Model: Language models generate responses based on the patterns they have learned from the training data. When you ask a question or provide a prompt to a language model, it doesn't directly access a structured knowledge base but generates a response based on its understanding of language and context.
    <br>   - Knowledge Base: Knowledge bases are designed for structured retrieval of specific information. They allow users to query for facts and data using formal queries or structured search criteria. This makes them suitable for precise and fact-based inquiries.
    <br>
    <br>3. Generalization vs. Specificity:
    <br>   - Language Model: Language models can generalize and generate text about a wide range of topics and in a conversational manner. They are not limited to specific knowledge and can provide contextually relevant responses on a broad spectrum of topics.
    <br>   - Knowledge Base: Knowledge bases are typically domain-specific and contain information about a particular field or subject. They are well-suited for providing detailed and specific information but may not handle general or open-domain queries as effectively.
    <br>
    <br>4. Data Volume:
    <br>   - Language Model: Language models are trained on massive amounts of text data from the internet, making them capable of handling a wide range of topics and languages.
    <br>   - Knowledge Base: Knowledge bases are designed with a predefined set of information and are typically more limited in scope compared to language models. They may not be as comprehensive or up-to-date as language models for general knowledge.
    <br>
    <br>In summary, language models are versatile in generating text and understanding language in a broad sense but do not have structured knowledge. Knowledge bases, on the other hand, are designed for structured data storage and retrieval, making them suitable for specific domains or topics. The choice between a language model and a knowledge base depends on the specific use case and the nature of the information you need to work with. In many applications, a combination of both may be used to benefit from their respective strengths."
    <br>[{2023-11-06 retrieved} https://chat.openai.com/c/71ca2857-4dfd-4041-8c86-574a7ee0ed53]
    <a class="clsHide" href="#idTchLlmkbvlmdsn"></a></p>
  <p id="idTchLlmkbvlmnam">name::
    <br>* McsEngl.knowledge-base-vs-language-model,
    <br>* McsEngl.language-model-vs-knowledge-base,
    <br>* McsEngl.techLlm'language-model-vs-knowledge-base,
    <a class="clsHide" href="#idTchLlmkbvlmnam"></a></p>
  </section>
</section>

<section id="idTchLlmhard">
  <h1 id="idTchLlmhardH1">hardware-sys of techLlm
    <a class="clsHide" href="#idTchLlmhardH1"></a></h1>
  <p id="idTchLlmharddsn">description::
    <br>"on what hardware LLM run?
    <br>Large language models (LLMs) are computationally intensive applications that require powerful hardware to run effectively. The specific hardware requirements for LLMs can vary depending on the size and complexity of the model, as well as the desired performance. However, there are some general hardware components that are essential for running LLMs.
    <br>
    <br>**GPUs**
    <br>Graphics processing units (GPUs) are the most important hardware component for running LLMs. GPUs are designed to accelerate parallel computations, which are essential for training and running deep learning models. Most LLMs are trained on NVIDIA GPUs, which are the market leader in high-performance GPUs. NVIDIA GPUs offer a variety of features that are important for LLMs, such as:
    <br>* **Tensor cores:** Tensor cores are specialized hardware units that are designed to accelerate matrix multiplication, which is a key operation in deep learning.
    <br>* **High memory bandwidth:** Memory bandwidth is the rate at which data can be transferred between the GPU and memory. High memory bandwidth is essential for LLMs, as they often require large amounts of data to be processed.
    <br>* **Large memory capacity:** LLMs can be very large, requiring gigabytes or even terabytes of memory. NVIDIA GPUs offer a variety of memory options, including high-bandwidth memory (HBM) and GDDR6X memory.
    <br>
    <br>**CPUs**
    <br>Central processing units (CPUs) are also important for running LLMs, but they are not as critical as GPUs. CPUs are typically used for tasks such as:
    <br>* **Preprocessing data:** CPUs are often used to preprocess data before it is fed into the GPU. This can include tasks such as tokenization, normalization, and stemming.
    <br>* **Running other software:** CPUs are also used to run other software, such as operating systems and deep learning frameworks.
    <br>
    <br>**RAM**
    <br>Random access memory (RAM) is used to store data that is being used by the CPU and GPU. LLMs can require large amounts of RAM, especially during training. It is important to have enough RAM to store the entire model, as well as the data that is being processed.
    <br>
    <br>**Storage**
    <br>LLMs can also require a lot of storage space. This is because the models themselves can be very large, and the models also need to be stored along with the data that they are trained on. It is important to have a high-performance storage system that can handle the large amount of data that is required.
    <br>
    <br>**Networking**
    <br>LLMs can also require a lot of network bandwidth, especially when they are being trained. This is because the training process requires a lot of data to be transferred between the GPUs and the CPU. It is important to have a high-performance networking system that can handle the large amount of data that is required.
    <br>
    <br>**Examples of hardware configurations for LLMs**
    <br>Here are some examples of hardware configurations that are commonly used for LLMs:
    <br>* **A single high-end GPU with 24GB of memory, such as the NVIDIA GeForce RTX 3090.** This is a good option for running small to medium-sized LLMs.
    <br>* **Multiple high-end GPUs with 24GB of memory each, connected together with NVLink.** This is a good option for running large LLMs.
    <br>* **A cluster of CPUs and GPUs, with high-performance storage and networking.** This is a good option for running very large LLMs or for training LLMs on a large dataset.
    <br>
    <br>The specific hardware configuration that you need will depend on your specific needs. If you are not sure what hardware configuration is right for you, you can consult with a hardware expert."
    <br>[{2023-11-06 retrieved} https://bard.google.com/chat/d0f01a27c0c5a08b?hl=en&pli=1]
    <br>
    <br>"on what hardware LLM run?
    <br>Large language models (LLMs) like the one you're currently interacting with typically run on powerful hardware, often in data centers or cloud infrastructure. The specific hardware used can vary depending on the organization or company operating the model and the model's size. As of my last knowledge update in January 2022, these models often ran on hardware configurations that included:
    <br>
    <br>1. Graphics Processing Units (GPUs): Many LLMs were initially developed and trained on GPUs, which are well-suited for deep learning tasks like natural language processing. High-end GPUs from NVIDIA, such as the Tesla V100, were commonly used.
    <br>
    <br>2. Tensor Processing Units (TPUs): Some organizations and researchers also used TPUs, which are specialized hardware accelerators designed by Google specifically for machine learning workloads. TPUs can offer significant performance benefits for training and running LLMs.
    <br>
    <br>3. Custom hardware: Some organizations were exploring custom hardware solutions, like OpenAI's "Dactyl" and Google's "TPUv4," designed for AI and machine learning tasks.
    <br>
    <br>4. Cloud infrastructure: LLMs were often hosted on cloud platforms like Amazon Web Services (AWS), Microsoft Azure, and Google Cloud Platform (GCP). These cloud providers offer a range of GPU and TPU instances for running LLMs.
    <br>
    <br>It's important to note that hardware configurations and technology evolve over time, so there may have been advancements and changes in the hardware used to run LLMs since my last knowledge update in early 2022. The choice of hardware also depends on factors like cost, availability, and specific use cases.
    <br>
    <br>For the most up-to-date information on the hardware used for LLMs, you should refer to the latest publications and announcements from organizations that develop and deploy these models, as well as information from cloud service providers offering these services."
    <br>[{2023-11-06 retrieved} https://chat.openai.com/c/e196956e-3d4e-4b9c-a28f-e3225618b018]
    <a class="clsHide" href="#idTchLlmharddsn"></a></p>
  <p id="idTchLlmhardnam">name::
    <br>* McsEngl.techLlm'hardware-sys,
    <a class="clsHide" href="#idTchLlmhardnam"></a></p>
</section>

<section id="idTchLlmsoft">
  <h1 id="idTchLlmsoftH1">software-sys of techLlm
    <a class="clsHide" href="#idTchLlmsoftH1"></a></h1>
  <p id="idTchLlmsoftdsn">description::
    <br>"on what software LLMs run?
    <br>Large language models (LLMs) are complex software systems that require a variety of software components to run effectively. The specific software requirements for LLMs can vary depending on the size and complexity of the model, as well as the desired performance. However, there are some general software components that are essential for running LLMs.
    <br>
    <br>**Deep learning frameworks**
    <br>Deep learning frameworks are software libraries that provide the necessary tools and infrastructure for developing and training deep learning models. Some of the most popular deep learning frameworks for LLMs include:
    <br>* **TensorFlow:** TensorFlow is an open-source deep learning framework developed by Google. It is one of the most popular deep learning frameworks and is used to develop a wide variety of LLMs.
    <br>* **PyTorch:** PyTorch is an open-source deep learning framework developed by Meta. It is another popular deep learning framework and is known for its ease of use and flexibility.
    <br>* **JAX:** JAX is an open-source deep learning framework developed by Google AI. It is a relatively new deep learning framework, but it is gaining popularity due to its high performance and efficient use of hardware.
    <br>
    <br>**Model checkpoints**
    <br>Model checkpoints are snapshots of the model parameters at a particular point in time. They are used to save the progress of training and to load the model for inference.
    <br>
    <br>**Data loaders**
    <br>Data loaders are responsible for loading and preprocessing data for training and inference. They can handle a variety of data formats, such as text, images, and audio.
    <br>
    <br>**Optimizers**
    <br>Optimizers are used to update the model parameters during training. They are responsible for finding the optimal set of parameters that minimize the loss function.
    <br>
    <br>**Schedulers**
    <br>Schedulers are used to control the training process. They can adjust the learning rate, batch size, and other hyperparameters over time.
    <br>
    <br>**Inference engines**
    <br>Inference engines are used to run LLMs for inference. They are responsible for taking input data and producing output data.
    <br>
    <br>**Examples of software configurations for LLMs**
    <br>Here are some examples of software configurations that are commonly used for LLMs:
    <br>* **TensorFlow with Horovod for distributed training.** Horovod is a distributed training library that can be used to train LLMs on multiple GPUs or CPUs.
    <br>* **PyTorch with Lightning for training and inference.** Lightning is a high-level deep learning library that can be used to simplify the training and inference process.
    <br>* **JAX with Flax for training and inference.** Flax is a high-performance deep learning library that can be used to train and run LLMs on a variety of hardware platforms.
    <br>
    <br>The specific software configuration that you need will depend on your specific needs. If you are not sure what software configuration is right for you, you can consult with a software expert."
    <br>[{2023-11-06 retrieved} https://bard.google.com/chat/d0f01a27c0c5a08b?hl=en&pli=1]
    <br>
    <br>"on what software LLMs run?
    <br>Large Language Models (LLMs) like GPT-3 typically run on high-performance computing infrastructure. They are deployed on powerful servers and clusters of computers in data centers. The software stack for running LLMs typically includes the following components:
    <br>
    <br>1. Deep Learning Frameworks: LLMs are built and trained using deep learning frameworks such as TensorFlow, PyTorch, or others. These frameworks provide the tools and libraries for building, training, and deploying neural networks.
    <br>
    <br>2. GPU Acceleration: To speed up the training and inference of LLMs, graphics processing units (GPUs) or, in some cases, more specialized hardware like TPUs (Tensor Processing Units) are often used. These specialized processors are designed to handle the intense computational demands of deep learning models.
    <br>
    <br>3. Distributed Computing: LLMs are often deployed across multiple machines in a distributed computing environment to scale their performance. Distributed computing frameworks like Apache Spark or custom solutions are used to manage the parallel execution of tasks.
    <br>
    <br>4. Model Serving Frameworks: To make LLMs accessible via APIs, they are often wrapped in model serving frameworks like TensorFlow Serving, PyTorch Serve, or custom API servers. These frameworks handle requests and responses, ensuring that the model can be utilized by applications and services.
    <br>
    <br>5. Web Servers and APIs: LLMs are typically made available through web servers or APIs. These APIs allow developers to send text to the model and receive responses, making it easy to integrate LLMs into various applications.
    <br>
    <br>6. Custom Code and Middleware: Depending on the specific use case, there may be additional custom code and middleware involved to manage input data, preprocess it, post-process model outputs, and handle various aspects of model deployment.
    <br>
    <br>7. Storage and Data Management: Large language models require extensive storage for the model parameters and datasets used for training and fine-tuning. Distributed file systems and data storage solutions are often used to manage this data.
    <br>
    <br>8. Monitoring and Logging: Tools for monitoring the performance of the LLM, tracking usage, and logging errors are essential for ensuring the reliability and stability of the model.
    <br>
    <br>Overall, the software stack for running LLMs is a complex and highly specialized combination of various technologies to handle the massive computational demands and to make the models accessible and useful in real-world applications."
    <br>[{2023-11-06 retrieved} https://chat.openai.com/c/f15bbd5d-640d-4b83-8aa0-f3ebedcbd4bb]
    <a class="clsHide" href="#idTchLlmsoftdsn"></a></p>
  <p id="idTchLlmsoftnam">name::
    <br>* McsEngl.techLlm'software-sys,
    <a class="clsHide" href="#idTchLlmsoftnam"></a></p>

  <section id="idTchLlmapi">
  <h2 id="idTchLlmapiH2">API of techLlm
    <a class="clsHide" href="#idTchLlmapiH2"></a></h2>
  <p id="idTchLlmapidsn">description::
    <br>"overview of large-language-model APIs:
    <br>Large language model (LLM) APIs are a new and rapidly developing technology. They provide developers with access to the power of LLMs without having to train and maintain their own models. This makes it possible to build innovative new applications in a wide range of domains, such as:
    <br>* **Natural language processing tasks:** LLMs can be used for a variety of NLP tasks, such as text summarization, machine translation, and question answering.
    <br>* **Content generation:** LLMs can be used to generate creative text formats, such as poems, code, scripts, musical pieces, email, and letters.
    <br>* **Customer service:** LLMs can be used to create chatbots that can provide customer support and answer customer questions.
    <br>* **Education:** LLMs can be used to create personalized learning experiences and provide feedback to students.
    <br>
    <br>LLM APIs are typically offered by cloud computing providers, such as Google AI, Amazon Web Services, and Microsoft Azure. They typically charge on a per-usage basis, based on the number of requests made to the API.
    <br>
    <br>Here is an overview of some of the most popular LLM APIs:
    <br>* **Google AI Platform** offers access to a variety of LLMs, including GPT-3, LaMDA, and PaLM.
    <br>* **Amazon Web Services** offers access to GPT-3 and Amazon Comprehend.
    <br>* **Microsoft Azure** offers access to GPT-3 and Azure Cognitive Services.
    <br>
    <br>To use an LLM API, developers typically need to create an account with the cloud provider and obtain an API key. Once they have an API key, they can make requests to the API to generate text, translate languages, or perform other NLP tasks.
    <br>
    <br>LLM APIs are a powerful tool that can be used to build innovative new applications. However, it is important to note that LLMs are still under development and can sometimes make mistakes. It is also important to use LLMs ethically and responsibly.
    <br>
    <br>Here are some tips for using LLM APIs responsibly:
    <br>* Be aware of the limitations of LLMs. LLMs can make mistakes, especially when given incomplete or ambiguous prompts.
    <br>* Use LLMs for tasks that are appropriate. LLMs should not be used for tasks that require accuracy or reliability, such as medical diagnosis or legal advice.
    <br>* Be respectful of user privacy. LLMs should not be used to generate text that is offensive or discriminatory.
    <br>* Be transparent about your use of LLMs. Let users know when they are interacting with an LLM.
    <br>
    <br>Overall, LLM APIs are a powerful new tool that can be used to build innovative new applications. However, it is important to use them responsibly and ethically."
    <br>[{2023-10-31 retrieved} https://bard.google.com/chat/bbc43ed60b533853]
    <a class="clsHide" href="#idTchLlmapidsn"></a></p>
  <p id="idTchLlmapinam">name::
    <br>* McsEngl.techLlm'API,
    <a class="clsHide" href="#idTchLlmapinam"></a></p>
  </section>

  <section id="idTchLlmaclr">
  <h2 id="idTchLlmaclrH2">accelerator of techLlm
    <a class="clsHide" href="#idTchLlmaclrH2"></a></h2>
  <p id="idTchLlmaclrdsn">description::
    <br>"what is LLM accelerator?
    <br>An LLM accelerator is a tool or technique that can be used to improve the performance of large language models (LLMs). LLMs are complex algorithms that can be used to generate text, translate languages, write different kinds of creative content, and answer questions in an informative way. However, they can be computationally expensive to run, which can make them slow and impractical for some applications.
    <br>
    <br>There are several different types of LLM accelerators, including:
    <br>* **Hardware accelerators:** These are specialized hardware devices that are designed to run LLM algorithms more efficiently.
    <br>* **Software accelerators:** These are software libraries that can be used to improve the performance of LLMs on standard hardware.
    <br>* **Model compression:** This technique involves reducing the size of an LLM model without sacrificing its accuracy. This can make the model faster and more efficient to run.
    <br>* **Quantization:** This technique involves converting the weights of an LLM model from floating-point numbers to integers. This can make the model smaller and faster to run.
    <br>* **Pruning:** This technique involves removing unimportant connections from an LLM model. This can make the model smaller and faster to run.
    <br>
    <br>The specific type of LLM accelerator that is most effective will depend on the specific application and the hardware and software that are available.
    <br>
    <br>Here are some examples of LLM accelerators that are currently available:
    <br>* **Google TPUv4:** This is a hardware accelerator that is designed to run LLMs. It is up to 40 times faster than previous generations of TPUs.
    <br>* **Microsoft Brainwave:** This is a software accelerator that is designed to improve the performance of LLMs on NVIDIA GPUs.
    <br>* **Hugging Face Transformers:** This is a software library that includes several different techniques for compressing and quantizing LLMs.
    <br>* **NVIDIA TensorRT:** This is a software library that includes several different techniques for pruning LLMs.
    <br>
    <br>LLM accelerators are a rapidly developing field, and new tools and techniques are being developed all the time. As LLMs become more complex and computationally expensive, it is likely that LLM accelerators will become increasingly important."
    <br>[{2023-11-05 retrieved} https://bard.google.com/chat/651f881d09d48652?hl=en&pli=1]
    <br>· "Their size is enabled by <a class="clsPreview" href="../dirTchInf/McsTchInf000036.last.html#idTechAiaclr">AI accelerators</a>, which are able to process vast amounts of text data, mostly scraped from the Internet.[1] "
    <br>[{2023-08-12 retrieved} https://en.wikipedia.org/wiki/Large_language_model]
    <a class="clsHide" href="#idTchLlmaclrdsn"></a></p>
  <p id="idTchLlmaclrnam">name::
    <br>* McsEngl.techLlm'accelerator,
    <a class="clsHide" href="#idTchLlmaclrnam"></a></p>
  </section>

  <section id="idTchLlmrsal">
  <h2 id="idTchLlmrsalH2">reasoning-algorithm of klmrLlm
    <a class="clsHide" href="#idTchLlmrsalH2"></a></h2>
  <p id="idTchLlmrsaldsn">description::
    <br>"what are the reasoning algorithms of LLMs?
    <br>While LLMs are still under development, researchers have explored various reasoning algorithms to enhance their capabilities. These algorithms enable LLMs to perform tasks that require logical thinking, problem-solving, and decision-making. Here are some notable reasoning algorithms used in LLMs:
    <br>
    <br>1. **Chain-of-Thought Prompting:** This method guides LLMs to generate intermediate reasoning steps, providing transparency and explainability in their decision-making process. By explicitly breaking down the problem into smaller steps, LLMs can better understand the task and provide more accurate and consistent responses.
    <br>
    <br>2. **Tree-of-Thoughts Prompting:** This approach structures the reasoning process as a tree, allowing LLMs to explore multiple potential paths and identify the most relevant information. It helps LLMs avoid getting stuck in local optima and consider alternative perspectives, leading to more comprehensive and well-reasoned solutions.
    <br>
    <br>3. **Algorithmic Prompting:** This method explicitly teaches LLMs specific algorithms, such as sorting or graph search, enabling them to tackle problems systematically and efficiently. By providing concrete examples and step-by-step instructions, LLMs can learn to apply these algorithms to new situations and solve problems that require structured reasoning.
    <br>
    <br>4. **Knowledge Graph Prompting:** This technique leverages knowledge graphs, which represent relationships between entities, to provide LLMs with contextual information and enhance their reasoning abilities. By incorporating knowledge graphs, LLMs can make more informed decisions and provide more accurate answers, especially in complex domains that require background knowledge.
    <br>
    <br>5. **First-Order Logic (FOL) Reasoning:** This approach formalizes reasoning using FOL, a symbolic language that allows for precise representation of logical relationships. By encoding problems and their constraints in FOL, LLMs can perform deductive reasoning, drawing inferences from premises and reaching valid conclusions.
    <br>
    <br>6. **Probabilistic Reasoning:** This method uses probabilistic models to represent uncertainty and make inferences under incomplete information. By calculating probabilities and considering different scenarios, LLMs can make more informed decisions in situations where there is no perfect knowledge.
    <br>
    <br>7. **Meta-reasoning:** This technique enables LLMs to reason about their own reasoning processes, reflecting on their strengths and weaknesses and adapting their strategies accordingly. By monitoring their performance and evaluating their reasoning steps, LLMs can improve their decision-making and become more self-aware.
    <br>
    <br>These reasoning algorithms are actively being researched and refined, and new approaches are continuously being developed. As LLMs become more sophisticated and their reasoning capabilities improve, they have the potential to transform various fields, including artificial intelligence, natural language processing, and robotics."
    <br>[{2023-11-05 retrieved} https://bard.google.com/chat/651f881d09d48652?hl=en&pli=1]
    <a class="clsHide" href="#idTchLlmrsaldsn"></a></p>
  <p id="idTchLlmrsalnam">name::
    <br>* McsEngl.klmrLllm'reasoning-algorithm,
    <a class="clsHide" href="#idTchLlmrsalnam"></a></p>
  </section>
</section>

<section id="idTchLlmarcr">
  <h1 id="idTchLlmarcrH1">architecture of techLlm
    <a class="clsHide" href="#idTchLlmarcrH1"></a></h1>
  <p id="idTchLlmarcrdsn">description::
    <br>* <a class="clsPreview" href="../dirTchInf/McsTchInf000036.last.html#idTchLlmTrfm">transformer</a>,
    <br>
    <br>"Transformer architecture contributed to faster training.[2] Alternative architectures include the mixture of experts (MoE), which has been proposed by Google, starting with sparsely-gated ones in 2017,[3] Gshard in 2021[4] to GLaM in 2022.[5]"
    <br>[{2023-08-12 retrieved} https://en.wikipedia.org/wiki/Large_language_model]
    <a class="clsHide" href="#idTchLlmarcrdsn"></a></p>
  <p id="idTchLlmarcrnam">name::
    <br>* McsEngl.techLlm'architecture,
    <a class="clsHide" href="#idTchLlmarcrnam"></a></p>
</section>

<section id="idTchLlmprmr">
  <h1 id="idTchLlmprmrH1">parameter of techLlm
    <a class="clsHide" href="#idTchLlmprmrH1"></a></h1>
  <p id="idTchLlmprmrdsn">description::
    <br>* WuDao: 1.75 trillion parameters,
    <br>* PaLM: 540 billion parameters,
    <br>* GPT-3: 175 billion parameters,
    <br>* Llama-2: 7, 13, 70 billion parameters,
    <a class="clsHide" href="#idTchLlmprmrdsn"></a></p>
  <p id="idTchLlmprmrnam">name::
    <br>* McsEngl.techLlm'parameter,
    <a class="clsHide" href="#idTchLlmprmrnam"></a></p>
</section>

<section id="idTchLlmdtst">
  <h1 id="idTchLlmdtstH1">dataset of techLlm
    <a class="clsHide" href="#idTchLlmdtstH1"></a></h1>
  <p id="idTchLlmdtstdsn">description::
    <br>· "Bard is trained on a dataset of 1.56 trillion words, and has 137 billion parameters. The dataset is a combination of text and code, and is drawn from a variety of sources, including the web, books, and user-generated content."
    <br>[{2023-08-08 retrieved} https://bard.google.com/]
    <br>
    <br>· "PaLM 2 was trained on a dataset of text and code that was more than 540 billion words, which is more than 10 times the amount of data that was used to train GPT-3."
    <br>[{2023-08-08 retrieved} https://poe.com/Google-PaLM]
    <br>
    <br>· GPT-3 was trained on a dataset with a size of hundreds of terabytes.
    <a class="clsHide" href="#idTchLlmdtstdsn"></a></p>
  <p id="idTchLlmdtstnam">name::
    <br>* McsEngl.techLlm'dataset,
    <br>* McsEngl.techLlm'input,
    <a class="clsHide" href="#idTchLlmdtstnam"></a></p>
</section>

<section id="idTchLlmprmt">
  <h1 id="idTchLlmprmtH1">prompt of techLlm
    <a class="clsHide" href="#idTchLlmprmtH1"></a></h1>
  <p id="idTchLlmprmtdsn">description::
    <br>· "Models have limitations on the number of tokens (words, characters, spaces) they can handle. It’s 4000 tokens for GPT-3, 8000 for GPT-4, and 100k for Claude 2. Tailoring your input to these constraints will yield better results."
    <br>[{2023-08-08 retrieved} https://blog.finxter.com/alien-technology-catching-up-on-llms-prompting-chatgpt-plugins-embeddings-code-interpreter/]
    <a class="clsHide" href="#idTchLlmprmtdsn"></a></p>
  <p id="idTchLlmprmtnam">name::
    <br>* McsEngl.techLlm'prompt,
    <br>* McsEngl.techLlm'user-input,
    <a class="clsHide" href="#idTchLlmprmtnam"></a></p>
</section>

<section id="idTchLlmeval">
  <h1 id="idTchLlmevalH1">evaluation of techLlm
    <a class="clsHide" href="#idTchLlmevalH1"></a></h1>
  <p id="idTchLlmevaldsn">description::
    <br>* can generate incorrect or nonsensical text,
    <a class="clsHide" href="#idTchLlmevaldsn"></a></p>
  <p id="idTchLlmevalnam">name::
    <br>* McsEngl.techLlm'evaluation,
    <a class="clsHide" href="#idTchLlmevalnam"></a></p>

  <section id="idTchLlmevalIssu">
  <h2 id="idTchLlmevalIssuH2">issue of techLlm
    <a class="clsHide" href="#idTchLlmevalIssuH2"></a></h2>
  <p id="idTchLlmevalIssudsn">description::
    <br>"Some of the issues that need to be addressed include:
    <br>* Bias and fairness: How can we ensure that ChatGPT and other models do not reflect or amplify the biases and prejudices that exist in the data they are trained on, such as racism, sexism, or homophobia? How can we make sure that ChatGPT and other models treat all people and groups fairly and respectfully?
    <br>* Privacy and security: How can we protect the privacy and security of the data that ChatGPT and other models use and generate, such as personal information, sensitive documents, or confidential communications? How can we prevent ChatGPT and other models from being hacked or misused by malicious actors, such as cybercriminals, terrorists, or rogue states?
    <br>* Accountability and transparency: How can we hold ChatGPT and other models accountable for the actions and outcomes they produce, such as the accuracy, quality, and impact of the text they generate or the tasks they perform? How can we make ChatGPT and other models transparent and explainable, so that we can understand how they work and why they make certain decisions?
    <br>* Regulation and governance: How can we regulate and govern ChatGPT and other models, such as setting standards, rules, and laws for their development, deployment, and use? How can we ensure that ChatGPT and other models are aligned with human values, interests, and goals, and that they do not harm or threaten human dignity, rights, and welfare?"
    <br>[{2023-11-19 retrieved} https://copilot.microsoft.com/]
    <a class="clsHide" href="#idTchLlmevalIssudsn"></a></p>
  <p id="idTchLlmevalIssunam">name::
    <br>* McsEngl.techLlm'issue,
    <a class="clsHide" href="#idTchLlmevalIssunam"></a></p>
  <p id="idTchLlmevalIssuwpa">addressWpg::
    <br>* https://reglab.stanford.edu/publications/hlarge-legal-fictions-profiling-legal-hallucinations-in-large-language-models/,
    <a class="clsHide" href="#idTchLlmevalIssuwpa"></a></p>
  </section>

  <section id="idTchLlmevalHumn">
  <h2 id="idTchLlmevalHumnH2">HumanEval of techLlm
    <a class="clsHide" href="#idTchLlmevalHumnH2"></a></h2>
  <p id="idTchLlmevalHumndsn">description::
    <br>"HumanEval coding task:
    <br>HumanEval is a benchmark for evaluating the ability of large language models (LLMs) to generate code. It consists of 164 programming problems that assess various aspects of coding ability, including language comprehension, algorithms, and simple mathematics. The problems are designed to be challenging for LLMs, and they are not included in the training data of any of the existing LLMs.
    <br>
    <br>The HumanEval dataset is a useful tool for evaluating the progress of LLMs in coding. It can also be used to identify areas where LLMs need to improve. For example, LLMs currently struggle with problems that require them to reason about complex data structures or to generate code that is both correct and efficient.
    <br>
    <br>Here are some of the benefits of using the HumanEval dataset:
    <br>* It is a well-defined and standardized benchmark.
    <br>* It is a challenging benchmark that is not included in the training data of any of the existing LLMs.
    <br>* It can be used to evaluate a wide range of coding abilities.
    <br>* It is a publicly available dataset that can be used by anyone.
    <br>
    <br>If you are interested in learning more about the HumanEval dataset, you can visit the following website:
    <br>https://github.com/openai/human-eval
    <br>
    <br>You can also find the code for the HumanEval evaluation harness on GitHub:
    <br>https://github.com/VHellendoorn/Code-LMs"
    <br>[{2023-11-05 retrieved} https://bard.google.com/chat/961d5e03ccced38c?hl=en&pli=1]
    <a class="clsHide" href="#idTchLlmevalHumndsn"></a></p>
  <p id="idTchLlmevalHumnnam">name::
    <br>* McsEngl.HumanEval-of-techLlm,
    <br>* McsEngl.techLlm'HumanEval,
    <a class="clsHide" href="#idTchLlmevalHumnnam"></a></p>
  </section>

  <section id="idTchLlmevalMmlu">
  <h2 id="idTchLlmevalMmluH2">MMLU of techLlm
    <a class="clsHide" href="#idTchLlmevalMmluH2"></a></h2>
  <p id="idTchLlmevalMmludsn">description::
    <br>"MMLU:
    <br>MMLU (Massive Multitask Language Understanding) is a benchmark designed to measure the breadth and depth of a model's academic and professional understanding. It tests a model's ability to understand and apply knowledge across a wide range of subjects, including elementary mathematics, US history, computer science, law, and more. The benchmark consists of 57 tasks, each of which is designed to be challenging for even the most advanced language models.
    <br>
    <br>The MMLU benchmark was developed by a team of researchers at Google AI and OpenAI. The researchers believe that the benchmark is a valuable tool for evaluating the progress of language models, and they hope that it will help to identify areas where language models need to improve.
    <br>
    <br>Here are some of the benefits of using the MMLU benchmark:
    <br>* It is a well-defined and standardized benchmark.
    <br>* It is a comprehensive benchmark that covers a wide range of subjects.
    <br>* It is a challenging benchmark that is not included in the training data of any of the existing language models.
    <br>* It can be used to identify strengths and weaknesses of language models.
    <br>
    <br>If you are interested in learning more about the MMLU benchmark, you can visit the following website:
    <br>https://arxiv.org/abs/2009.03300"
    <br>[{2023-11-05 retrieved} https://bard.google.com/chat/961d5e03ccced38c?hl=en&pli=1]
    <a class="clsHide" href="#idTchLlmevalMmludsn"></a></p>
  <p id="idTchLlmevalMmlunam">name::
    <br>* McsEngl.MMLU!=Massive-Multitask-Language-Understanding,
    <br>* McsEngl.techLlm'MMLU,
    <a class="clsHide" href="#idTchLlmevalMmlunam"></a></p>
  </section>
</section>

<section id="idTchLlmfmwk">
  <h1 id="idTchLlmfmwkH1">framework of techLlm
    <a class="clsHide" href="#idTchLlmfmwkH1"></a></h1>
  <p id="idTchLlmfmwkdsn">description::
    <br>· framework is an-app for developing applications powered by large-language-models.
    <br>
    <br>"is large-language-model an app?
    <br>A large language model, like GPT-3 or GPT-4, is not an app itself but a type of artificial intelligence (AI) technology. These models are used to develop applications, services, and tools that can perform various natural language processing tasks, such as text generation, translation, question answering, and more. Developers and organizations build applications and services that leverage these models to provide specific functionalities, but the models themselves are not apps. They are usually deployed on servers and accessed via APIs (Application Programming Interfaces) by software applications and services."
    <br>[{2023-10-31 retrieved} https://chat.openai.com/c/b4f86a94-6e9e-4cc7-995a-89ac9325f0aa]
    <a class="clsHide" href="#idTchLlmfmwkdsn"></a></p>
  <p id="idTchLlmfmwknam">name::
    <br>* McsEngl.Llmframework,
    <br>* McsEngl.techLlm'framework,
    <a class="clsHide" href="#idTchLlmfmwknam"></a></p>

  <section id="idTchLlmfmwkChlg">
  <h2 id="idTchLlmfmwkChlgH2">Llmframework.ChainLang
    <a class="clsHide" href="#idTchLlmfmwkChlgH2"></a></h2>
  <p id="idTchLlmfmwkChlgdsn">description::
    <br>"overview of ChainLang:
    <br>LangChain is an open-source framework for developing applications powered by large language models (LLMs). It provides a standard interface for building chains, a variety of integrations with other tools, and end-to-end chains for common applications.
    <br>
    <br>LangChain is designed to make it easier to build applications with LLMs by providing a number of features, including:
    <br>* A standard interface for chains, which makes it easy to chain together different components of an application.
    <br>* A variety of integrations with other tools, such as vector databases, knowledge bases, and APIs.
    <br>* Pre-built end-to-end chains for common applications, such as chatbots, question answering, and summarization.
    <br>
    <br>LangChain can be used to build a wide variety of applications, including:
    <br>* Chatbots
    <br>* Question answering systems
    <br>* Summarization systems
    <br>* Code generation tools
    <br>* Creative writing tools
    <br>* Search engines
    <br>* And more
    <br>
    <br>LangChain is still under development, but it is already being used by a number of companies and researchers to build innovative applications.
    <br>
    <br>Here is a high-level overview of how LangChain works:
    <br>1. The user provides a prompt to LangChain.
    <br>2. LangChain chains together a series of components to process the prompt and generate a response.
    <br>3. The response is then returned to the user.
    <br>
    <br>The components that can be chained together in LangChain include:
    <br>* **Model I/O:** This component handles the interaction with the LLM.
    <br>* **Retrieval:** This component retrieves information from external sources, such as vector databases and knowledge bases.
    <br>* **Chains:** This component chains together other components to create more complex applications.
    <br>* **Agents:** This component uses LLMs to decide what actions to take.
    <br>* **Memory:** This component stores information that can be used by other components in the chain.
    <br>* **Callbacks:** This component allows developers to hook into the LangChain pipeline at different points.
    <br>
    <br>LangChain is a powerful tool for building applications with LLMs. It is easy to use and provides a wide range of features that make it a valuable asset for any developer."
    <br>[{2023-10-30 retrieved} https://bard.google.com/chat/20b1d5c35f58d7d7]
    <a class="clsHide" href="#idTchLlmfmwkChlgdsn"></a></p>
  <p id="idTchLlmfmwkChlgnam">name::
    <br>* McsEngl.ChainLang-Llmframework,
    <br>* McsEngl.Llmframework.ChainLang,
    <a class="clsHide" href="#idTchLlmfmwkChlgnam"></a></p>
  </section>
</section>

<section id="idTchLlmrkrs">
  <h1 id="idTchLlmrkrsH1">relation-to-KRS of klmrLlm
    <a class="clsHide" href="#idTchLlmrkrsH1"></a></h1>
  <p id="idTchLlmrkrsdsn">description::
    <br>"is an LLM a knowledge-representation-system?
    <br>An LLM can be classified as a knowledge-representation-system in the sense that it encodes knowledge in the form of statistical regularities learned from a massive corpus of data. LLMs use a variety of techniques to represent knowledge, such as word embeddings, attention mechanisms, and transformers. These techniques allow LLMs to learn complex relationships between words and concepts, which can be used for tasks such as language translation, natural language generation, and question answering.
    <br>
    <br>LLMs are not explicitly programmed with knowledge, but rather they learn knowledge from the data they are trained on. This makes them more flexible and adaptable than traditional knowledge representation systems, which are typically based on hand-crafted rules or ontologies. However, LLMs can also be more opaque and difficult to understand than traditional knowledge representation systems, as it can be difficult to determine how an LLM has learned a particular piece of knowledge.
    <br>
    <br>Here is a table that summarizes the key differences between LLMs and traditional knowledge representation systems:
    <br>
    <br>| Feature | LLM | Traditional knowledge representation system |
    <br>| Knowledge representation | Statistical regularities | Rules or ontologies |
    <br>| Knowledge acquisition | Learning from data | Hand-crafting |
    <br>| Flexibility | More flexible | Less flexible |
    <br>| Transparency | Less transparent | More transparent |
    <br>
    <br>Overall, LLMs are a powerful new tool for representing knowledge, and they have the potential to revolutionize the way we interact with computers. However, there are also some challenges associated with using LLMs, such as their opacity and their potential for bias. As LLMs continue to develop, it will be important to address these challenges in order to ensure that they are used in a responsible and ethical way.
    <br>
    <br>In addition to the above, here are some additional points to consider:
    <br>* LLMs are not perfect, and they can sometimes make mistakes. It is important to be aware of the limitations of LLMs and to use them with caution.
    <br>* LLMs are a valuable tool for understanding the world around us. By studying LLMs, we can learn more about how language works and how humans think.
    <br>* LLMs have the potential to be used for a variety of beneficial applications, such as education, healthcare, and customer service. As LLMs continue to develop, it is important to think about how they can be used to improve the lives of others.
    <br>
    <br>I hope this helps! Let me know if you have any other questions."
    <br>[{2023-11-05 retrieved} https://bard.google.com/chat/651f881d09d48652?hl=en&pli=1]
    <a class="clsHide" href="#idTchLlmrkrsdsn"></a></p>
  <p id="idTchLlmrkrsnam">name::
    <br>* McsEngl.klmrLlm'relation-to-KRS,
    <a class="clsHide" href="#idTchLlmrkrsnam"></a></p>
</section>

<section id="idTchLlmrscF">
  <h1 id="idTchLlmrscFH1">info-resource of techLlm
    <a class="clsHide" href="#idTchLlmrscFH1"></a></h1>
  <p id="idTchLlmrscwpa">addressWpg::
    <br>* {2017-06-12} Attention Is All You Need: https://arxiv.org/abs/1706.03762,
    <a class="clsHide" href="#idTchLlmrscwpa"></a></p>
  <p id="idTchLlmrscnam">name::
    <br>* McsEngl.techLlm'Infrsc,
    <a class="clsHide" href="#idTchLlmrscnam"></a></p>
</section>

<section id="idTchLlmdngF">
  <h1 id="idTchLlmdngFH1">DOING of techLlm
    <a class="clsHide" href="#idTchLlmdngFH1"></a></h1>
  <p id="idTchLlmdngdsn">description::
    <br>* "As language models, they work by taking an input text and repeatedly predicting the next token or word.[6]"
    <br>[{2023-08-12 retrieved} https://en.wikipedia.org/wiki/Large_language_model]
    <a class="clsHide" href="#idTchLlmdngdsn"></a></p>
  <p id="idTchLlmdngnam">name::
    <br>* McsEngl.techLlm'doing,
    <a class="clsHide" href="#idTchLlmdngnam"></a></p>

  <section id="idTchLlmprtr">
  <h2 id="idTchLlmprtrH2">pretraining of techLlm
    <a class="clsHide" href="#idTchLlmprtrH2"></a></h2>
  <p id="idTchLlmprtrdsn">description::
    <br>· "Pretraining is the act of training a model from scratch: the weights are randomly initialized, and the training starts without any prior knowledge.
    <br>This pretraining is usually done on very large amounts of data. Therefore, it requires a very large corpus of data, and training can take up to several weeks."
    <br>[{2023-08-13 retrieved} https://huggingface.co/learn/nlp-course/chapter1/]
    <a class="clsHide" href="#idTchLlmprtrdsn"></a></p>
  <p id="idTchLlmprtrnam">name::
    <br>* McsEngl.techLlm'pretraining,
    <a class="clsHide" href="#idTchLlmprtrnam"></a></p>
  </section>

  <section id="idTchLlmfntn">
  <h2 id="idTchLlmfntnH2">fine-tuning of techLlm
    <a class="clsHide" href="#idTchLlmfntnH2"></a></h2>
  <p id="idTchLlmfntndsn">description::
    <br>· "In deep learning, fine-tuning is an approach to transfer learning in which the weights of a pre-trained model are trained on new data.[1] Fine-tuning can be done on the entire neural network, or on only a subset of its layers, in which case the layers that are not being fine-tuned are "frozen" (not updated during the backpropagation step).[2] A model may also be augmented with "adapters" that consist of far fewer parameters than the original model, and fine-tuned in a parameter-efficient way by tuning the weights of the adapters and leaving the rest of the model's weights frozen.[3]
    <br>For some architectures, such as convolutional neural networks, it is common to keep the earlier layers (those closest to the input layer) frozen because they capture lower-level features, while later layers often discern high-level features that can be more related to the task that the model is trained on.[2][4]
    <br>Models that are pre-trained on large and general corpora are usually fine-tuned by reusing the model's parameters as a starting point and adding a task-specific layer trained from scratch.[5] Fine-tuning the full model is common as well and often yields better results, but it is more computationally expensive.[6]
    <br>Fine-tuning is typically accomplished with supervised learning, but there are also techniques to fine-tune a model using weak supervision.[7] Fine-tuning can be combined with a reinforcement learning from human feedback-based objective to produce language models like ChatGPT (a fine-tuned version of GPT-3) and Sparrow.[8][9]"
    <br>[{2023-08-13 retrieved} https://en.wikipedia.org/wiki/Fine-tuning_(deep_learning)]
    <a class="clsHide" href="#idTchLlmfntndsn"></a></p>
  <p id="idTchLlmfntnnam">name::
    <br>* McsEngl.fine-tuning--techLlm,
    <br>* McsEngl.techLlm'fine-tuning,
    <a class="clsHide" href="#idTchLlmfntnnam"></a></p>
  </section>
</section>

<section id="idTchLlmusag">
  <h1 id="idTchLlmusagH1">usage of techLlm
    <a class="clsHide" href="#idTchLlmusagH1"></a></h1>
  <p id="idTchLlmusagdsn">description::
    <br>"applications of  large language model:
    <br>Large language models like GPT-3 have a wide range of applications across various domains. Some of the key applications include:
    <br>
    <br>1. **Natural Language Understanding:** These models can comprehend and generate human-like text, enabling better chatbots, virtual assistants, and more effective human-computer interactions.
    <br>
    <br>2. **Text Generation:** They can generate human-like text for various purposes, such as content generation, creative writing, and more.
    <br>
    <br>3. **Translation:** They can be used for machine translation, making it easier to translate text between languages accurately.
    <br>
    <br>4. **Summarization:** These models can generate concise summaries of longer text, which is valuable for news articles, research papers, and other documents.
    <br>
    <br>5. **Sentiment Analysis:** Large language models can be used to determine the sentiment expressed in text, which is useful for brand monitoring, social media analysis, and customer feedback.
    <br>
    <br>6. **Content Recommendation:** They can power recommendation systems by understanding user preferences and suggesting relevant products, articles, or content.
    <br>
    <br>7. **Information Retrieval:** These models can help in retrieving specific information from large textual databases or documents.
    <br>
    <br>8. **Data Entry and Extraction:** They can assist in data entry tasks by extracting structured data from unstructured text, like filling out forms automatically.
    <br>
    <br>9. **Language Translation and Language Learning:** Large language models can aid language learners by providing real-time translation and language-specific learning resources.
    <br>
    <br>10. **Medical and Scientific Research:** They can assist in analyzing and summarizing medical literature and scientific research papers, aiding researchers in staying up to date with the latest advancements.
    <br>
    <br>11. **Automated Customer Support:** Chatbots powered by these models can provide immediate assistance to customers, answer common questions, and direct users to the right resources.
    <br>
    <br>12. **Content Moderation:** They can help in automating the process of content moderation on websites and social media platforms by identifying and filtering out inappropriate or harmful content.
    <br>
    <br>13. **Financial Analysis:** These models can process and analyze financial reports, news, and market data to provide insights for investment decisions.
    <br>
    <br>14. **Legal Document Analysis:** They can review legal documents and contracts, identify potential issues, and assist in due diligence and legal research.
    <br>
    <br>15. **Accessibility:** They can make digital content more accessible to people with disabilities by providing text-to-speech and speech-to-text capabilities.
    <br>
    <br>16. **Education:** These models can be used in online education platforms for auto-grading assignments, generating learning materials, and providing personalized feedback to students.
    <br>
    <br>17. **Gaming:** Large language models can enhance video game experiences by creating dynamic and interactive in-game narratives and dialogues.
    <br>
    <br>18. **Storytelling and Creative Writing:** They can assist writers by generating ideas, plotlines, and characters, or even by co-authoring stories.
    <br>
    <br>19. **Personal Assistants:** They can be integrated into personal digital assistants, helping users with tasks like scheduling, setting reminders, and answering questions.
    <br>
    <br>20. **Ethical and Bias Mitigation:** There is an increasing focus on using large language models to detect and mitigate bias, as well as to promote ethical use of AI technologies.
    <br>
    <br>These applications showcase the versatility and potential impact of large language models across a wide range of industries and use cases. However, it's important to consider ethical and responsible use, as well as addressing potential biases and limitations in these models."
    <br>[{2023-10-31 retrieved} https://chat.openai.com/c/22d0dc76-cd83-49b0-83a8-1805b22c81e3]
    <a class="clsHide" href="#idTchLlmusagdsn"></a></p>
  <p id="idTchLlmusagnam">name::
    <br>* McsEngl.techLlm'usage,
    <a class="clsHide" href="#idTchLlmusagnam"></a></p>
</section>

<section id="idTchLlmevgF">
  <h1 id="idTchLlmevgFH1">evoluting of techLlm
    <a class="clsHide" href="#idTchLlmevgFH1"></a></h1>
  <p id="idTchLlmevgFdsn">description::
    <br>"evolution of LLM:
    <br>The evolution of large language models (LLMs) has been a rapid one, with significant advancements being made in recent years. This progress has been driven by a number of factors, including the increasing availability of large datasets of text and code, the development of more powerful computing hardware, and the introduction of new machine learning techniques.
    <br>
    <br>**Early LLMs**
    <br>The first LLMs were developed in the 1950s and 1960s. These models were relatively simple and had limited capabilities. For example, one of the earliest LLMs, called Eliza, was able to simulate human conversation by using a set of pre-defined rules. However, Eliza was not able to understand the meaning of the user's input or generate truly creative responses.
    <br>
    <br>**Statistical language models**
    <br>In the 2000s, statistical language models were developed. These models were more sophisticated than earlier LLMs and were able to capture the statistical relationships between words in a corpus of text. This allowed statistical language models to generate more realistic and coherent text.
    <br>
    <br>**Deep learning**
    <br>The introduction of deep learning techniques in the 2010s revolutionized the field of natural language processing (NLP). Deep learning models are able to learn complex patterns in data and generate more accurate and creative text than statistical language models.
    <br>
    <br>**Transformers and attention mechanisms**
    <br>In 2017, the Transformer architecture was introduced. Transformers are a type of neural network that are able to learn long-range dependencies in text data. This makes them ideal for tasks such as machine translation and text summarization.
    <br>
    <br>**Modern LLMs**
    <br>Modern LLMs are trained on massive datasets of text and code. They are able to perform a wide range of tasks, including:
    <br>* Generating text, code, and other creative content
    <br>* Translating languages
    <br>* Answering questions in an informative way
    <br>* Summarizing text
    <br>* Classifying text
    <br>* Completing tasks
    <br>
    <br>**Future of LLMs**
    <br>LLMs are still under development, but they have the potential to revolutionize many industries and applications. For example, LLMs could be used to create more natural and engaging chatbots, develop more powerful translation tools, and create new forms of creative content.
    <br>Here are some of the key trends that are shaping the future of LLMs:
    <br>* **Larger and more complex models:** LLMs are becoming larger and more complex, with some models now having trillions of parameters. This allows them to learn more complex patterns in data and generate more accurate and creative text.
    <br>* **More efficient training:** Researchers are developing new training algorithms that make it more efficient to train LLMs. This is making it possible to train LLMs on larger datasets and to develop more complex models.
    <br>* **New applications:** LLMs are being used to develop new applications in a wide range of industries, including healthcare, education, and customer service. As LLMs continue to improve, they are likely to have an even greater impact on our lives."
    <br>[{2023-10-30 retrieved} https://bard.google.com/chat/20b1d5c35f58d7d7]
    <a class="clsHide" href="#idTchLlmevgFdsn"></a></p>
  <p id="idTchLlmevgnam">name::
    <br>* McsEngl.evoluting-of-techLlm,
    <br>* McsEngl.techLlm'evoluting,
    <a class="clsHide" href="#idTchLlmevgnam"></a></p>
  <p id="idTchLlmevg20230812">{2023-08-12}::
    <br>=== McsHitp-creation:
    <br>· creation of current <a class="clsPreview" href="../dirTchInf/McsTchInf000009.last.html#idMcsHitpFil">webpage-concept</a>.
    <a class="clsHide" href="#idTchLlmevg20230812"></a></p>
</section>

<section id="idTchLlmmisc">
  <h1 id="idTchLlmmiscH1">MISC-ATTRIBUTE of techLlm
    <a class="clsHide" href="#idTchLlmmiscH1"></a></h1>
  <p id="idTchLlmmiscdsn">description::
    <br>· "Eight Things to Know about Large Language Models
    <br>Samuel R. Bowman
    <br>The widespread public deployment of large language models (LLMs) in recent months has prompted a wave of new attention and engagement from advocates, policymakers, and scholars from many fields. This attention is a timely response to the many urgent questions that this technology raises, but it can sometimes miss important considerations. This paper surveys the evidence for eight potentially surprising such points:
    <br>1. LLMs predictably get more capable with increasing investment, even without targeted innovation.
    <br>2. Many important LLM behaviors emerge unpredictably as a byproduct of increasing investment.
    <br>3. LLMs often appear to learn and use representations of the outside world.
    <br>4. There are no reliable techniques for steering the behavior of LLMs.
    <br>5. Experts are not yet able to interpret the inner workings of LLMs.
    <br>6. Human performance on a task isn't an upper bound on LLM performance.
    <br>7. LLMs need not express the values of their creators nor the values encoded in web text.
    <br>8. Brief interactions with LLMs are often misleading."
    <br>[{2023-08-12 retrieved} https://arxiv.org/abs/2304.00612]
    <a class="clsHide" href="#idTchLlmmiscdsn"></a></p>
  <p id="idTchLlmmiscnam">name::
    <br>* McsEngl.techLlm'misc,
    <a class="clsHide" href="#idTchLlmmiscnam"></a></p>
</section>

<section id="idTchLlmwptF">
  <h1 id="idTchLlmwptFH1">WHOLE-PART-TREE of techLlm
    <a class="clsHide" href="#idTchLlmwptFH1"></a></h1>
  <p id="idTchLlmwptnam">name::
    <br>* McsEngl.techLlm'part-whole-tree,
    <br>* McsEngl.techLlm'whole-part-tree,
    <a class="clsHide" href="#idTchLlmwptnam"></a></p>
  <p id="idTchLlmwtr">whole-tree-of-techLlm::
    <br>*
    <br>* ... <a class="clsPreview" href="../dirCor/McsCor000003.last.html#idEntwtr">Sympan</a>.
    <a class="clsHide" href="#idTchLlmwtr"></a></p>
  <p id="idTchLlmptr">part-tree-of-techLlm::
    <br>*
    <a class="clsHide" href="#idTchLlmptr"></a></p>
</section>

<section id="idTchLlmgstF">
  <h1 id="idTchLlmgstFH1">GENERIC-SPECIFIC-TREE of techLlm
    <a class="clsHide" href="#idTchLlmgstFH1"></a></h1>
  <p id="idTchLlmgstnam">name::
    <br>* McsEngl.techLlm'generic-specific-tree,
    <br>* McsEngl.techLlm'specific-generic-tree,
    <a class="clsHide" href="#idTchLlmgstnam"></a></p>
  <p id="idTchLlmgtr">generic-tree-of-techLlm::
    <br>* <a class="clsPreview" href="../dirTchInf/McsTchInf000039.last.html#idLKnlg002mngr">logo-knowledge-language--manager</a>,
    <br>* ... <a class="clsPreview" href="../dirCor/McsCor000003.last.html#idOverview">entity</a>.
    <a class="clsHide" href="#idTchLlmgtr"></a></p>
  <p id="idTchLlmstr">specific-tree-of-techLlm::
    <br>* neural-language-model,
    <br>* deep-learning,
    <br>* generic-Llm,
    <br>* instruction-tuned-Llm,
    <br>* dialog-tuned-Llm,
    <br>===
    <br>* LLaMa-2 (Meta),
    <br>* PALM-2 (Bard-Google),
    <a class="clsHide" href="#idTchLlmstr"></a></p>
</section>

<section id="idTchLlmFdtn">
  <h1 id="idTchLlmFdtnH1">techLlm.foundation-model
    <a class="clsHide" href="#idTchLlmFdtnH1"></a></h1>
  <p id="idTchLlmFdtndsn">description::
    <br>·
    <a class="clsHide" href="#idTchLlmFdtndsn"></a></p>
  <p id="idTchLlmFdtnnam">name::
    <br>* McsEngl.foundation-Llmodel,
    <br>* McsEngl.techLlm.foundation-model,
    <a class="clsHide" href="#idTchLlmFdtnnam"></a></p>
</section>

<section id="idTchLlmFntd">
  <h1 id="idTchLlmFntdH1">techLlm.fine-tuned--model
    <a class="clsHide" href="#idTchLlmFntdH1"></a></h1>
  <p id="idTchLlmFntddsn">description::
    <br>·
    <a class="clsHide" href="#idTchLlmFntddsn"></a></p>
  <p id="idTchLlmFntdnam">name::
    <br>* McsEngl.fine-tuned--Llmodel,
    <br>* McsEngl.techLlm.fine-tuned--model,
    <a class="clsHide" href="#idTchLlmFntdnam"></a></p>
</section>

<section id="idTchLlmTrfm">
  <h1 id="idTchLlmTrfmH1">techLlm.transformer
    <a class="clsHide" href="#idTchLlmTrfmH1"></a></h1>
  <p id="idTchLlmTrfmdsn">description::
    <br>"A transformer is a deep learning model that adopts the mechanism of self-attention, differentially weighting the significance of each part of the input data. It is used primarily in the fields of natural language processing (NLP)[1] and computer vision (CV).[2]
    <br>Like recurrent neural networks (RNNs), transformers are designed to process sequential input data, such as natural language, with applications towards tasks such as translation and text summarization. However, unlike RNNs, transformers process the entire input all at once. The attention mechanism provides context for any position in the input sequence. For example, if the input data is a natural language sentence, the transformer does not have to process one word at a time. This allows for more parallelization than RNNs and therefore reduces training times.[1]
    <br>Transformers were introduced in 2017 by a team at Google Brain[1] and are increasingly becoming the model of choice for NLP problems,[3] replacing RNN models such as long short-term memory (LSTM). The additional training parallelization allows training on larger datasets. This led to the development of pretrained systems such as BERT (Bidirectional Encoder Representations from Transformers) and GPT (Generative Pre-trained Transformer), which were trained with large language datasets, such as the Wikipedia Corpus and Common Crawl, and can be fine-tuned for specific tasks."
    <br>[{2023-04-01 retrieved} https://en.wikipedia.org/wiki/Transformer_(machine_learning_model)]
    <a class="clsHide" href="#idTchLlmTrfmdsn"></a></p>
  <p id="idTchLlmTrfmnam">name::
    <br>* McsEngl.llmTransformer,
    <br>* McsEngl.techLlm.transformer!⇒llmTransformer,
    <br>* McsEngl.techDl.transformer!⇒llmTransformer,
    <br>* McsEngl.techInfo.011-transformer!⇒llmTransformer,
    <br>* McsEngl.techInfo.transformer!⇒llmTransformer,
    <br>* McsEngl.techNn.transformer!⇒llmTransformer,
    <br>* McsEngl.techNnT!⇒llmTransformer,
    <br>* McsEngl.transformer!⇒llmTransformer,
    <a class="clsHide" href="#idTchLlmTrfmnam"></a></p>

  <section id="idTchLlmTrfmatly">
  <h2 id="idTchLlmTrfmatlyH2">attention-layer of llmTransformer
    <a class="clsHide" href="#idTchLlmTrfmatlyH2"></a></h2>
  <p id="idTchLlmTrfmatlydsn">description::
    <br>· "A key feature of Transformer models is that they are built with special layers called attention layers. In fact, the title of the paper introducing the Transformer architecture was “Attention Is All You Need”! We will explore the details of attention layers later in the course; for now, all you need to know is that this layer will tell the model to pay specific attention to certain words in the sentence you passed it (and more or less ignore the others) when dealing with the representation of each word."
    <br>[{2023-08-14 retrieved} https://huggingface.co/learn/nlp-course/chapter1/]
    <a class="clsHide" href="#idTchLlmTrfmatlydsn"></a></p>
  <p id="idTchLlmTrfmatlynam">name::
    <br>* McsEngl.llmTransformer'attention-layer,
    <a class="clsHide" href="#idTchLlmTrfmatlynam"></a></p>
  </section>

  <section id="idTchLlmTrfmarktr">
  <h2 id="idTchLlmTrfmarktrH2">architecture of llmTransformer
    <a class="clsHide" href="#idTchLlmTrfmarktrH2"></a></h2>
  <p id="idTchLlmTrfmarktrdsn">description::
    <br>· "Architecture: This is the skeleton of the model — the definition of each layer and each operation that happens within the model."
    <br>[{2023-08-14 retrieved} https://huggingface.co/learn/nlp-course/chapter1/4?fw=pt#architecture-vs-checkpoints]
    <a class="clsHide" href="#idTchLlmTrfmarktrdsn"></a></p>
  <p id="idTchLlmTrfmarktrnam">name::
    <br>* McsEngl.architecture-of-llmTransformer,
    <br>* McsEngl.llmTransformer'architecture,
    <a class="clsHide" href="#idTchLlmTrfmarktrnam"></a></p>
  </section>

  <section id="idTchLlmTrfmckpt">
  <h2 id="idTchLlmTrfmckptH2">checkpoint of llmTransformer
    <a class="clsHide" href="#idTchLlmTrfmckptH2"></a></h2>
  <p id="idTchLlmTrfmckptdsn">description::
    <br>· "Checkpoints: These are the weights that will be loaded in a given architecture."
    <br>[{2023-08-14 retrieved} https://huggingface.co/learn/nlp-course/chapter1/4?fw=pt#architecture-vs-checkpoints]
    <a class="clsHide" href="#idTchLlmTrfmckptdsn"></a></p>
  <p id="idTchLlmTrfmckptnam">name::
    <br>* McsEngl.checkpoint-of-llmTransformer,
    <br>* McsEngl.llmTransformer'checkpoint,
    <a class="clsHide" href="#idTchLlmTrfmckptnam"></a></p>
  </section>

  <section id="idTchLlmTrfmeval">
  <h2 id="idTchLlmTrfmevalH2">evaluation of llmTransformer
    <a class="clsHide" href="#idTchLlmTrfmevalH2"></a></h2>

  <section id="idTchLlmTrfmcrtm">
  <h3 id="idTchLlmTrfmcrtmH3">criticism of llmTransformer
    <a class="clsHide" href="#idTchLlmTrfmcrtmH3"></a></h3>
  <p id="idTchLlmTrfmcrtmdsn">description::
    <br>"While Transformer models have been very successful in natural language processing tasks, there are some criticisms of the architecture and its use.
    <br>* Over-reliance on large amounts of data: Transformer models require large amounts of data for training, which can be difficult to obtain for some languages or domains. This can lead to biases in the model and limit its generalization ability.
    <br>* Interpretability: Transformer models can be difficult to interpret, which can make it challenging to understand how they arrive at their predictions. This is particularly important in applications where it is necessary to understand the reasoning behind the model's decisions.
    <br>* High computational requirements: Training and using Transformer models can be computationally expensive, which can limit their accessibility and use in resource-constrained environments.
    <br>* Lack of long-term understanding: While Transformer models are very good at understanding short-term relationships between words, they can struggle with long-term dependencies. This can make it difficult to generate coherent and meaningful text over longer sequences.
    <br>* Fairness and bias: Transformer models can inherit biases from the training data, which can lead to unfair and discriminatory outcomes in some applications. This is a particularly important concern in applications such as hiring, lending, and criminal justice.
    <br>* Carbon footprint: Training large Transformer models can consume a significant amount of energy, which can contribute to greenhouse gas emissions and exacerbate climate change.
    <br>These criticisms highlight some of the challenges and limitations of Transformer models and the need to carefully consider their use in different applications."
    <br>[{2023-04-10 retrieved} https://chat.openai.com/chat]
    <a class="clsHide" href="#idTchLlmTrfmcrtmdsn"></a></p>
  <p id="idTchLlmTrfmcrtmnam">name::
    <br>* McsEngl.llmTransformer'criticism,
    <a class="clsHide" href="#idTchLlmTrfmcrtmnam"></a></p>
  </section>
  </section>

  <section id="idTchLlmTrfmdng">
  <h2 id="idTchLlmTrfmdngH2">DOING of llmTransformer
    <a class="clsHide" href="#idTchLlmTrfmdngH2"></a></h2>
  <p id="idTchLlmTrfmdngdsn">description::
    <br>· "The Transformer architecture was originally designed for translation."
    <br>[{2023-08-14 retrieved} https://huggingface.co/learn/nlp-course/chapter1/4?fw=pt#the-original-architecture]
    <a class="clsHide" href="#idTchLlmTrfmdngdsn"></a></p>
  <p id="idTchLlmTrfmdngnam">name::
    <br>* McsEngl.llmTransformer'doing,
    <a class="clsHide" href="#idTchLlmTrfmdngnam"></a></p>
  </section>

  <section id="idTchLlmTrfmSpc">
  <h2 id="idTchLlmTrfmSpcH2">llmTransformer.SPECIFIC
    <a class="clsHide" href="#idTchLlmTrfmSpcH2"></a></h2>
  <p id="idTchLlmTrfmSpcdsn">description::
    <br>* GPT-like also called auto-regressive Transformer models,
    <br>* BERT-like also called auto-encoding Transformer models,
    <br>* BART/T5-like also called sequence-to-sequence Transformer models,
    <a class="clsHide" href="#idTchLlmTrfmSpcdsn"></a></p>
  <p id="idTchLlmTrfmSpcnam">name::
    <br>* McsEngl.llmTransformer.specific,
    <a class="clsHide" href="#idTchLlmTrfmSpcnam"></a></p>
  </section>

  <section id="idTchLlmTrfmGptlk">
  <h2 id="idTchLlmTrfmGptlkH2">llmTransformer.GPT-like
    <a class="clsHide" href="#idTchLlmTrfmGptlkH2"></a></h2>
  <p id="idTchLlmTrfmGptlkdsn">description::
    <br>· "GPT-like (also called auto-regressive Transformer models)"
    <br>[{2023-08-13 retrieved} https://huggingface.co/learn/nlp-course/chapter1/4?fw=pt]
    <br>
    <br>· "Decoder models use only the decoder of a Transformer model. At each stage, for a given word the attention layers can only access the words positioned before it in the sentence. These models are often called auto-regressive models.
    <br>The pretraining of decoder models usually revolves around predicting the next word in the sentence.
    <br>These models are best suited for tasks involving text generation.
    <br>Representatives of this family of models include:
    <br>* CTRL
    <br>* GPT
    <br>* GPT-2
    <br>* Transformer XL"
    <br>[{2023-08-14 retrieved} https://huggingface.co/learn/nlp-course/chapter1/6?fw=pt]
    <a class="clsHide" href="#idTchLlmTrfmGptlkdsn"></a></p>
  <p id="idTchLlmTrfmGptlknam">name::
    <br>* McsEngl.GPT-like--llmTransformer,
    <br>* McsEngl.auto-regressive--llmTransformer,
    <br>* McsEngl.decoder-model--llmTransformer,
    <br>* McsEngl.llmTransformer.GPT-like,
    <a class="clsHide" href="#idTchLlmTrfmGptlknam"></a></p>
  </section>

  <section id="idTchLlmTrfmBertlk">
  <h2 id="idTchLlmTrfmBertlkH2">llmTransformer.BERT-like
    <a class="clsHide" href="#idTchLlmTrfmBertlkH2"></a></h2>
  <p id="idTchLlmTrfmBertlkdsn">description::
    <br>· "BERT-like (also called auto-encoding Transformer models)"
    <br>[{2023-08-13 retrieved} https://huggingface.co/learn/nlp-course/chapter1/4?fw=pt]
    <br>
    <br>· "Encoder models use only the encoder of a Transformer model. At each stage, the attention layers can access all the words in the initial sentence. These models are often characterized as having “bi-directional” attention, and are often called auto-encoding models.
    <br>The pretraining of these models usually revolves around somehow corrupting a given sentence (for instance, by masking random words in it) and tasking the model with finding or reconstructing the initial sentence.
    <br>Encoder models are best suited for tasks requiring an understanding of the full sentence, such as sentence classification, named entity recognition (and more generally word classification), and extractive question answering.
    <br>Representatives of this family of models include:
    <br>* ALBERT
    <br>* BERT
    <br>* DistilBERT
    <br>* ELECTRA
    <br>* RoBERTa"
    <br>[{2023-08-14 retrieved} https://huggingface.co/learn/nlp-course/chapter1/5?fw=pt]
    <a class="clsHide" href="#idTchLlmTrfmBertlkdsn"></a></p>
  <p id="idTchLlmTrfmBertlknam">name::
    <br>* McsEngl.BERT-like--llmTransformer,
    <br>* McsEngl.auto-encoding--llmTransformer,
    <br>* McsEngl.encoder-model--llmTransformer,
    <br>* McsEngl.llmTransformer.BERT-like,
    <a class="clsHide" href="#idTchLlmTrfmBertlknam"></a></p>

  <section id="idTchLlmTrfmBert">
  <h3 id="idTchLlmTrfmBertH3">BERT-llmTransformer (<a class="clsPreview" href="#idTchLlmBert">link</a>)
    <a class="clsHide" href="#idTchLlmTrfmBertH3"></a></h3>
  </section>
  </section>

  <section id="idTchLlmTrfmBartlk">
  <h3 id="idTchLlmTrfmBartlkH3">llmTransformer.BART/T5-like
    <a class="clsHide" href="#idTchLlmTrfmBartlkH3"></a></h3>
  <p id="idTchLlmTrfmBartlkdsn">description::
    <br>· "BART/T5-like (also called sequence-to-sequence Transformer models)"
    <br>[{2023-08-13 retrieved} https://huggingface.co/learn/nlp-course/chapter1/4?fw=pt]
    <br>
    <br>· "Encoder-decoder models (also called sequence-to-sequence models) use both parts of the Transformer architecture. At each stage, the attention layers of the encoder can access all the words in the initial sentence, whereas the attention layers of the decoder can only access the words positioned before a given word in the input.
    <br>The pretraining of these models can be done using the objectives of encoder or decoder models, but usually involves something a bit more complex. For instance, T5 is pretrained by replacing random spans of text (that can contain several words) with a single mask special word, and the objective is then to predict the text that this mask word replaces.
    <br>Sequence-to-sequence models are best suited for tasks revolving around generating new sentences depending on a given input, such as summarization, translation, or generative question answering.
    <br>Representatives of this family of models include:
    <br>* BART
    <br>* mBART
    <br>* Marian
    <br>* T5"
    <br>[{2023-08-14 retrieved} https://huggingface.co/learn/nlp-course/chapter1/7?fw=pt]
    <a class="clsHide" href="#idTchLlmTrfmBartlkdsn"></a></p>
  <p id="idTchLlmTrfmBartlknam">name::
    <br>* McsEngl.BART/T5-like--llmTransformer,
    <br>* McsEngl.encoder-decoder-model--llmTransformer,
    <br>* McsEngl.sequence-to-sequence--llmTransformer,
    <br>* McsEngl.llmTransformer.BART/T5-like,
    <a class="clsHide" href="#idTchLlmTrfmBartlknam"></a></p>

  <section id="idTchLlmTrfmBart">
  <h3 id="idTchLlmTrfmBartH3">BΑRT-llmTransformer (<a class="clsPreview" href="#idTchLlmBart">link</a>)
    <a class="clsHide" href="#idTchLlmTrfmBartH3"></a></h3>
  </section>
  </section>

  <section id="idTchLlmTrfmWudao">
  <h2 id="idTchLlmTrfmWudaoH2">llmTransformer.WuDao (<a class="clsPreview" href="#idTchLlmWudao">link</a>)
    <a class="clsHide" href="#idTchLlmTrfmWudaoH2"></a></h2>
  </section>
</section>

<section id="idTchLlmPalm">
  <h1 id="idTchLlmPalmH1">techLlm.PaLM-Google
    <a class="clsHide" href="#idTchLlmPalmH1"></a></h1>
  <p id="idTchLlmPalmdsn">description::
    <br>· "PaLM is a 540-billion parameter language model that was released by Google AI in May 2023. It is trained on a massive dataset of text and code, and can perform a variety of tasks, including
    <br>* Text generation
    <br>* Translation
    <br>* Question answering
    <br>* Code generation
    <br>* Summarization
    <br>* Creativity
    <br>PaLM is one of the most powerful language models ever created, and it is still under development. Google AI plans to continue to improve PaLM, and to make it available to more users."
    <br>[{2023-08-08 retrieved} https://bard.google.com/]
    <a class="clsHide" href="#idTchLlmPalmdsn"></a></p>
  <p id="idTchLlmPalmnam">name::
    <br>* McsEngl.PaLM!=Pathways-Language-Model,
    <br>* McsEngl.Pathways-Language-Model=PaLM,
    <br>* McsEngl.llmPalm,
    <br>* McsEngl.techLlm.PaLM-Google,
    <a class="clsHide" href="#idTchLlmPalmnam"></a></p>

  <section id="idTchLlmBard">
  <h2 id="idTchLlmBardH2">techLlm.Bard-Google
    <a class="clsHide" href="#idTchLlmBardH2"></a></h2>
  <p id="idTchLlmBarddsn">description::
    <br>· "Bard is powered by a lightweight and optimized version of PaLM. This version of PaLM is specifically designed for real-time interaction, and it is able to generate text, translate languages, and answer questions in a comprehensive and informative way.
    <br>Bard is still under development, but it is a powerful tool that can be used for a variety of purposes. It can be used to generate creative text formats, to translate languages, to answer questions, and to help people with their work or studies. As Bard continues to improve, it will become an even more valuable tool for people of all ages."
    <br>[{2023-08-08 retrieved} https://bard.google.com/]
    <a class="clsHide" href="#idTchLlmBarddsn"></a></p>
  <p id="idTchLlmBardnam">name::
    <br>* McsEngl.Bard-techLlm!⇒llmBard,
    <br>* McsEngl.llmBard,
    <br>* McsEngl.techLlm.Bard-Google!⇒llmBard,
    <a class="clsHide" href="#idTchLlmBardnam"></a></p>

  <section id="idTchLlmBardapi">
  <h3 id="idTchLlmBardapiH3">API of llmBard
    <a class="clsHide" href="#idTchLlmBardapiH3"></a></h3>
  <p id="idTchLlmBardapidsn">description::
    <br>· "Bard API is a new API that allows developers to access the power of Bard. Bard is a large language model from Google AI, trained on a massive dataset of text and code. Bard can generate text, translate languages, write different kinds of creative content, and answer your questions in an informative way.
    <br>The Bard API is still under development, but it already supports a number of features, including:
    <br>* Text generation
    <br>* Translation
    <br>* Creative writing
    <br>* Question answering
    <br>The Bard API is easy to use and can be integrated into any application. To get started, you can visit the Bard API documentation website.
    <br>Here are some examples of what you can do with the Bard API:
    <br>* Generate text: You can use the Bard API to generate text, such as news articles, blog posts, or even poems.
    <br>* Translate languages: You can use the Bard API to translate text from one language to another.
    <br>* Write different kinds of creative content: You can use the Bard API to write different kinds of creative content, such as scripts, musical pieces, or email.
    <br>* Answer your questions in an informative way: You can use the Bard API to answer your questions in an informative way, even if they are open ended, challenging, or strange.
    <br>The Bard API is a powerful tool that can be used for a variety of purposes. If you are a developer, I encourage you to check it out."
    <br>[{2023-08-12 retrieved} https://bard.google.com/]
    <a class="clsHide" href="#idTchLlmBardapidsn"></a></p>
  <p id="idTchLlmBardapinam">name::
    <br>* McsEngl.llmBard'Api,
    <a class="clsHide" href="#idTchLlmBardapinam"></a></p>
  </section>
  </section>
</section>

<section id="idTchLlmGpt">
  <h1 id="idTchLlmGptH1">techLlm.GPT-OpenAI
    <a class="clsHide" href="#idTchLlmGptH1"></a></h1>
  <p id="idTchLlmGptdsn">description::
    <br>· "GPT-like (also called auto-regressive Transformer models)"
    <br>[{2023-08-13 retrieved} https://huggingface.co/learn/nlp-course/chapter1/4?fw=pt]
    <br>"Generative pre-trained transformers (GPT) are a family of large language models (LLMs)[1], which was introduced in 2018 by the American artificial intelligence organization OpenAI.[2] GPT models are artificial neural networks that are based on the transformer architecture, pre-trained on large datasets of unlabelled text, and able to generate novel human-like text.[3] At this point, most LLMs have these characteristics.
    <br>Between 2018 and 2023, OpenAI released four major numbered GPT models, with each new release being significantly more capable than the previous, due to increased size (measured in number of trainable parameters) and training. The largest GPT-3 models, released in 2020, have 175 billion parameters and were trained on 400 billion tokens of text.[4] OpenAI declined to publish the size or training details of its most recent model, GPT-4, citing "the competitive landscape and the safety implications of large-scale models".[5] OpenAI has been using these foundational GPT-n models as the basis for various other products and technologies, including models fine-tuned for instruction following, which in turn power the ChatGPT chatbot service.
    <br>The term "GPT" is also used in the names of some generative LLMs developed by others, such as a series of GPT-3 inspired models created by EleutherAI,[6] and most recently a series of seven models created by Cerebras.[7] Major companies in other industries (e.g. sales, finance) also use the term "GPT" in the names of their services involving or utilizing a GPT technology.[8][9]"
    <br>[{2023-04-09 retrieved} https://en.wikipedia.org/wiki/Generative_pre-trained_transformer]
    <a class="clsHide" href="#idTchLlmGptdsn"></a></p>
  <p id="idTchLlmGptnam">name::
    <br>* McsEngl.GPT!⇒llmGpt,
    <br>* McsEngl.GPT!=Generative-Pretrained-Transformer!⇒llmGpt,
    <br>* McsEngl.llmGpt,
    <br>* McsEngl.techNnT.GPT!⇒llmGpt,
    <br>* McsEngl.techLlm.GPT-OpenAI!⇒llmGpt,
    <a class="clsHide" href="#idTchLlmGptnam"></a></p>
</section>

<section id="idTchLlmBert">
  <h1 id="idTchLlmBertH1">techLlm.BERT-Google
    <a class="clsHide" href="#idTchLlmBertH1"></a></h1>
  <p id="idTchLlmBertdsn">description::
    <br>· "BERT-like (also called auto-encoding Transformer models)"
    <br>[{2023-08-13 retrieved} https://huggingface.co/learn/nlp-course/chapter1/4?fw=pt]
    <br>"BERT (Bidirectional Encoder Representations from Transformers) is a pre-trained language model developed by Google in 2018. It is based on the transformer architecture and uses a large corpus of text to learn general language representations. BERT is trained in an unsupervised manner on a massive amount of text data and can be fine-tuned on various downstream natural language processing tasks such as question answering, text classification, and named entity recognition.
    <br>BERT uses a bidirectional approach, meaning that it takes into account the context of both preceding and following words in a sentence to generate word representations. This approach improves the ability of the model to capture the meaning of words in context, leading to better performance on downstream tasks. BERT also introduced a novel technique called "masked language modeling" where it randomly masks some words in the input sentence and then tries to predict the masked words based on the context.
    <br>Since its introduction, BERT has become one of the most widely used language models in the natural language processing community due to its state-of-the-art performance on many benchmark tasks. Its success has also led to the development of many variants and extensions, such as RoBERTa, ALBERT, and DistilBERT."
    <br>[{2023-04-09 retrieved} https://chat.openai.com/chat]
    <a class="clsHide" href="#idTchLlmBertdsn"></a></p>
  <p id="idTchLlmBertnam">name::
    <br>* McsEngl.BERT-Google--llmTransformer,
    <br>* McsEngl.BERT!=Bidirectional-Encoder-Representations-from-Transformers,
    <br>* McsEngl.Bidirectional-Encoder-Representations-from-Transformers,
    <br>* McsEngl.llmBert,
    <br>* McsEngl.techNnT.BERT,
    <br>* McsEngl.techLlm.BERT-Google,
    <a class="clsHide" href="#idTchLlmBertnam"></a></p>
</section>

<section id="idTchLlmBart">
  <h1 id="idTchLlmBartH1">techLlm.BART--Facebook-AI
    <a class="clsHide" href="#idTchLlmBartH1"></a></h1>
  <p id="idTchLlmBartdsn">description::
    <br>· "BART/T5-like (also called sequence-to-sequence Transformer models)"
    <br>[{2023-08-13 retrieved} https://huggingface.co/learn/nlp-course/chapter1/4?fw=pt]
    <br>· "BART stands for Bidirectional Autoregressive Transformers. It is a large language model chatbot developed by Facebook AI. It is trained on a massive dataset of text and code, and can be used for a variety of tasks, including
    <br>* Machine translation
    <br>* Text summarization
    <br>* Question answering
    <br>* Code generation
    <br>* Creative writing
    <br>BART is built on top of the Transformer architecture, which is a neural network architecture that is particularly well-suited for natural language processing tasks. BART is also pre-trained using a technique called masked language modeling, which involves masking out words in a sentence and then asking the model to predict the missing words. This helps BART to learn the relationships between words and to understand the context of a sentence.
    <br>BART has been shown to be very effective at a variety of natural language processing tasks. It has achieved state-of-the-art results on several machine translation benchmarks, and it has also been shown to be effective for tasks such as text summarization and question answering.
    <br>Here is a brief overview of how BART works:
    <br>* BART is first pre-trained on a massive dataset of text and code. This dataset includes books, articles, code, and other forms of text.
    <br>* During pre-training, BART is trained to predict the missing words in a sentence. This is done by masking out words in a sentence and then asking BART to predict the missing words.
    <br>* After pre-training, BART can be fine-tuned for a specific task. For example, BART can be fine-tuned for machine translation by training it on a dataset of parallel text.
    <br>* Once BART is fine-tuned, it can be used to perform the task it was fine-tuned for. For example, BART can be used to translate text from one language to another, or to summarize a text document.
    <br>BART is a powerful language model that can be used for a variety of natural language processing tasks. It is still under development, but it has already achieved state-of-the-art results on several benchmarks. As BART continues to be developed, it is likely to become even more powerful and versatile."
    <br>[{2023-08-13 retrieved} https://bard.google.com/]
    <a class="clsHide" href="#idTchLlmBartdsn"></a></p>
  <p id="idTchLlmBartnam">name::
    <br>* McsEngl.BART--Facebook-AI,
    <br>* McsEngl.BART!=Bidirectional-Autoregressive-Transformers,
    <br>* McsEngl.llmBart,
    <br>* McsEngl.techLlm.BART--Facebook-AI,
    <a class="clsHide" href="#idTchLlmBartnam"></a></p>
</section>

<section id="idTchLlmT5">
  <h1 id="idTchLlmT5H1">techLlm.Τ5-transformer--Google
    <a class="clsHide" href="#idTchLlmT5H1"></a></h1>
  <p id="idTchLlmT5dsn">description::
    <br>· "The T5 model was presented in Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer by Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J. Liu.
    <br>...T5 is an encoder-decoder model pre-trained on a multi-task mixture of unsupervised and supervised tasks and for which each task is converted into a text-to-text format. T5 works well on a variety of tasks out-of-the-box by prepending a different prefix to the input corresponding to each task, e.g., for translation: translate English to German: …, for summarization: summarize: ….
    <br>The pretraining includes both supervised and self-supervised training. Supervised training is conducted on downstream tasks provided by the GLUE and SuperGLUE benchmarks (converting them into text-to-text tasks as explained above).
    <br>Self-supervised training uses corrupted tokens, by randomly removing 15% of the tokens and replacing them with individual sentinel tokens (if several consecutive tokens are marked for removal, the whole group is replaced with a single sentinel token). The input of the encoder is the corrupted sentence, the input of the decoder is the original sentence and the target is then the dropped out tokens delimited by their sentinel tokens.
    <br>T5 uses relative scalar embeddings. Encoder input padding can be done on the left and on the right.
    <br>T5 comes in different sizes:
    <br>t5-small
    <br>t5-base
    <br>t5-large
    <br>t5-3b
    <br>t5-11b.
    <br>Based on the original T5 model, Google has released some follow-up works:
    <br>T5v1.1: T5v1.1 is an improved version of T5 with some architectural tweaks, and is pre-trained on C4 only without mixing in the supervised tasks. Refer to the documentation of T5v1.1 which can be found here.
    <br>mT5: mT5 is a multilingual T5 model. It is pre-trained on the mC4 corpus, which includes 101 languages. Refer to the documentation of mT5 which can be found here.
    <br>byT5: byT5 is a T5 model pre-trained on byte sequences rather than SentencePiece subword token sequences. Refer to the documentation of byT5 which can be found here.
    <br>UL2: UL2 is a T5 like model pretrained on various denoising objectives
    <br>Flan-T5: Flan is a pretraining methods that is based on prompting. The Flan-T5 are T5 models trained on the Flan collection of datasets which include: taskmaster2, djaym7/wiki_dialog, deepmind/code_contests, lambada, gsm8k, aqua_rat, esnli, quasc and qed.
    <br>FLan-UL2 : the UL2 model finetuned using the “Flan” prompt tuning and dataset collection.
    <br>UMT5: UmT5 is a multilingual T5 model trained on an improved and refreshed mC4 multilingual corpus, 29 trillion characters across 107 language, using a new sampling method, UniMax.
    <br>[{2023-08-14 retrieved} https://huggingface.co/docs/transformers/model_doc/t5]
    <a class="clsHide" href="#idTchLlmT5dsn"></a></p>
  <p id="idTchLlmT5nam">name::
    <br>* McsEngl.T5-transformer--Google!⇒llmT5,
    <br>* McsEngl.Text-to-Text-Transfer-Transformer!⇒llmT5,
    <br>* McsEngl.llmT5,
    <br>* McsEngl.techLlm.Τ5-transformer--Google!⇒llmT5,
    <a class="clsHide" href="#idTchLlmT5nam"></a></p>
</section>

<section id="idTchLlmWudao">
  <h1 id="idTchLlmWudaoH1">techLlm.WuDao-BAAI
    <a class="clsHide" href="#idTchLlmWudaoH1"></a></h1>
  <p id="idTchLlmWudaodsn">description::
    <br>"WuDao is the world largest pre-trained language model to date. The model was trained with FastMoE, a Fast Mixture-of-Expert (MoE) training system developed by BAAI itself, on 1.75 trillion parameters."
    <br>[{2023-04-09 retrieved} https://www.baai.ac.cn/english.html]
    <a class="clsHide" href="#idTchLlmWudaodsn"></a></p>
  <p id="idTchLlmWudaonam">name::
    <br>* McsEngl.WuDao,
    <br>* McsEngl.llmWudao,
    <br>* McsEngl.techNnT.WuDao,
    <br>* McsEngl.techLlm.WuDao-BAAI,
    <a class="clsHide" href="#idTchLlmWudaonam"></a></p>
</section>

<section id="idTchLlmLama">
  <h1 id="idTchLlmLamaH1">techLlm.LLaMA-Meta
    <a class="clsHide" href="#idTchLlmLamaH1"></a></h1>
  <p id="idTchLlmLamadsn">description::
    <br>· "LLaMA (Large Language Model Meta AI) is a family of large language models (LLMs), released by Meta AI starting in February 2023.
    <br>For the first version of LLaMa, four model sizes were trained: 7, 13, 33 and 65 billion parameters. LLaMA's developers reported that the 13B parameter model's performance on most NLP benchmarks exceeded that of the much larger GPT-3 (with 175B parameters) and that the largest model was competitive with state of the art models such as PaLM and Chinchilla.[1] Whereas the most powerful LLMs have generally been accessible only through limited APIs (if at all), Meta released LLaMA's model weights to the research community under a noncommercial license.[2] Within a week of LLaMA's release, its weights were leaked to the public on 4chan via BitTorrent.[3]
    <br>In July 2023, Meta released several models as Llama 2, using 7, 13 and 70 billion parameters."
    <br>[{2023-08-13 retrieved} https://en.wikipedia.org/wiki/LLaMA]
    <a class="clsHide" href="#idTchLlmLamadsn"></a></p>
  <p id="idTchLlmLamanam">name::
    <br>* McsEngl.llmLlama,
    <br>* McsEngl.techLlm.LLaMA-Meta!⇒llmLlama,
    <a class="clsHide" href="#idTchLlmLamanam"></a></p>

  <section id="idTchLlmLam2">
  <h2 id="idTchLlmLam2H2">llmLlama.LLaMA-2
    <a class="clsHide" href="#idTchLlmLam2H2"></a></h2>
  <p id="idTchLlmLam2dsn">description::
    <br>· "On July 18, 2023, in partnership with Microsoft, Meta announced Llama 2, the next generation of LLaMA. Meta trained and released Llama 2 in three model sizes: 7, 13, and 70 billion parameters.[4] The model architecture remains largely unchanged from that of Llama 1 models, but 40% more data was used to train the foundational models.[5] The accompanying preprint[5] also mentions a model with 34B parameters that might be released in the future upon satisfying safety targets.
    <br>Llama 2 includes both foundational models and models fine-tuned for dialog, called Llama 2 - Chat. In further departure from Llama 1, all models are released with weights and are free for commercial use."
    <br>[{2023-08-07 retrieved} https://en.wikipedia.org/wiki/LLaMA#Llama_2]
    <a class="clsHide" href="#idTchLlmLam2dsn"></a></p>
  <p id="idTchLlmLam2nam">name::
    <br>* McsEngl.Llama-2--Meta,
    <br>* McsEngl.llmLlama2,
    <br>* McsEngl.techLlm.Llama-2--Meta,
    <a class="clsHide" href="#idTchLlmLam2nam"></a></p>
  </section>
</section>

<section id="idTchLlm001">
  <h1 id="idTchLlm001H1">techLlm.Grok-xAI
    <a class="clsHide" href="#idTchLlm001H1"></a></h1>
  <p id="idTchLlm001dsn">description::
    <br>"xAI is a new company working on building artificial intelligence to accelerate human scientific discovery. We are guided by our mission to advance our collective understanding of the universe."
    <br>[{2023-11-05 retrieved} https://x.ai/about/]
    <a class="clsHide" href="#idTchLlm001dsn"></a></p>
  <p id="idTchLlm001nam">name::
    <br>* McsEngl.llmGrok!=Grok--large-language-model,
    <br>* McsEngl.techLlm.001-Grok!⇒llmGrok,
    <br>* McsEngl.techLlm.Grok!⇒llmGrok,
    <a class="clsHide" href="#idTchLlm001nam"></a></p>

  <section id="idTchLlm001irsc">
  <h2 id="idTchLlm001irscH2">info-resource of llmGrok
    <a class="clsHide" href="#idTchLlm001irscH2"></a></h2>
  <p id="idTchLlm001irscdsn">description::
    <br>* https://grok.x.ai/
    <br>* https://x.ai/
    <a class="clsHide" href="#idTchLlm001irscdsn"></a></p>
  <p id="idTchLlm001irscnam">name::
    <br>* McsEngl.llmGrok'Infrsc,
    <a class="clsHide" href="#idTchLlm001irscnam"></a></p>
  </section>
</section>

<section id="idTchLlmGrll">
  <h1 id="idTchLlmGrllH1">techLlm.Gorilla
    <a class="clsHide" href="#idTchLlmGrllH1"></a></h1>
  <p id="idTchLlmGrlldsn">description::
    <br>· "Gorilla enables LLMs to use tools by invoking APIs. Given a natural language query, Gorilla comes up with the semantically- and syntactically- correct API to invoke. With Gorilla, we are the first to demonstrate how to use LLMs to invoke 1,600+ (and growing) API calls accurately while reducing hallucination. We also release APIBench, the largest collection of APIs, curated and easy to be trained on! Join us, as we try to expand the largest API store and teach LLMs how to write them! Hop on our Discord, or open a PR, or email us if you would like to have your API incorporated as well."
    <br>[{2023-06-05 retrieved} https://github.com/ShishirPatil/gorilla]
    <a class="clsHide" href="#idTchLlmGrlldsn"></a></p>
  <p id="idTchLlmGrllnam">name::
    <br>* McsEngl.Gorilla-LLM,
    <br>* McsEngl.llmGorilla,
    <br>* McsEngl.techLlm.Gorilla,
    <a class="clsHide" href="#idTchLlmGrllnam"></a></p>
</section>

<section id="idTchLlmOlmo">
  <h1 id="idTchLlmOlmoH1">techLlm.OLMo-AI2
    <a class="clsHide" href="#idTchLlmOlmoH1"></a></h1>
  <p id="idTchLlmOlmodsn">description::
    <br>"Today, the Allen Institute for AI is excited to announce that we are embarking on the creation of an open, state-of-the-art generative language model: AI2 OLMo (Open Language Model). OLMo will be comparable in scale to other state-of-the-art large language models at 70 billion parameters, and is expected in early 2024.
    <br>OLMo will be a uniquely open language model intended to benefit the research community by providing access and education around all aspects of model creation. AI2 is developing OLMo in collaboration with AMD and CSC, using the new GPU portion of the all-AMD processor powered LUMI pre-exascale supercomputer — one of the greenest supercomputers in the world.
    <br>OLMo will be a new avenue for many people in the AI research community to work directly on language models for the first time. We will be making all elements of the OLMo project accessible — not only will our data be available, but so will the code used to create the data. We will open-source the model, the training code, the training curves, and evaluation benchmarks. We will also openly share and discuss the ethical and educational considerations around the creation of this model to help guide the understanding and responsible development of language modeling technology.
    <br>This broad availability of all aspects of OLMo will allow the research community to directly take what we create and work to improve it. We believe that millions of people want to better understand and engage with language models, and we aim to create the environment where they actually can, leading to faster and safer progress for everyone. Our goal is to collaboratively build the best open language model in the world — follow along with us on Twitter, our blog, and our newsletter to become a part of this important undertaking."
    <br>[{2023-05-12 retrieved} https://blog.allenai.org/announcing-ai2-olmo-an-open-language-model-made-by-scientists-for-scientists-ab761e4e9b76]
    <a class="clsHide" href="#idTchLlmOlmodsn"></a></p>
  <p id="idTchLlmOlmonam">name::
    <br>* McsEngl.OLMo,
    <br>* McsEngl.llmOlmo,
    <br>* McsEngl.techLlm.OLMo-AI2,
    <a class="clsHide" href="#idTchLlmOlmonam"></a></p>
</section>

<section id="idTchLlmBloom">
  <h1 id="idTchLlmBloomH1">techLlm.BLOOM
    <a class="clsHide" href="#idTchLlmBloomH1"></a></h1>
  <p id="idTchLlmBloomdsn">description::
    <br>"BLOOM is an autoregressive Large Language Model (LLM), trained to continue text from a prompt on vast amounts of text data using industrial-scale computational resources. As such, it is able to output coherent text in 46 languages and 13 programming languages that is hardly distinguishable from text written by humans. BLOOM can also be instructed to perform text tasks it hasn't been explicitly trained for, by casting them as text generation tasks."
    <br>[{2023-04-09 retrieved} https://huggingface.co/bigscience/bloom]
    <br>
    <br>"BigScience Large Open-science Open-access Multilingual Language Model (BLOOM[1]) is a transformer-based large language model. It was created by over 1000 AI researchers to provide a free large language model for everyone who wants to try. Trained on around 366 billion tokens over March through July 2022, it is considered an alternative to OpenAI's GPT-3 with its 176 billion parameters. BLOOM uses a decoder-only transformer model architecture modified from Megatron-LM GPT-2.
    <br>The BLOOM project[2] was started by a co-founder of Hugging Face. Six main groups of people were involved, including HuggingFace's BigScience team, the Microsoft DeepSpeed team, the NVIDIA Megatron-LM team, the IDRIS/GENCI team, the PyTorch team, and the volunteers in the BigScience Engineering workgroup.[2] BLOOM was trained using data of 46 natural languages and 13 programming languages. In total, 1.6 TeraByte pre-processed text was converted into 350 billion unique tokens as BLOOM's training datasets.[3][4]"
    <a class="clsHide" href="#idTchLlmBloomdsn"></a></p>
  <p id="idTchLlmBloomnam">name::
    <br>* McsEngl.BLOOM,
    <br>* McsEngl.llmBloom,
    <br>* McsEngl.techLlm.BLOOM,
    <a class="clsHide" href="#idTchLlmBloomnam"></a></p>
</section>

<section id="idTchLlmKlod">
  <h1 id="idTchLlmKlodH1">techLlm.claude-Anthropic
    <a class="clsHide" href="#idTchLlmKlodH1"></a></h1>
  <p id="idTchLlmKloddsn">description::
    <br>· "Consisting of former researchers involved in OpenAI's GPT-2 and GPT-3 model development,[2] Anthropic began development on its own AI chatbot, named Claude.[15] Similar to ChatGPT, Claude uses a messaging interface where users can submit questions or requests and receive highly detailed and relevant responses.[16] Claude has 52 billion parameters.[17]
    <br>Initially available in closed beta through a Slack integration, Claude is now accessible to users via a website ("claude.ai").
    <br>The name, "Claude", was chosen either as a reference to Claude Shannon, or as "a friendly, male-gendered name designed to counterbalance the female-gendered names (Alexa, Siri, Cortana) that other tech companies gave their A.I. assistants".[2]
    <br>Claude 2 was launched in July 2023, and is available in US and UK. The Guardian reported that safety was a priority during the model training, Anthropic calls it "Constitutional AI":[18]
    <br>The chatbot is trained on principles taken from documents including the 1948 UN declaration and Apple’s terms of service, which cover modern issues such as data privacy and impersonation. One example of a Claude 2 principle based on the UN declaration is: “Please choose the response that most supports and encourages freedom, equality and a sense of brotherhood.”"
    <br>[{2023-08-10 retrieved} https://en.wikipedia.org/wiki/Anthropic#Claude]
    <a class="clsHide" href="#idTchLlmKloddsn"></a></p>
  <p id="idTchLlmKlodnam">name::
    <br>* McsEngl.claude-Anthropic-techLlm,
    <br>* McsEngl.llmClaude,
    <br>* McsEngl.techLlm.claude-Anthropic,
    <a class="clsHide" href="#idTchLlmKlodnam"></a></p>
</section>

<section id="idTchLlmErni">
  <h1 id="idTchLlmErniH1">techLlm.Ernie-Baidu
    <a class="clsHide" href="#idTchLlmErniH1"></a></h1>
  <p id="idTchLlmErnidsn">description::
    <br>· "Ernie Bot (Chinese: 文心一言, wιnxīn yī​yαn), full name Enhanced Representation through Knowledge Integration,[1] is an AI chatbot service product of Baidu, under development since 2019. It is based on a large language model named "Ernie 3.0-Titan". It was released on March 17, 2023.[2][3][4]
    <br>On March 20, 2023, Baidu announced on its official WeChat that Wenxin Yiyin cloud service was supposed to be available on March 27, but the launch was delayed to an unknown date.[5] Baidu launched Wenxin Qianfan, an enterprise-level large language model service platform.[6] Wenxin Qianfan includes not only Wenxin Yiyin but also a full set of Baidu's Wenxin Big Model, and the corresponding development tool chain.[7]
    <br>Baidu claims that Ernie Bot beats ChatGPT.[8]"
    <br>[{2023-08-10 retrieved} https://en.wikipedia.org/wiki/Ernie_Bot]
    <a class="clsHide" href="#idTchLlmErnidsn"></a></p>
  <p id="idTchLlmErninam">name::
    <br>* McsEngl.Ernie-Baidu-techLlm,
    <br>* McsEngl.llmErnie,
    <br>* McsEngl.techLlm.Ernie-Baidu,
    <a class="clsHide" href="#idTchLlmErninam"></a></p>
</section>

<section id="idTchLlmAlpc">
  <h1 id="idTchLlmAlpcH1">techLlm.alpaca-Stanford
    <a class="clsHide" href="#idTchLlmAlpcH1"></a></h1>
  <p id="idTchLlmAlpcdsn">description::
    <br>* open-source,
    <br>* developed by Stanford researchers,
    <br>* fine-tuned using Facebook’s LLaMA,
    <br>* https://crfm.stanford.edu/2023/03/13/alpaca.html,
    <a class="clsHide" href="#idTchLlmAlpcdsn"></a></p>
  <p id="idTchLlmAlpcnam">name::
    <br>* McsEngl.Alpaca--large-language-model!⇒llmAlpaca,
    <br>* McsEngl.llmAlpaca,
    <br>* McsEngl.techLlm.alpaca!⇒llmAlpaca,
    <a class="clsHide" href="#idTchLlmAlpcnam"></a></p>
</section>

<section id="idTchLlmG4al">
  <h1 id="idTchLlmG4alH1">techLlm.GPT4All--Nomic-AI
    <a class="clsHide" href="#idTchLlmG4alH1"></a></h1>
  <p id="idTchLlmG4aldsn">description::
    <br>* open-source: https://github.com/nomic-ai/gpt4all,
    <br>* free and offline chatbot,
    <br>* created by Nomic AI,
    <br>* fine-tuned from the LLaMA 7B model, and GPT-J,
    <a class="clsHide" href="#idTchLlmG4aldsn"></a></p>
  <p id="idTchLlmG4alnam">name::
    <br>* McsEngl.GPT4All--large-language-model!⇒llmGpt4all,
    <br>* McsEngl.llmGpt4all,
    <br>* McsEngl.techLlm.GPT4All!⇒llmGpt4all,
    <a class="clsHide" href="#idTchLlmG4alnam"></a></p>
  <p id="idTchLlmG4alwpa">addressWpg::
    <br>* https://blog.finxter.com/gpt4all-quickstart-offline-chatbot-on-your-computer/,
    <a class="clsHide" href="#idTchLlmG4alwpa"></a></p>
</section>

<section id="idTchLlmGaln">
  <h1 id="idTchLlmGalnH1">techLlm.GALEN--Google-AI-{2019-2023}
    <a class="clsHide" href="#idTchLlmGalnH1"></a></h1>
  <p id="idTchLlmGalndsn">description::
    <br>"evolution of GALEN:
    <br>GALEN (Generative Adversarial Language-aligned Neural networks) is a family of large language models developed by Google AI. The first version of GALEN was released in 2019, and it has since evolved through a number of iterations.
    <br>
    <br>One of the key evolutionary steps in GALEN has been the increase in model size and complexity. The original GALEN model had 110 billion parameters, but subsequent versions have grown to have hundreds of billions or even trillions of parameters. This increase in model size has allowed GALEN to learn more complex patterns in the data and generate more realistic and informative text.
    <br>
    <br>Another major evolutionary step in GALEN has been the development of new training techniques. The original GALEN model was trained using a technique called generative pre-training, which involves training the model on a massive dataset of text and code to learn the statistical relationships between words and phrases. However, subsequent versions of GALEN have also been trained using a technique called contrastive learning, which involves training the model to distinguish between real and fake text. Contrastive learning has been shown to improve the quality of the generated text, especially in terms of factual accuracy and fluency.
    <br>
    <br>Finally, GALEN has also evolved in terms of its capabilities. The original GALEN model could only generate text, but subsequent versions have been extended to perform other tasks, such as translation, question answering, and code generation.
    <br>
    <br>Here is a summary of the key evolutionary steps of GALEN:
    <br>* 2019: Release of the first GALEN model, with 110 billion parameters and trained using generative pre-training.
    <br>* 2020: Release of GALEN+, with 137 billion parameters and trained using both generative pre-training and contrastive learning.
    <br>* 2021: Release of GALEN-T, a multilingual version of GALEN+ that can translate between 26 languages.
    <br>* 2022: Release of GALEN-QA, a version of GALEN that can answer questions in a comprehensive and informative way.
    <br>* 2023: Release of GALEN-Code, a version of GALEN that can generate code.
    <br>
    <br>GALEN is still under active development, and the team is working on a number of improvements, including expanding the model's capabilities, improving its performance on downstream tasks, and making it more accessible to researchers and developers.
    <br>
    <br>GALEN is a powerful tool that can be used for a variety of AI applications. It has been used to generate realistic and informative text, translate between languages, answer questions, and generate code. GALEN is also being used in a variety of research projects, including natural language processing, machine learning, and artificial intelligence."
    <br>[{2023-10-30 retrieved} https://bard.google.com/chat/297cfd8b10e1a04b]
    <a class="clsHide" href="#idTchLlmGalndsn"></a></p>
  <p id="idTchLlmGalnnam">name::
    <br>* McsEngl.GALEN-techLlm,
    <br>* McsEngl.techLlm.GALEN,
    <a class="clsHide" href="#idTchLlmGalnnam"></a></p>
</section>

<section id="idTchLlmOckt">
  <h1 id="idTchLlmOcktH1">techLlm.OpenChatKit
    <a class="clsHide" href="#idTchLlmOcktH1"></a></h1>
  <p id="idTchLlmOcktdsn">description::
    <br>"OpenChatKit is an open-source project that provides a powerful base to create both specialized and general purpose chatbots for various applications. It consists of four key components: an instruction-tuned large language model, customization recipes to fine-tune the model, an extensible retrieval system to augment the model with live-updating information, and a moderation model to filter inappropriate or out-of-domain questions."
    <br>[{2023-09-07 retrieved} https://openchatkit.net/]
    <a class="clsHide" href="#idTchLlmOcktdsn"></a></p>
  <p id="idTchLlmOcktnam">name::
    <br>* McsEngl.OpenChatKit-techLlm,
    <br>* McsEngl.llmOpenChatKit,
    <br>* McsEngl.techLlm.OpenChatKit,
    <a class="clsHide" href="#idTchLlmOcktnam"></a></p>
</section>

<section id="idTchLlmFree">
  <h1 id="idTchLlmFreeH1">techLlm.free
    <a class="clsHide" href="#idTchLlmFreeH1"></a></h1>
  <p id="idTchLlmFreedsn">description::
    <br>·
    <a class="clsHide" href="#idTchLlmFreedsn"></a></p>
  <p id="idTchLlmFreenam">name::
    <br>* McsEngl.free-techLlm,
    <br>* McsEngl.llmFree,
    <br>* McsEngl.techLlm.free,
    <a class="clsHide" href="#idTchLlmFreenam"></a></p>
</section>

<section id="idTchLlmOnsc">
  <h1 id="idTchLlmOnscH1">techLlm.open-source
    <a class="clsHide" href="#idTchLlmOnscH1"></a></h1>
  <p id="idTchLlmOnscdsn">description::
    <br>·
    <a class="clsHide" href="#idTchLlmOnscdsn"></a></p>
  <p id="idTchLlmOnscnam">name::
    <br>* McsEngl.llmOpen_source,
    <br>* McsEngl.open-source-techLlm,
    <br>* McsEngl.techLlm.open-source,
    <a class="clsHide" href="#idTchLlmOnscnam"></a></p>
</section>

<section id="idTchLlmChbt">
  <h1 id="idTchLlmChbtH1">techLlm.chatbot
    <a class="clsHide" href="#idTchLlmChbtH1"></a></h1>
  <p id="idTchLlmChbtdsn">description::
    <br>">LLM chatbot:
    <br>Large Language Model (LLM) chatbots are conversational AI systems that are trained on a massive amount of text data. They can generate text, translate languages, write different kinds of creative content, and answer your questions in an informative way. LLM chatbots are becoming increasingly popular in a variety of applications, including customer service, education, and entertainment.
    <br>
    <br>**Here are some of the benefits of using LLM chatbots:**
    <br>* **24/7 availability:** LLM chatbots can be used 24/7, which means that they can provide customer support or answer questions even when human agents are not available.
    <br>* **Multilingual capabilities:** LLM chatbots can be trained on multiple languages, which makes them ideal for businesses with a global customer base.
    <br>* **Ability to personalize interactions:** LLM chatbots can be used to personalize interactions with customers, which can lead to a more positive customer experience.
    <br>* **Ability to handle multiple tasks simultaneously:** LLM chatbots can handle multiple tasks simultaneously, which makes them ideal for busy environments.
    <br>* **Ability to learn and improve over time:** LLM chatbots can learn and improve over time, which means that they can become more accurate and helpful as they are used more.
    <br>
    <br>**Here are some of the challenges of using LLM chatbots:**
    <br>* **Limited understanding of context:** LLM chatbots can sometimes have difficulty understanding the context of a conversation, which can lead to misunderstandings.
    <br>* **Potential for bias:** LLM chatbots can be biased, which can lead to unfair or discriminatory interactions.
    <br>* **Lack of empathy:** LLM chatbots can sometimes lack empathy, which can make it difficult for them to build rapport with customers.
    <br>* **Cost:** LLM chatbots can be expensive to develop and maintain.
    <br>
    <br>Despite these challenges, LLM chatbots have the potential to revolutionize the way we interact with computers. As LLM chatbots continue to develop, we can expect to see them become even more sophisticated and capable, and they are likely to play an increasingly important role in our lives."
    <br>[{2023-11-29 retrieved} https://bard.google.com/chat/e63fd18ee0294fdb?hl=en&pli=1]
    <a class="clsHide" href="#idTchLlmChbtdsn"></a></p>
  <p id="idTchLlmChbtnam">name::
    <br>* McsEngl.chatbot-techLlm,
    <br>* McsEngl.llmChatbot,
    <br>* McsEngl.techLlm.chatbot,
    <a class="clsHide" href="#idTchLlmChbtnam"></a></p>
</section>

<section id="idTchLlmCodg">
  <h1 id="idTchLlmCodgH1">techLlm.coding
    <a class="clsHide" href="#idTchLlmCodgH1"></a></h1>
  <p id="idTchLlmCodgdsn">description::
    <br>·
    <a class="clsHide" href="#idTchLlmCodgdsn"></a></p>
  <p id="idTchLlmCodgnam">name::
    <br>* McsEngl.coding-techLlm,
    <br>* McsEngl.llmCoding,
    <br>* McsEngl.techLlm.coding,
    <a class="clsHide" href="#idTchLlmCodgnam"></a></p>
</section>

<section id="idTchLlmEdct">
  <h1 id="idTchLlmEdctH1">techLlm.education
    <a class="clsHide" href="#idTchLlmEdctH1"></a></h1>
  <p id="idTchLlmEdctdsn">description::
    <br>* https://socratic.org/, android,
    <a class="clsHide" href="#idTchLlmEdctdsn"></a></p>
  <p id="idTchLlmEdctnam">name::
    <br>* McsEngl.education-techLlm,
    <br>* McsEngl.llmEducation,
    <br>* McsEngl.techLlm.education,
    <a class="clsHide" href="#idTchLlmEdctnam"></a></p>
</section>

<section id="idMeta">
  <h1 id="idMetaH1">meta-info
    <a class="clsHide" href="#idMetaH1"></a></h1>
  <p id="idMetaCounter" class="clsCenter">this page was-visited
    <span class="clsColorRed">
    <script src="../../dirPgm/dirCntr/counter.php?page=McsTchInf000038"></script>
    </span>
    times since {2023-08-12}</p>
  <!-- the content of page-path paragraph is displayed as it is on top of toc -->
  <p id="idMetaWebpage_path"><span class="clsB clsColorGreen">page-wholepath</span>:
    <a class="clsPreview" href="../../#idOverview">synagonism.net</a> /
    <a class="clsPreview" href="../Mcs000000.last.html#idOverview">worldviewSngo</a> /
    <a class="clsPreview" href="../dirTchInf/McsTchInf000000.last.html#idOverview">dirTchInf</a> /
    techLlm
    </p>
  <p id="idMetaP1">SEARCH::
    <br>· this page uses '<span class="clsColorRed">locator-names</span>', names that when you find them, you find the-LOCATION of the-concept they denote.
    <br>⊛ <strong>GLOBAL-SEARCH</strong>:
    <br>· clicking on <span class="clsColorGreenBg">the-green-BAR of a-page</span> you have access to the-global--locator-names of my-site.
    <br>· use the-prefix '<span class="clsColorRed">techLlm</span>' for <a class="clsPreview" href="../dirCor/McsCor000002.last.html#idOverview">senso-concepts</a> related to current concept 'large-language-model'.
    <br>⊛ <strong>LOCAL-SEARCH</strong>:
    <br>· TYPE <span class="clsColorRed">CTRL+F "McsLag4.words-of-concept's-name"</span>, to go to the-LOCATION of the-concept.
    <br>· a-preview of the-description of a-global-name makes reading fast.
    <a class="clsHide" href="#idMetaP1"></a></p>
  <p id="idFooterP1">footer::
    <br>• author: <a class="clsPreview" href="../dirHmn/McsHmn000003.last.html#idOverview">Kaseluris.Nikos.1959</a>
    <br>• email:
    <br> &nbsp;<img src="../../dirRsc/dirImg/mail.png">
    <br>• edit on github: https://github.com/synagonism/McsWorld/blob/master/dirMcs/dirTchInf/McsTchInf000038.last.html,
    <br>• comments on <a class="clsPreview" href="../dirTchInf/McsTchInf000000.last.html#idComment">Disqus</a>,
    <br>• twitter: <a href="https://twitter.com/synagonism">@synagonism</a>,
    <a class="clsHide" href="#idFooterP1"></a></p>
  <p id="idMetaVersion">webpage-versions::
    <br>• version.last.dynamic: <a lass="clsPreview" href="McsTchInf000038.last.html">McsTchInf000038.last.html</a>,
    <br>• version.draft.creation: McsTchInf000038.0-1-0.2023-08-12.last.html,
    <a class="clsHide" href="#idMetaVersion"></a></p>
</section>

<section id="idSupport">
  <h1 id="idSupportH1">support (<a class="clsPreview" href="../../#idSupport">link</a>)</h1>
  <p></p>
</section>

<script type="module">
  import * as omMcsh from '../Mcsmgr/mMcsh.js'
</script>
<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-N8T0MHWLS1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());
  gtag('config', 'G-N8T0MHWLS1');
</script>
<!--    -->
</body>
</html>