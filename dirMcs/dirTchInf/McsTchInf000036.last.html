<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Mcs.techAi-(McsTchInf000036.0-12-0.2023-08-23 draft) artificial-intelligence</title>
  <meta name="keywords" content="artificial-intelligence, techAi, ModelConceptSensorial, McsHitp, Synagonism">
  <link rel="stylesheet" href="../Mcsmgr/mHitp.css">
</head>

<body>
<header id="idHeader">
  <p></p>
  <h1 id="idHeaderH1">artificial-intelligence
    <br>senso-concept-Mcs (techAi)
    </h1>
  <p id="idHeadercrd">McsHitp-creation:: {2023-07-30}
    <a class="clsHide" href="#idHeadercrd"></a></p>
</header>

<section id="idOverview">
  <h1 id="idOverviewH1">overview of techAi
    <a class="clsHide" href="#idOverviewH1"></a></h1>
  <p id="idDescription">description::
    <br>· techAi is techInfo with <a class="clsPreview" href="../dirCor/McsCor000011.last.html#idSysMng001dngF">intelligence</a> (= doing of organism's managing-sys).
    <a class="clsHide" href="#idDescription"></a></p>
  <p id="idName">name::
    <br>* McsEngl.McsTchInf000036.last.html//dirTchInf//dirMcs!⇒techAi,
    <br>* McsEngl.dirTchInf/McsTchInf000036.last.html!⇒techAi,
    <br>* McsEngl.AI!=artificial-intelligence!⇒techAi,
    <br>* McsEngl.artificial-intelligence!⇒techAi,
    <br>* McsEngl.human-level-AI!⇒techAi,
    <br>* McsEngl.sciAi!⇒techAi,
    <br>* McsEngl.strong-AI!⇒techAi,
    <br>* McsEngl.techAi,
    <br>* McsEngl.techAi!=artificial-intelligence-tech!⇒techAi,
    <br>* McsEngl.techInfo.007-artificial-intelligence!⇒techAi,
    <br>* McsEngl.techInfo.artificial-intelligence!⇒techAi,
    <br>====== langoGreek:
    <br>* McsElln.ΤΑ-τεχνητή-νοημοσύνη!=techAi,
    <br>* McsElln.τεχνητή-νοημοσύνη!=techAi,
    <a class="clsHide" href="#idName"></a></p>
</section>

<section id="idTechAiitlc">
  <h1 id="idTechAiitlcH1">intelligence (<a class="clsPreview" href="../dirCor/McsCor000018.last.html#idBrngHmn007">link</a>) of techAi
    <a class="clsHide" href="#idTechAiitlcH1"></a></h1>
</section>

<section id="idTechAieval">
  <h1 id="idTechAievalH1">evaluation of techAi
    <a class="clsHide" href="#idTechAievalH1"></a></h1>
  <p id="idTechAievaldsn">description::
    <br>· "AI is incredibly smart and shockingly stupid"
    <br>[{2023-08-07 retrieved} https://twitter.com/TEDAI2023/status/1687961952352342016]
    <a class="clsHide" href="#idTechAievaldsn"></a></p>
  <p id="idTechAievalnam">name::
    <br>* McsEngl.techAi'evaluation,
    <a class="clsHide" href="#idTechAievalnam"></a></p>

  <section id="idTechAisfty">
  <h2 id="idTechAisftyH2">safty of techAi
    <a class="clsHide" href="#idTechAisftyH2"></a></h2>
  <p id="idTechAisftydsn">description::
    <br>"AI safety is an interdisciplinary field concerned with preventing accidents, misuse, or other harmful consequences that could result from artificial intelligence (AI) systems. It encompasses machine ethics and AI alignment, which aim to make AI systems moral and beneficial, and AI safety encompasses technical problems including monitoring systems for risks and making them highly reliable. Beyond AI research, it involves developing norms and policies that promote safety."
    <br>[{2023-04-10 retrieved} https://en.wikipedia.org/wiki/AI_safety]
    <a class="clsHide" href="#idTechAisftydsn"></a></p>
  <p id="idTechAisftynam">name::
    <br>* McsEngl.techAi'risk,
    <br>* McsEngl.techAi'safty,
    <a class="clsHide" href="#idTechAisftynam"></a></p>
  </section>

  <section id="idTechAietcs">
  <h2 id="idTechAietcsH2">ethics of techAi
    <a class="clsHide" href="#idTechAietcsH2"></a></h2>
  <p id="idTechAietcsdsn">description::
    <br>· Transparency, accountability, and open source.
    <br>"The ethics of artificial intelligence is the branch of the ethics of technology specific to artificially intelligent systems.[1] It is sometimes divided into a concern with the moral behavior of humans as they design, make, use and treat artificially intelligent systems, and a concern with the behavior of machines, in machine ethics. It also includes the issue of a possible singularity due to superintelligent AI."
    <br>[{2023-04-10 retrieved} https://en.wikipedia.org/wiki/Ethics_of_artificial_intelligence]
    <br>
    <br>"The first global agreement on the ethics of AI was adopted in September 2021 by UNESCO's 193 Member States.[227]"
    <br>[{2023-04-10 retrieved} https://en.wikipedia.org/wiki/Artificial_intelligence#Regulation]
    <a class="clsHide" href="#idTechAietcsdsn"></a></p>
  <p id="idTechAietcsnam">name::
    <br>* McsEngl.techAi'ethics,
    <a class="clsHide" href="#idTechAietcsnam"></a></p>
  </section>

  <section id="idTechAibias">
  <h2 id="idTechAibiasH2">bias of techAi
    <a class="clsHide" href="#idTechAibiasH2"></a></h2>
  <p id="idTechAibiasdsn">description::
    <br>"AI programs can become biased after learning from real-world data. It is not typically introduced by the system designers but is learned by the program, and thus the programmers are often unaware that the bias exists.[204] Bias can be inadvertently introduced by the way training data is selected.[205] It can also emerge from correlations: AI is used to classify individuals into groups and then make predictions assuming that the individual will resemble other members of the group. In some cases, this assumption may be unfair.[206] An example of this is COMPAS, a commercial program widely used by U.S. courts to assess the likelihood of a defendant becoming a recidivist. ProPublica claims that the COMPAS-assigned recidivism risk level of black defendants is far more likely to be overestimated than that of white defendants, despite the fact that the program was not told the races of the defendants.[207]"
    <br>[{2023-04-10 retrieved} https://en.wikipedia.org/wiki/Artificial_intelligence#Algorithmic_bias]
    <a class="clsHide" href="#idTechAibiasdsn"></a></p>
  <p id="idTechAibiasnam">name::
    <br>* McsEngl.techAi'bias,
    <a class="clsHide" href="#idTechAibiasnam"></a></p>
  </section>

  <section id="idTechAiwpnz">
  <h2 id="idTechAiwpnzH2">weaponization of techAi
    <a class="clsHide" href="#idTechAiwpnzH2"></a></h2>
  <p id="idTechAiwpnzdsn">description::
    <br>·
    <a class="clsHide" href="#idTechAiwpnzdsn"></a></p>
  <p id="idTechAiwpnznam">name::
    <br>* McsEngl.techAi'weaponization,
    <a class="clsHide" href="#idTechAiwpnznam"></a></p>
  </section>

  <section id="idTechAifocs">
  <h2 id="idTechAifocsH2">failure-of-critical-systems of techAi
    <a class="clsHide" href="#idTechAifocsH2"></a></h2>
  <p id="idTechAifocsdsn">description::
    <br>·
    <a class="clsHide" href="#idTechAifocsdsn"></a></p>
  <p id="idTechAifocsnam">name::
    <br>* McsEngl.techAi'failure-of-critical-systems,
    <a class="clsHide" href="#idTechAifocsnam"></a></p>
  </section>

  <section id="idTechAisvlc">
  <h2 id="idTechAisvlcH2">surveillance of techAi
    <a class="clsHide" href="#idTechAisvlcH2"></a></h2>
  <p id="idTechAisvlcdsn">description::
    <br>·
    <a class="clsHide" href="#idTechAisvlcdsn"></a></p>
  <p id="idTechAisvlcnam">name::
    <br>* McsEngl.techAi'surveillance,
    <a class="clsHide" href="#idTechAisvlcnam"></a></p>
  </section>

  <section id="idTechAiuelmt">
  <h2 id="idTechAiuelmtH2">technological-unemployment of techAi
    <a class="clsHide" href="#idTechAiuelmtH2"></a></h2>
  <p id="idTechAiuelmtdsn">description::
    <br>"In the past, technology has tended to increase rather than reduce total employment, but economists acknowledge that "we're in uncharted territory" with AI.[195] A survey of economists showed disagreement about whether the increasing use of robots and AI will cause a substantial increase in long-term unemployment, but they generally agree that it could be a net benefit if productivity gains are redistributed.[196] Subjective estimates of the risk vary widely; for example, Michael Osborne and Carl Benedikt Frey estimate 47% of U.S. jobs are at "high risk" of potential automation, while an OECD report classifies only 9% of U.S. jobs as "high risk".[t][198]
    <br>Unlike previous waves of automation, many middle-class jobs may be eliminated by artificial intelligence; The Economist states that "the worry that AI could do to white-collar jobs what steam power did to blue-collar ones during the Industrial Revolution" is "worth taking seriously".[199] Jobs at extreme risk range from paralegals to fast food cooks, while job demand is likely to increase for care-related professions ranging from personal healthcare to the clergy.[200]"
    <br>[{2023-04-10 retrieved} https://en.wikipedia.org/wiki/Artificial_intelligence#Technological_unemployment]
    <a class="clsHide" href="#idTechAiuelmtdsn"></a></p>
  <p id="idTechAiuelmtnam">name::
    <br>* McsEngl.techAi'technological-unemployment,
    <a class="clsHide" href="#idTechAiuelmtnam"></a></p>
  </section>

  <section id="idTechAiexlr">
  <h2 id="idTechAiexlrH2">existential-risk of techAi
    <a class="clsHide" href="#idTechAiexlrH2"></a></h2>
  <p id="idTechAiexlrdsn">description::
    <br>"Existential risk from artificial general intelligence is the hypothesis that substantial progress in artificial general intelligence (AGI) could result in human extinction or some other unrecoverable global catastrophe.[1][2][3]
    <br>The existential risk ("x-risk") school argues as follows: The human species currently dominates other species because the human brain has some distinctive capabilities that other animals lack. If AI surpasses humanity in general intelligence and becomes "superintelligent", then it could become difficult or impossible for humans to control. Just as the fate of the mountain gorilla depends on human goodwill, so might the fate of humanity depend on the actions of a future machine superintelligence.[4]
    <br>The probability of this type of scenario is widely debated, and hinges in part on differing scenarios for future progress in computer science.[5] Concerns about superintelligence have been voiced by leading computer scientists and tech CEOs such as Geoffrey Hinton,[6] Alan Turing,[a] Elon Musk,[9] and OpenAI CEO Sam Altman.[10] As of 2022, circa half of AI researchers believe that there is a 10 percent or greater chance that our inability to control AI will cause an existential catastrophe.[11][12]"
    <br>[{2023-04-10 retrieved} https://en.wikipedia.org/wiki/Existential_risk_from_artificial_general_intelligence]
    <a class="clsHide" href="#idTechAiexlrdsn"></a></p>
  <p id="idTechAiexlrnam">name::
    <br>* McsEngl.techAi'existential-risk,
    <a class="clsHide" href="#idTechAiexlrnam"></a></p>
  </section>

  <section id="idTechAicprt">
  <h2 id="idTechAicprtH2">copyright of techAi
    <a class="clsHide" href="#idTechAicprtH2"></a></h2>
  <p id="idTechAicprtdsn">description::
    <br>"AI's decision-making abilities raises the question of legal responsibility and copyright status of created works. These issues are being refined in various jurisdictions.[219]"
    <br>[{2023-04-10 retrieved} https://en.wikipedia.org/wiki/Artificial_intelligence#Copyright]
    <a class="clsHide" href="#idTechAicprtdsn"></a></p>
  <p id="idTechAicprtnam">name::
    <br>* McsEngl.techAi'copyright,
    <a class="clsHide" href="#idTechAicprtnam"></a></p>
  </section>

  <section id="idTechAialnt">
  <h2 id="idTechAialntH2">alignment of techAi
    <a class="clsHide" href="#idTechAialntH2"></a></h2>
  <p id="idTechAialntdsn">description::
    <br>"In the field of artificial intelligence (AI), AI alignment research aims to steer AI systems towards their designers’ intended goals and interests. An aligned AI system advances the intended objective; a misaligned AI system is competent at advancing some objective, but not the intended one.[1]"
    <br>[{2023-04-10 retrieved} https://en.wikipedia.org/wiki/AI_alignment]
    <a class="clsHide" href="#idTechAialntdsn"></a></p>
  <p id="idTechAialntnam">name::
    <br>* McsEngl.AI-alignment,
    <br>* McsEngl.AI-control-problem,
    <br>* McsEngl.techAi'alignment,
    <a class="clsHide" href="#idTechAialntnam"></a></p>
  </section>
</section>

<section id="idTechAistcd">
  <h1 id="idTechAistcdH1">Softcode of techAi
    <a class="clsHide" href="#idTechAistcdH1"></a></h1>
  <p id="idTechAistcddsn">description::
    <br>* program,
    <br>* library,
    <br>* framwork,
    <a class="clsHide" href="#idTechAistcddsn"></a></p>
  <p id="idTechAistcdnam">name::
    <br>* McsEngl.Aisoftcode,
    <br>* McsEngl.techAi'Softcode!⇒Aisoftcode,
    <a class="clsHide" href="#idTechAistcdnam"></a></p>

  <section id="idTechAistcdTrfl">
  <h2 id="idTechAistcdTrflH2">Aisoftcode.TensorFlow-library
    <a class="clsHide" href="#idTechAistcdTrflH2"></a></h2>
  <p id="idTechAistcdTrfldsn">description::
    <br>· "TensorFlow is a free and open-source software library for machine learning and artificial intelligence. It can be used across a range of tasks but has a particular focus on training and inference of deep neural networks.[3][4]
    <br>TensorFlow was developed by the Google Brain team for internal Google use in research and production.[5][6][7] The initial version was released under the Apache License 2.0 in 2015.[1][8] Google released the updated version of TensorFlow, named TensorFlow 2.0, in September 2019.[9]
    <br>TensorFlow can be used in a wide variety of programming languages, including Python, JavaScript, C++, and Java.[10] This flexibility lends itself to a range of applications in many different sectors."
    <br>[{2023-08-14 retrieved} https://en.wikipedia.org/wiki/TensorFlow]
    <a class="clsHide" href="#idTechAistcdTrfldsn"></a></p>
  <p id="idTechAistcdTrflnam">name::
    <br>* McsEngl.Ailibrary.TensorFlow,
    <br>* McsEngl.TensorFlow-Ailibrary,
    <a class="clsHide" href="#idTechAistcdTrflnam"></a></p>
  </section>
</section>

<section id="idTechAiaclr">
  <h1 id="idTechAiaclrH1">AI-accelerator of techAi
    <a class="clsHide" href="#idTechAiaclrH1"></a></h1>
  <p id="idTechAiaclrdsn">description::
    <br>· "An AI accelerator is a class of specialized hardware accelerator[1] or computer system[2][3] designed to accelerate artificial intelligence and machine learning applications, including artificial neural networks and machine vision. Typical applications include algorithms for robotics, Internet of Things, and other data-intensive or sensor-driven tasks.[4] They are often manycore designs and generally focus on low-precision arithmetic, novel dataflow architectures or in-memory computing capability. As of 2018, a typical AI integrated circuit chip contains billions of MOSFET transistors.[5] A number of vendor-specific terms exist for devices in this category, and it is an emerging technology without a dominant design."
    <br>[{2023-07-31 retrieved} https://en.wikipedia.org/wiki/AI_accelerator]
    <a class="clsHide" href="#idTechAiaclrdsn"></a></p>
  <p id="idTechAiaclrnam">name::
    <br>* McsEngl.AI-accelerator,
    <br>* McsEngl.techAi'accelerator,
    <a class="clsHide" href="#idTechAiaclrnam"></a></p>

  <section id="idTechAiaclrTpu">
  <h2 id="idTechAiaclrTpuH2">AI-accelerator.TPU
    <a class="clsHide" href="#idTechAiaclrTpuH2"></a></h2>
  <p id="idTechAiaclrTpudsn">description::
    <br>· "Tensor Processing Unit (TPU) is an AI accelerator application-specific integrated circuit (ASIC) developed by Google for neural network machine learning, using Google's own TensorFlow software.[1] Google began using TPUs internally in 2015, and in 2018 made them available for third party use, both as part of its cloud infrastructure and by offering a smaller version of the chip for sale."
    <br>[{2023-07-31 retrieved} https://en.wikipedia.org/wiki/Tensor_Processing_Unit]
    <a class="clsHide" href="#idTechAiaclrTpudsn"></a></p>
  <p id="idTechAiaclrTpunam">name::
    <br>* McsEngl.TPU-tensor-processing-unit,
    <br>* McsEngl.tensor-processing-unit,
    <br>* McsEngl.AI-accelerator.TPU,
    <a class="clsHide" href="#idTechAiaclrTpunam"></a></p>
  </section>
</section>

<section id="idTechAirgln">
  <h1 id="idTechAirglnH1">regulation of techAi
    <a class="clsHide" href="#idTechAirglnH1"></a></h1>
  <p id="idTechAirglndsn">description::
    <br>"The regulation of artificial intelligence is the development of public sector policies and laws for promoting and regulating artificial intelligence (AI); it is therefore related to the broader regulation of algorithms. The regulatory and policy landscape for AI is an emerging issue in jurisdictions globally, including in the European Union and in supra-national bodies like the IEEE, OECD and others. Since 2016, a wave of AI ethics guidelines have been published in order to maintain social control over the technology.[1] Regulation is considered necessary to both encourage AI and manage associated risks. In addition to regulation, AI-deploying organizations need to play a central role in creating and deploying trustworthy AI in line with the principles of trustworthy AI,[2] and take accountability to mitigate the risks.[3] Regulation of AI through mechanisms such as review boards can also be seen as social means to approach the AI control problem.[4][5]"
    <br>[{2023-04-10 retrieved} https://en.wikipedia.org/wiki/Regulation_of_artificial_intelligence]
    <a class="clsHide" href="#idTechAirglndsn"></a></p>
  <p id="idTechAirglnnam">name::
    <br>* McsEngl.AI-regulation,
    <br>* McsEngl.techAi'regulation,
    <a class="clsHide" href="#idTechAirglnnam"></a></p>
</section>

<section id="idTechAiozn">
  <h1 id="idTechAioznH1">organization of techAi
    <a class="clsHide" href="#idTechAioznH1"></a></h1>
  <p id="idTechAiozndsn">description::
    <br>* AI2,
    <br>* GPAI,
    <a class="clsHide" href="#idTechAiozndsn"></a></p>
  <p id="idTechAioznnam">name::
    <br>* McsEngl.oznAi,
    <br>* McsEngl.techAi'organization!⇒oznAi,
    <a class="clsHide" href="#idTechAioznnam"></a></p>

  <section id="idTechAioznAi2">
  <h2 id="idTechAioznAi2H2">oznAi.AI2
    <a class="clsHide" href="#idTechAioznAi2H2"></a></h2>
  <p id="idTechAioznAi2dsn">description::
    <br>· "The Allen Institute for AI (AI2) was Paul Allen’s brainchild. Emboldened by the success of the Allen Institute for Brain Science, Paul wanted to launch an independent inquiry into the nature of the mind based on AI. In essence, he decided to hedge his bets as to whether neuroscience or AI (or both) would yield breakthrough insights into the nature of intelligence.
    <br>... Today AI2 employs over 200 researchers, engineers, and support staff over multiple sites, and is recognized worldwide as a premiere AI research organization. Its research scope has grown, adding new programs in computer vision and perception (PRIOR), commonsense reasoning (Mosaic), natural language processing (AllenNLP), and most recently AI for the Environment, as well as launching a new branch in Israel (AI2 Israel). Most importantly, AI2 has produced, and continues to produce, many high-impact results that have significantly altered the course of the field, described throughout this book. Paul Allen was the visionary who created AI2 and had the courage to turn his AI dreams into a reality, and until his tragic passing in 2018 he keenly followed our work and regularly challenged us all to strive for breakthroughs. The research collected here summarizes many of our important results to date, and I think he would be proud. But then he would immediately follow that with his characteristic, relentless push forward, reflective of a true visionary: "So what's next?""
    <br>[{2023-06-29 retrieved} https://works.allenai.org/]
    <a class="clsHide" href="#idTechAioznAi2dsn"></a></p>
  <p id="idTechAioznAi2nam">name::
    <br>* McsEngl.AI2,
    <br>* McsEngl.AI2-Allen-Institute-for-AI,
    <br>* McsEngl.oznAi.AI2,
    <a class="clsHide" href="#idTechAioznAi2nam"></a></p>
  </section>

  <section id="idTechAioznGpai">
  <h2 id="idTechAioznGpaiH2">oznAi.GPAI
    <a class="clsHide" href="#idTechAioznGpaiH2"></a></h2>
  <p id="idTechAioznGpaidsn">description::
    <br>"The Global Partnership on Artificial Intelligence (GPAI, or "gee-pay") is an international and multi-stakeholder initiative that aims to advance the responsible and human-centric development and use of artificial intelligence.[2] Specifically, GPAI brings together leading experts from science, industry, civil society, and governments to "bridge the gap between theory and practice" through applied AI projects and activities.[3] The goal is to facilitate international collaboration, reduce duplication between governments, and act as a global reference point on discussions on responsible AI.[3][4]
    <br>First announced on the margins of the 2018 G7 Summit by Canadian Prime Minister Justin Trudeau and French President Emmanuel Macron, GPAI officially launched on June 15, 2020[5] with fifteen founding members: Australia, Canada, France, Germany, India,[6] Italy, Japan, Mexico, New Zealand, the Republic of Korea, Singapore, Slovenia, the United Kingdom, the United States and the European Union.[7][8] The OECD hosts a dedicated secretariat to support GPAI's governing bodies and activities.[7] UNESCO joined the partnership in December 2020 as an observer.[9][7] On November 11, 2021, Czechia, Israel and few more EU countries also joined the GPAI,[10] bringing the total membership to 25 countries.[2] Since the November 2022 summit, the list of members stands at 29, with in addition to the above, Belgium, Brazil, Denmark, Ireland, The Netherlands, Poland, Senegal, Serbia, Sweden, and Turkey.[11]"
    <br>[{2023-04-10 retrieved} https://en.wikipedia.org/wiki/Global_Partnership_on_Artificial_Intelligence]
    <a class="clsHide" href="#idTechAioznGpaidsn"></a></p>
  <p id="idTechAioznGpainam">name::
    <br>* McsEngl.GPAI!⇒oznGpai,
    <br>* McsEngl.GPAI-Global-Partnership-on-Artificial-Intelligence!⇒oznGpai,
    <br>* McsEngl.Global-Partnership-on-Artificial-Intelligence!⇒oznGpai,
    <br>* McsEngl.oznAi.GPAI!⇒oznGpai,
    <br>* McsEngl.oznGpai,
    <a class="clsHide" href="#idTechAioznGpainam"></a></p>
  </section>

  <section id="idTechAioznOpnai">
  <h2 id="idTechAioznOpnaiH2">oznAi.OpenAI
    <a class="clsHide" href="#idTechAioznOpnaiH2"></a></h2>
  <p id="idTechAioznOpnaidsn">description::
    <br>· "OpenAI is an American artificial intelligence (AI) research laboratory consisting of the non-profit OpenAI and its for-profit subsidiary corporation OpenAI Limited Partnership. OpenAI conducts AI research with the declared intention of developing "safe and beneficial" artificial general intelligence, which it defines as "highly autonomous systems that outperform humans at most economically valuable work".[4]
    <br>OpenAI was founded in 2015 by Ilya Sutskever, Greg Brockman, Trevor Blackwell, Vicki Cheung, Andrej Karpathy, Durk Kingma, Jessica Livingston, John Schulman, Pamela Vagata, and Wojciech Zaremba, with Sam Altman and Elon Musk serving as the initial board members.[5][6][7] Microsoft provided OpenAI LP with a $1 billion investment in 2019 and a $10 billion investment in 2023.[8][9]"
    <br>[{2023-07-31 retrieved} https://en.wikipedia.org/wiki/OpenAI]
    <a class="clsHide" href="#idTechAioznOpnaidsn"></a></p>
  <p id="idTechAioznOpnainam">name::
    <br>* McsEngl.OpenAI!⇒oznOpenai,
    <br>* McsEngl.oznAi.OpenAI!⇒oznOpenai,
    <br>* McsEngl.oznOpenai,
    <a class="clsHide" href="#idTechAioznOpnainam"></a></p>
  <p id="idTechAioznOpnaiwpa">addressWpg::
    <br>* https://blog.finxter.com/creating-a-simple-diet-bot-in-your-terminal-with-openais-api/,
    <a class="clsHide" href="#idTechAioznOpnaiwpa"></a></p>
  </section>
</section>

<section id="idTechAirscF">
  <h1 id="idTechAirscFH1">info-resource of techAi
    <a class="clsHide" href="#idTechAirscFH1"></a></h1>
  <p id="idTechAirscwpa">addressWpg::
    <br>* https://www.stateof.ai/,
    <br>* https://welcome.ai/about,
    <br>* https://knowledge4policy.ec.europa.eu/ai-watch_en,
    <br>* https://joinup.ec.europa.eu/collection/elise-european-location-interoperability-solutions-e-government/artificial-intelligence-public-sector,
    <br>* {2021-01-15} https://www.weforum.org/agenda/2021/01/ai-agriculture-water-irrigation-farming,
    <br>* {2020} Blagoj DELIPETREV, Chrisa TSINARAKIi, Uroš KOSTIĆ. “Historical Evolution of Artificial Intelligence”, EUR 30221EN, Publications Office of the European Union, Luxembourg, 2020, ISBN 978-92-76-18940-4, doi:10.2760/801580, JRC120469: https://publications.jrc.ec.europa.eu/repository/handle/JRC120469,
    <br>* {1995-12-26} http://sandcastle.cosc.brocku.ca/~bross/3P71/misc/outsider_ai.txt,
    <a class="clsHide" href="#idTechAirscwpa"></a></p>
  <p id="idTechAirscnam">name::
    <br>* McsEngl.techAi'Infrsc,
    <a class="clsHide" href="#idTechAirscnam"></a></p>
</section>

<section id="idTechAidngF">
  <h1 id="idTechAidngFH1">DOING of techAi
    <a class="clsHide" href="#idTechAidngFH1"></a></h1>
  <p id="idTechAidngdsn">description::
    <br>*
    <a class="clsHide" href="#idTechAidngdsn"></a></p>
  <p id="idTechAidngnam">name::
    <br>* McsEngl.techAi'doing,
    <a class="clsHide" href="#idTechAidngnam"></a></p>

  <section id="idTechAiappl">
  <h2 id="idTechAiapplH2">application-process of techAi
    <a class="clsHide" href="#idTechAiapplH2"></a></h2>
  <p id="idTechAiappldsn">description::
    <br>* <a class="clsPreview" href="#idTchInf010">computer-vision</a>,
    <br>* machine-learning,
    <br>* machine-reasoning,
    <br>* natural-language-understanding,
    <br>===
    <br>* education,
    <br>* government,
    <br>* economy,
    <br>* healthcare,
    <a class="clsHide" href="#idTechAiappldsn"></a></p>
  <p id="idTechAiapplnam">name::
    <br>* McsEngl.Aiappl,
    <br>* McsEngl.techAi'application-process!⇒Aiappl,
    <br>* McsEngl.techAi'use!⇒Aiappl,
    <a class="clsHide" href="#idTechAiapplnam"></a></p>
  <p id="idTechAiapplwpa">addressWpg::
    <br>* https://ai.googleblog.com/2023/02/google-research-2022-beyond-health.html,
    <a class="clsHide" href="#idTechAiapplwpa"></a></p>

  <section id="idTechAiapplChbt">
  <h3 id="idTechAiapplChbtH3">Aiappl.chatbot
    <a class="clsHide" href="#idTechAiapplChbtH3"></a></h3>
  <p id="idTechAiapplChbtdsn">description::
    <br>· "A chatbot (originally chatterbot[1]) is a software application that aims to mimic human conversation through text or voice interactions, typically online.[2][3] Modern chatbots are artificial intelligence (AI) systems that are capable of maintaining a conversation with a user in natural language and simulating the way a human would behave as a conversational partner. Such technologies often utilize aspects of deep learning and natural language processing.
    <br>Recently this field has gained widespread attention due to the popularity of OpenAI's ChatGPT,[4] followed by alternatives such as Microsoft's Bing Chat (which uses OpenAI's GPT-4) and Google's Bard.[5] Such examples reflect the recent practice of such products being built based upon broad foundational large language models that get fine-tuned so as to target specific tasks or applications (i.e. simulating human conversation, in the case of chatbots). Chatbots can also be designed or customized to further target even more specific situations and/or particular subject-matter domains.[6]
    <br>A major area where chatbots have long been used is in customer service and support, such as with various sorts of virtual assistants.[7] Recently, companies spanning various industries have begun using the latest generative artificial intelligence technologies to power more advanced developments in such areas.[6]"
    <br>[{2023-08-12 retrieved} https://en.wikipedia.org/wiki/Chatbot]
    <a class="clsHide" href="#idTechAiapplChbtdsn"></a></p>
  <p id="idTechAiapplChbtnam">name::
    <br>* McsEngl.Aiappl.chatbot,
    <br>* McsEngl.chatbot,
    <a class="clsHide" href="#idTechAiapplChbtnam"></a></p>
  <p id="idTechAiapplChbtstr">specific-tree-of-chatbot::
    <br>* LLM-chatbot,
    <a class="clsHide" href="#idTechAiapplChbtstr"></a></p>
  </section>
  </section>

  <section id="idTechAiprmpt">
  <h2 id="idTechAiprmptH2">prompting of techAi
    <a class="clsHide" href="#idTechAiprmptH2"></a></h2>
  <p id="idTechAiprmptdsn">description::
    <br>· "Prompt engineering or prompting is the process of structuring sentences so that they can be interpreted and understood by a generative AI model in such a way that its output is in accord with the user's intentions.[1][2] A prompt can be a description of a desired output such as "a high-quality photo of an astronaut riding a horse", a command such as "write a limerick about chickens", or a question such as "All men are mortal. Socrates is a man. Is Socrates mortal?". The ability to understand prompts, also called in-context learning, is an emergent ability of large language models. [3]
    <br>Prompt engineering for a text-to-text model like ChatGPT may involve phrasing a query, providing relevant context, refining or adjusting prompts, and asking follow-up questions.[4] A prompt may include a few examples for context, such as "maison is French for house. chat is French for cat, chien is French for", an approach called few-shot learning[5][6]. Prompting a text-to-image model may involve adding, removing, emphasizing and re-ordering words to achieve a desired subject, style, aesthetic, layout, lighting, and texture.[1][7]
    <br>When applied to PaLM, a 540B-sized model, prompt engineering has allowed the model to perform comparably with task-specific fine-tuned models on several tasks, even setting a new state of the art at the time on the GSM8K mathematical reasoning benchmark.[8] It is also known as "mesa"-optimization,[9] based on presence of (small) learn-to-learn models in the data.[10][11][12][13][14][15] Unlike pre-training or fine-tuning, prompt engineering does not modify the model it is applied to.[8][16][17][18][19][20]"
    <br>[{2023-08-13 retrieved} https://en.wikipedia.org/wiki/Prompt_engineering]
    <a class="clsHide" href="#idTechAiprmptdsn"></a></p>
  <p id="idTechAiprmptnam">name::
    <br>* McsEngl.techAi'prompting,
    <a class="clsHide" href="#idTechAiprmptnam"></a></p>
  </section>
</section>

<section id="idTechAievgF">
  <h1 id="idTechAievgFH1">evoluting of techAi
    <a class="clsHide" href="#idTechAievgFH1"></a></h1>
  <p id="idTechAievgnam">name::
    <br>* McsEngl.techAi'evoluting,
    <a class="clsHide" href="#idTechAievgnam"></a></p>
  <p id="idTechAievg20230730">{2023-07-30}::
    <br>=== McsHitp-creation:
    <br>· creation of current <a class="clsPreview" href="McsTchInf000009.last.html#idMcsHitp">concept</a>.
    <a class="clsHide" href="#idTechAievg20230730"></a></p>
  <p id="idTechAi20230314">{2023-03-14}-techAi-GPT-4::
    <br>"We’ve created GPT-4, the latest milestone in OpenAI’s effort in scaling up deep learning. GPT-4 is a large multimodal model (accepting image and text inputs, emitting text outputs) that, while less capable than humans in many real-world scenarios, exhibits human-level performance on various professional and academic benchmarks."
    <br>[https://openai.com/research/gpt-4]
    <br>* McsEngl.{science'2023-03-14}-techAi-GPT-4,
    <br>* McsEngl.{2023-03-14}-techAi-GPT-4,
    <a class="clsHide" href="#idTechAi20230314"></a></p>
  <p id="idTechAi2017">{2017}-techAi-Transformer::
    <br>"Transformers were introduced in 2017 by a team at Google Brain[1] and are increasingly becoming the model of choice for NLP problems,[3] replacing RNN models such as long short-term memory (LSTM)."
    <br>[{2023-04-10 retrieved} https://en.wikipedia.org/wiki/Transformer_(machine_learning_model)]
    <br>* McsEngl.{science'2017}-techAi-Transformers,
    <br>* McsEngl.{2017}-techAi-Transformers,
    <a class="clsHide" href="#idTechAi2017"></a></p>
  <p id="idTechAi2015">{2015}-techAi-landmark-year::
    <br>"According to Bloomberg's Jack Clark, 2015 was a landmark year for artificial intelligence, with the number of software projects that use AI within Google increased from a "sporadic usage" in 2012 to more than 2,700 projects.[i] He attributed this to an increase in affordable neural networks, due to a rise in cloud computing infrastructure and to an increase in research tools and datasets.[7]"
    <br>[{2023-04-10 retrieved} https://en.wikipedia.org/wiki/Artificial_intelligence#]
    <br>* McsEngl.{science'2015}-techAi-landmark-year,
    <br>* McsEngl.{2015}-techAi-landmark-year,
    <a class="clsHide" href="#idTechAi2015"></a></p>
  <p id="idTechAi19871993">{1987..1993}-techAi-2nd-winter::
    <br>· 2nd major AI winter.
    <br>[{2023-04-04 retrieved} https://en.wikipedia.org/wiki/AI_winter]
    <br>* McsEngl.{science'1987..1993}-techAi-2nd-winter,
    <br>* McsEngl.{1987..1993}-techAi-2nd-winter,
    <a class="clsHide" href="#idTechAi19871993"></a></p>
  <p id="idTechAi1980s">{1980s}-techAi-expert-systems::
    <br>"In the early 1980s, AI research was revived by the commercial success of expert systems,[34] a form of AI program that simulated the knowledge and analytical skills of human experts. By 1985, the market for AI had reached over a billion dollars. At the same time, Japan's fifth generation computer project inspired the U.S. and British governments to restore funding for academic research.[4] However, beginning with the collapse of the Lisp Machine market in 1987, AI once again fell into disrepute, and a second, longer-lasting winter began.[6]"
    <br>[{2023-04-10 retrieved} https://en.wikipedia.org/wiki/Artificial_intelligence]
    <br>* McsEngl.{science'1980s}-techAi-expert-systems,
    <br>* McsEngl.{1980s}-techAi-expert-systems,
    <a class="clsHide" href="#idTechAi1980s"></a></p>
  <p id="idTechAi19741980">{1974..1980}-techAi-1st-winter::
    <br>· 1st major AI winter.
    <br>[{2023-04-04 retrieved} https://en.wikipedia.org/wiki/AI_winter]
    <br>* McsEngl.{science'1974..1980}-techAi-1st-winter,
    <br>* McsEngl.{1974..1980}-techAi-1st-winter,
    <a class="clsHide" href="#idTechAi19741980"></a></p>
  <p id="idTechAi1969">{1969}-techAi-Shakey-the-Robot::
    <br>"1969 Shakey the Robot was the first general-purpose mobile robot capable of reasoning its actions. This project integrated research in robotics with computer vision and natural language processing, thus being the first project that combined logical reasoning and physical action (Bertram 1972)."
    <br>[{2020} Historical-Evolution-of-AI, p7, <a class="clsPreview" href="../dirLag/McsLag000013.last.html#idInfrscElnc000001">ifrcElnc000001</a>]
    <br>* McsEngl.{1969}-techAi-Shakey-the-Robot,
    <a class="clsHide" href="#idTechAi1969"></a></p>
  <p id="idTechAi1956">{1956}-techAi-Dartmouth-conference::
    <br>"The first “AI period” began with the Dartmouth conference in 1956, where AI got its name and mission.
    <br>McCarthy coined the term "artificial intelligence," which became the name of the scientific field.
    <br>The primary conference assertion was, "Every aspect of any other feature of learning or intelligence should be accurately described so that the machine can simulate it” (Russell and Norvig 2016).
    <br>Among the conference attendees were Ray Solomonoff, Oliver Selfridge, Trenchard More, Arthur Samuel, Herbert A. Simon, and Allen Newell, all of whom became key figures in the Ai field"
    <br>[{2020} Historical-Evolution-of-AI, p7, <a class="clsPreview" href="../dirLag/McsLag000013.last.html#idInfrscElnc000001">ifrcElnc000001</a>]
    <br>* McsEngl.{1956}-techAi-Dartmouth-conference,
    <a class="clsHide" href="#idTechAi1956"></a></p>
  <p id="idTechAi1955">{1955}-techAi-Logic-Theorist::
    <br>"1955 The Logic Theorist had proven 38 theorems from Principia Mathematica and introduced critical concepts in artificial intelligence, like heuristics, list processing, ‘reasoning as search,' etc. (Newell et al. 1962)."
    <br>[{2020} Historical-Evolution-of-AI, p7, <a class="clsPreview" href="../dirLag/McsLag000013.last.html#idInfrscElnc000001">ifrcElnc000001</a>]
    <br>* McsEngl.{1955}-techAi-Logic-Theorist,
    <a class="clsHide" href="#idTechAi1955"></a></p>
  <p id="idTechAi1950">{1950}-techAi-Turing-test::
    <br>"In 1950, Alan Turing published the milestone paper "Computing machinery and intelligence" (Turing 1950), considering the fundamental question "Can machines think?”
    <br>Turing proposed an imitation game, known as the Turing test afterwards, where if a machine could carry on a conversation indistinguishable from a conversation with a human being, then it is reasonable to say that the machine is intelligent.
    <br>The Turing test was the first experiment proposed to measure machine intelligence"
    <br>[{2020} Historical-Evolution-of-AI, p7, <a class="clsPreview" href="../dirLag/McsLag000013.last.html#idInfrscElnc000001">ifrcElnc000001</a>]
    <br>* McsEngl.{1950}-techAi-Turing-test,
    <a class="clsHide" href="#idTechAi1950"></a></p>
  <p id="idTechAi1943">{1943}-techAi-first-work::
    <br>"The first work that is now generally recognized as AI was McCullouch and Pitts' 1943 formal design for Turing-complete "artificial neurons".[20]"
    <br>* McsEngl.{science'1943}-techAi-first-work,
    <br>* McsEngl.{1943}-techAi-first-work,
    <a class="clsHide" href="#idTechAi1943"></a></p>
  <p id="idTechAievgnam">name::
    <br>* McsEngl.techAi'evoluting,
    <a class="clsHide" href="#idTechAievgnam"></a></p>

  <section id="idTechAievgwntr">
  <h2 id="idTechAievgwntrH2">winter of techAi
    <a class="clsHide" href="#idTechAievgwntrH2"></a></h2>
  <p id="idTechAievgwntrdsn">description::
    <br>"In the history of artificial intelligence, an AI winter is a period of reduced funding and interest in artificial intelligence research.[1] The term was coined by analogy to the idea of a nuclear winter.[2] The field has experienced several hype cycles, followed by disappointment and criticism, followed by funding cuts, followed by renewed interest years or even decades later.
    <br>The term first appeared in 1984 as the topic of a public debate at the annual meeting of AAAI (then called the "American Association of Artificial Intelligence"). It is a chain reaction that begins with pessimism in the AI community, followed by pessimism in the press, followed by a severe cutback in funding, followed by the end of serious research.[2] At the meeting, Roger Schank and Marvin Minsky—two leading AI researchers who had survived the "winter" of the 1970s—warned the business community that enthusiasm for AI had spiraled out of control in the 1980s and that disappointment would certainly follow. Three years later, the billion-dollar AI industry began to collapse.[2]
    <br>Hype is common in many emerging technologies, such as the railway mania or the dot-com bubble. The AI winter was a result of such hype, due to over-inflated promises by developers, unnaturally high expectations from end-users, and extensive promotion in the media.[3] Despite the rise and fall of AI's reputation, it has continued to develop new and successful technologies. AI researcher Rodney Brooks would complain in 2002 that "there's this stupid myth out there that AI has failed, but AI is around you every second of the day."[4] In 2005, Ray Kurzweil agreed: "Many observers still think that the AI winter was the end of the story and that nothing since has come of the AI field. Yet today many thousands of AI applications are deeply embedded in the infrastructure of every industry."[5]
    <br>Enthusiasm and optimism about AI has generally increased since its low point in the early 1990s. Beginning about 2012, interest in artificial intelligence (and especially the sub-field of machine learning) from the research and corporate communities led to a dramatic increase in funding and investment."
    <br>[{2023-04-04 retrieved} https://en.wikipedia.org/wiki/AI_winter]
    <a class="clsHide" href="#idTechAievgwntrdsn"></a></p>
  <p id="idTechAievgwntrnam">name::
    <br>* McsEngl.AI-winter,
    <br>* McsEngl.techAi'winter,
    <a class="clsHide" href="#idTechAievgwntrnam"></a></p>
  </section>
</section>

<section id="idTechAiwptF">
  <h1 id="idTechAiwptFH1">WHOLE-PART-TREE of techAi
    <a class="clsHide" href="#idTechAiwptFH1"></a></h1>
  <p id="idTechAiwptnam">name::
    <br>* McsEngl.techAi'part-whole-tree,
    <br>* McsEngl.techAi'whole-part-tree,
    <a class="clsHide" href="#idTechAiwptnam"></a></p>
  <p id="idTechAiwtr">whole-tree-of-techAi::
    <br>*
    <br>* ... <a class="clsPreview" href="../dirCor/McsCor000003.last.html#idEntwtr">Sympan</a>.
    <a class="clsHide" href="#idTechAiwtr"></a></p>
  <p id="idTechAiptr">part-tree-of-techAi::
    <br>*
    <a class="clsHide" href="#idTechAiptr"></a></p>
</section>

<section id="idTechAigstF">
  <h1 id="idTechAigstFH1">GENERIC-SPECIFIC-TREE of techAi
    <a class="clsHide" href="#idTechAigstFH1"></a></h1>
  <p id="idTechAigstnam">name::
    <br>* McsEngl.techAi'generic-specific-tree,
    <br>* McsEngl.techAi'specific-generic-tree,
    <a class="clsHide" href="#idTechAigstnam"></a></p>
  <p id="idTechAigtr">generic-tree-of-techAi::
    <br>* <a class="clsPreview" href="../dirCor/McsCor000003.last.html#idOverview">entity</a>.
    <br>* <a class="clsPreview" href="../dirEdu/McsEdu000002.last.html#idOverview">science</a>,
    <br>* <a class="clsPreview" href="../dirTch/McsTch000002.last.html#idTch003">tool</a>,
    <br>===
    <br>"Human beings across time have shared one important characteristic: they use tools to improve what they can achieve.
    <br>AI can be one such tool, and it can work well, provided we remember it is a tool. As a tool it must be put in the hands of a human, who can use appropriately and intentionally, for achieving the goals they have."
    <br>[{2020-10-08} https://clearbox.ai/blog/2020-06-16-making-ai-less-magic-and-more-human/]
    <br>===
    <br>"AI is technology or science
    <br>Artificial intelligence (AI) is both a science and a technology.
    <br>As a science, AI is the study of how to create intelligent machines. This involves understanding the principles of human intelligence and how to replicate them in machines. AI scientists use mathematics, computer science, and other disciplines to develop new AI algorithms and systems.
    <br>As a technology, AI is the application of AI algorithms and systems to real-world problems. This includes the development of AI-powered products and services, such as self-driving cars, virtual assistants, and fraud detection systems.
    <br>The line between AI science and AI technology is not always clear. For example, the development of a new AI algorithm is often considered to be a scientific achievement, but the application of that algorithm to a real-world problem is considered to be a technological achievement.
    <br>Ultimately, AI is both a science and a technology. It is the science of creating intelligent machines, and it is the technology of applying those machines to real-world problems.
    <br>Here are some examples of how AI is being used in science:
    <br>* AI is being used to develop new drug discovery methods.
    <br>* AI is being used to analyze large datasets of medical images and other data to improve diagnosis and treatment.
    <br>* AI is being used to develop new environmental models to help us understand and mitigate climate change.
    <br>* AI is being used to study the human brain and to develop new treatments for neurological disorders.
    <br>Here are some examples of how AI is being used in technology:
    <br>* AI is being used to develop self-driving cars.
    <br>* AI is being used to develop virtual assistants like Siri and Alexa.
    <br>* AI is being used to develop fraud detection systems.
    <br>* AI is being used to develop personalized recommendations systems.
    <br>* AI is being used to develop new gaming experiences.
    <br>AI is a rapidly evolving field, and it is difficult to predict what the future holds. However, it is clear that AI has the potential to revolutionize many aspects of our lives, from the way we work to the way we interact with the world around us."
    <br>[{2023-08-23 retrieved} https://bard.google.com/]
    <br>
    <br>* McsEngl.techAi'generic,
    <br>* McsEngl.techAi:science,
    <br>* McsEngl.techAi:tool,
    <a class="clsHide" href="#idTechAigtr"></a></p>
  <p id="idTechAistr">specific-tree-of-techAi::
    <br>* <a class="clsPreview" href="#idTechAiSmtc">semantic-AI</a>,
    <br>* <a class="clsPreview" href="#idTechAiStcl">statistical-AI</a>,
    <br>===
    <br>* <a class="clsPreview" href="#idTechAiGnrl">general-AI</a>,
    <br>* <a class="clsPreview" href="#idTechAiNrw">specific-AI</a>,
    <br>===
    <br>* <a class="clsPreview" href="#idTchInf008">machine-learning</a>,
    <br>* <a class="clsPreview" href="#idTchNn">neural-network</a>,
    <br>
    <br>* McsEngl.techAi.specific,
    <a class="clsHide" href="#idTechAistr"></a></p>
</section>

<section id="idTechAirpbl">
  <h1 id="idTechAirpblH1">techAi.responsible
    <a class="clsHide" href="#idTechAirpblH1"></a></h1>
  <p id="idTechAirpbldsn">description::
    <br>"According to Google, responsible AI means not just avoiding risks, but also finding ways to improve people’s lives and address social and scientific problems, as these new technologies have applications in predicting disasters, improving medicine, precision agriculture, and more. "
    <br>[{2023-03-31 retrieved} https://sdtimes.com/ai/google-outlines-four-principles-for-responsible-ai/]
    <a class="clsHide" href="#idTechAirpbldsn"></a></p>
  <p id="idTechAirpblnam">name::
    <br>* McsEngl.responsible-AI,
    <br>* McsEngl.techAi.responsible,
    <a class="clsHide" href="#idTechAirpblnam"></a></p>

  <section id="idTechAirpblrsc">
  <h2 id="idTechAirpblrscH2">info-resource of responsible-AI
    <a class="clsHide" href="#idTechAirpblrscH2"></a></h2>
  <p id="idTechAirpblrscdsn">description::
    <br>* https://www.blog.google/technology/ai/ai-principles/,
    <br>* https://www.oecd.org/digital/artificial-intelligence/,
    <a class="clsHide" href="#idTechAirpblrscdsn"></a></p>
  <p id="idTechAirpblrscnam">name::
    <br>* McsEngl.responsible-AI'Infrsc,
    <a class="clsHide" href="#idTechAirpblrscnam"></a></p>
  </section>
</section>

<section id="idTechAiNrw">
  <h1 id="idTechAiNrwH1">techAi.narrow
    <a class="clsHide" href="#idTechAiNrwH1"></a></h1>
  <p id="idTechAiNrwdsn">description::
    <br>"Artificial Narrow Intelligence (ANI), often referred to as “Weak” AI is the type of AI that mostly exists today. ANI systems can perform one or a few specific tasks and operate within a predefined environment, e.g., those exploited by personal assistants Siri, Alexa, language translations, recommendation systems, image recognition systems, face identification, etc.
    <br>ANI can process data at lightning speed and boost the overall productivity and efficiency in many practical applications, e.g., translate between 100+ languages simultaneously, identify faces and objects in billions of images with high accuracy, assist users in many data-driven decisions in a quicker way. ANI can perform routine, repetitive, and mundane tasks that humans would prefer to avoid."
    <br>[{2020} Historical-Evolution-of-AI, <a class="clsPreview" href="../dirLag/McsLag000013.last.html#idInfrscElnc000001">ifrcElnc000001</a>]
    <a class="clsHide" href="#idTechAiNrwdsn"></a></p>
  <p id="idTechAiNrwnam">name::
    <br>* McsEngl.ANI'(Artificial-Narrow-Inteligence),
    <br>* McsEngl.ASI'(Artificial-Specific-Inteligence),
    <br>* McsEngl.Artificial-Narrow-Inteligence,
    <br>* McsEngl.techAi.narrow,
    <br>* McsEngl.weak-AI,
    <a class="clsHide" href="#idTechAiNrwnam"></a></p>
</section>

<section id="idTechAiGnrl">
  <h1 id="idTechAiGnrlH1">techAi.general
    <a class="clsHide" href="#idTechAiGnrlH1"></a></h1>
  <p id="idTechAiGnrldsn">description::
    <br>"Artificial General Intelligence (AGI) or “Strong” AI refers to machines that exhibit human intelligence. In other words, AGI aims to perform any intellectual task that a human being can. AGI is often illustrated in science fiction movies with situations where humans interact with machines that are conscious, sentient, and driven by emotion and self-awareness. At this moment, there is nothing like an AGI."
    <br>[{2020} Historical-Evolution-of-AI, <a class="clsPreview" href="../dirLag/McsLag000013.last.html#idInfrscElnc000001">ifrcElnc000001</a>]
    <a class="clsHide" href="#idTechAiGnrldsn"></a></p>
  <p id="idTechAiGnrlnam">name::
    <br>* McsEngl.AGI!=Artificial-General-Intelligence,
    <br>* McsEngl.Artificial-General-Intelligence!⇒techAgi,
    <br>* McsEngl.techAgi,
    <br>* McsEngl.techAi.general!⇒techAgi,
    <a class="clsHide" href="#idTechAiGnrlnam"></a></p>

  <section id="idTechAiGnrlrsc">
  <h2 id="idTechAiGnrlrscH2">info-resource of techAgi
    <a class="clsHide" href="#idTechAiGnrlrscH2"></a></h2>
  <p id="idTechAiGnrlrscdsn">description::
    <br>* {2023} https://blog.finxter.com/what-is-artificial-general-intelligence-a-comprehensive-overview/
    <a class="clsHide" href="#idTechAiGnrlrscdsn"></a></p>
  <p id="idTechAiGnrlrscnam">name::
    <br>* McsEngl.techAgi'Infrsc,
    <a class="clsHide" href="#idTechAiGnrlrscnam"></a></p>
  </section>
</section>

<section id="idTechAiSpr">
  <h1 id="idTechAiSprH1">techAi.supper
    <a class="clsHide" href="#idTechAiSprH1"></a></h1>
  <p id="idTechAiSprdsn">description::
    <br>"Artificial Superintelligence (ASI) is defined as “any intellect that greatly exceeds the cognitive performance of humans in virtually all domains of interest” (Bostrom 2016). ASI is supposed to surpass human intelligence in all aspects — such as creativity, general wisdom, and problem-solving. ASI is supposed to be capable of exhibiting intelligence that we have not seen in the brightest thinkers amongst us. Many thinkers are worried about ASI. At this moment, ASI belongs to science fiction.
    <br>If we ever succeed in creating an AI that is capable of generalizing, understanding causality, making a model of the world, it is highly likely that it will be closer to ASI than AGI. AI excels in numerical calculations, and there is no logical explanation as to why AI would downgrade its abilities to simulate humans. AI’s quest ultimately leads to ASI."
    <br>[{2020} Historical-Evolution-of-AI, <a class="clsPreview" href="../dirLag/McsLag000013.last.html#idInfrscElnc000001">ifrcElnc000001</a>]
    <a class="clsHide" href="#idTechAiSprdsn"></a></p>
  <p id="idTechAiSprnam">name::
    <br>* McsEngl.ASI'(Artificial-Superintelligence),
    <br>* McsEngl.Artificial-Superintelligence,
    <br>* McsEngl.techAi.supper,
    <a class="clsHide" href="#idTechAiSprnam"></a></p>
</section>

<section id="idTechAiSmtc">
  <h1 id="idTechAiSmtcH1">techAi.semantic
    <a class="clsHide" href="#idTechAiSmtcH1"></a></h1>
  <p id="idTechAiSmtcdsn">description::
    <br>"Semantic AI is used everywhere where the complexity of the underlying data is high and the details must not be ignored.
    <br>This distinguishes semantic AI from AI based on statistical methods (e.g. neural networks): statistical AI generalizes but details and traceability are lost. This is not bad for the classification of images - but not acceptable for the representation of processes or contracts."
    <br>[{2021-02-04} https://www.semafora-systems.com/]
    <br>"Historically, there have been two dominant paradigms of AI, namely symbolism and connectionism. Symbolism conjectures that symbols representing things in the world are the fundamental units of human intelligence, and that the cognitive process can be accomplished by the manipulation of the symbols, through a series of rules and logic operations upon the symbolic representations [2], [3]. Many early AI systems, from the middle 1950s to the late 1980s, were built upon symbolistic models. Symbolic methods have several virtues: they require only a few input samples, use powerful declarative languages for knowledge representation, and have conceptually straightforward internal functionality. It soon became apparent, however, that such a rule-based, top-down strategy demands substantial hand-tuning and lacks true learning. As discrete symbolic representations and hand-crafted rules are intolerant of ambiguous and noisy data, symbolic approaches typically fall short when solving real-world problems."
    <br>[{2023-03-29 retrieved} https://arxiv.org/pdf/2210.15889.pdf]
    <a class="clsHide" href="#idTechAiSmtcdsn"></a></p>
  <p id="idTechAiSmtcnam">name::
    <br>* McsEngl.semantic-AI,
    <br>* McsEngl.symbolic-AI,
    <br>* McsEngl.techAi.semantic,
    <br>* McsEngl.techAi.symbolism,
    <a class="clsHide" href="#idTechAiSmtcnam"></a></p>
</section>

<section id="idTechAiStcl">
  <h1 id="idTechAiStclH1">techAi.statistical
    <a class="clsHide" href="#idTechAiStclH1"></a></h1>
  <p id="idTechAiStcldsn">description::
    <br>"Statistical AI and classical AI are two different approaches to artificial intelligence (AI).Statistical AI, also known as machine learning, is a method of teaching computers to learn from data. It involves using statistical techniques to analyze large amounts of data and make predictions or decisions based on that data. This approach is often used in applications such as image recognition, natural language processing, and predictive analytics.Classical AI, on the other hand, is an approach that involves creating explicit rules and algorithms for a computer to follow. This approach is often used in applications such as expert systems and decision-making systems. It is based on symbolic reasoning and rule-based systems.While both approaches have their own advantages and limitations, they can also be combined to create more sophisticated AI systems."
    <br>[{2023-03-29 retrieved} https://www.quora.com/What-is-statistical-AI-and-classical-AI]
    <br>===
    <br>"From the earliest days, AI research has tended to fall into two largely separate strands: one focused on logical representations, and one focused on statistical ones. The first strand includes approacheslike logic programming, description logics, classical planning, symbolic parsing, rule induction, etc. The second includes approaches like Bayesian networks, hidden Markov models, Markov decision processes, statistical parsing, neural networks, etc. Logical approaches tend to emphasize handling complexity, and statistical ones uncertainty. Clearly, however, both of these are necessary to build intelligent agents and handle real-world applications"
    <br>[{2023-03-29 retrieved} https://homes.cs.washington.edu/~pedrod/papers/aaai06c.pdf]
    <br>===
    <br>"Connectionism, known by its most successful technique, deep neural networks (DNNs) [4], serves as the architecture behind the vast majority of recent successful AI systems. Inspired by the physiology of the nervous system, connectionism explains cognition by interconnected networks of simple and often uniform units. Learning happens as weight modification, in a data-driven manner; the network weights are adjusted in the direction that minimises the cumulative error from all the training samples, using techniques such as gradient back-propagation [5]. Connectionist models are fault-tolerant, as they learn sub-symbolics, i.e., continuous embedding vectors, and compare these vectorized representations instead of the literal meaning between entities and relations by discrete symbolic representations. Moreover, by learning statistical patterns from data, connectionist models enjoy the advantages of inductive learning and generalization capabilities. Like every coin has two sides, such approaches also suffer from several fundamental problems [6], [7]. First, connectionist models fall significantly short of compositional generalization, the robust ability of human cognition to correctly solve any problem that is composed of familiar parts [8]. Second, such bottom-up approaches are known to be data inefficient. Third, connectionist models are logically opaque, lacking comprehensibility. It is almost impossible to understand why decisions are made. In the absence of any kind of identifiable or verifiable train of logic, people are left with systems that are making potentially catastrophic decisions that are difficult to understand, arduous to correct, and therefore hard to be trusted. These shortcomings hinder the adoption of connectionist systems in decision-critical applications and reasoning-heavy tasks, such as medical diagnosis, autonomous driving, and mathematical reasoning, and lead to the increasing concern about contemporary AI techniques."
    <br>[{2023-03-29 retrieved} https://arxiv.org/pdf/2210.15889.pdf]
    <a class="clsHide" href="#idTechAiStcldsn"></a></p>
  <p id="idTechAiStclnam">name::
    <br>* McsEngl.connectionist-AI,
    <br>* McsEngl.statistical-AI,
    <br>* McsEngl.techAi.connectionism,
    <br>* McsEngl.techAi.statistical,
    <a class="clsHide" href="#idTechAiStclnam"></a></p>
</section>

<section id="idTechAiSas">
  <h1 id="idTechAiSasH1">techAi.semantic-and-statistical
    <a class="clsHide" href="#idTechAiSasH1"></a></h1>
  <p id="idTechAiSasdsn">description::
    <br>"By the 1950s, two visions for how to achieve machine intelligence emerged. One vision, known as Symbolic AI or GOFAI, was to use computers to create a symbolic representation of the world and systems that could reason about the world. Proponents included Allen Newell, Herbert A. Simon, and Marvin Minsky. Closely associated with this approach was the "heuristic search" approach, which likened intelligence to a problem of exploring a space of possibilities for answers.
    <br>The second vision, known as the connectionist approach, sought to achieve intelligence through learning. Proponents of this approach, most prominently Frank Rosenblatt, sought to connect Perceptron in ways inspired by connections of neurons.[21] <span class="clsColorRed">James Manyika and others have compared the two approaches to the mind (Symbolic AI) and the brain (connectionist)</span>. Manyika argues that symbolic approaches dominated the push for artificial intelligence in this period, due in part to its connection to intellectual traditions of Descartes, Boole, Gottlob Frege, Bertrand Russell, and others. Connectionist approaches based on cybernetics or artificial neural networks were pushed to the background but have gained new prominence in recent decades.[22]"
    <br>[{2023-04-10 retrieved} https://en.wikipedia.org/wiki/Artificial_intelligence#]
    <a class="clsHide" href="#idTechAiSasdsn"></a></p>
  <p id="idTechAiSasnam">name::
    <br>* McsEngl.NeSy-neural-symbolic-computing!⇒techAiSas,
    <br>* McsEngl.connectionist-and-symbolic-techAi!⇒techAiSas,
    <br>* McsEngl.neuro-symbolic-techAi!⇒techAiSas,
    <br>* McsEngl.semantic-and-statistical-techAi!⇒techAiSas,
    <br>* McsEngl.symbolic-and-connectionist-techAi!⇒techAiSas,
    <br>* McsEngl.techAiSas,
    <br>* McsEngl.techAi.semantic-and-statistical!⇒techAiSas,
    <a class="clsHide" href="#idTechAiSasnam"></a></p>
</section>

<section id="idTechAiCnpt">
  <h1 id="idTechAiCnptH1">techAi.conceptual (<a class="clsPreview" href="McsTchInf000007.last.html#idLagCnptmgr">link</a>)
    <a class="clsHide" href="#idTechAiCnptH1"></a></h1>
</section>

<section id="idTechAiFrdl">
  <h1 id="idTechAiFrdlH1">techAi.friendly
    <a class="clsHide" href="#idTechAiFrdlH1"></a></h1>
  <p id="idTechAiFrdldsn">description::
    <br>"Friendly artificial intelligence (also friendly AI or FAI) refers to hypothetical artificial general intelligence (AGI) that would have a positive (benign) effect on humanity or at least align with human interests or contribute to fostering the improvement of the human species. It is a part of the ethics of artificial intelligence and is closely related to machine ethics. While machine ethics is concerned with how an artificially intelligent agent should behave, friendly artificial intelligence research is focused on how to practically bring about this behavior and ensuring it is adequately constrained."
    <br>[{2023-04-10 retrieved} https://en.wikipedia.org/wiki/Friendly_artificial_intelligence]
    <a class="clsHide" href="#idTechAiFrdldsn"></a></p>
  <p id="idTechAiFrdlnam">name::
    <br>* McsEngl.FAI-friendly-AI,
    <br>* McsEngl.friendly-AI,
    <br>* McsEngl.techAi.friendly,
    <a class="clsHide" href="#idTechAiFrdlnam"></a></p>
</section>

<section id="idTechAiGnrv">
  <h1 id="idTechAiGnrvH1">techAi.generative
    <a class="clsHide" href="#idTechAiGnrvH1"></a></h1>
  <p id="idTechAiGnrvdsn">description::
    <br>"Generative AI refers to a type of artificial intelligence (AI) that is capable of generating new and original content, such as images, music, videos, and text. This is achieved through the use of deep learning algorithms and neural networks, which are trained on large datasets to learn the patterns and structure of the input data.
    <br>One popular approach to generative AI is through the use of generative adversarial networks (GANs), which consist of two neural networks that work together to generate new content. One network, called the generator, creates new samples that are similar to the training data, while the other network, called the discriminator, attempts to distinguish between the generated samples and the real ones.
    <br>Generative AI has many applications, including in art, music, and fashion, as well as in fields such as natural language processing and computer vision. However, it also raises ethical concerns, such as the potential for misuse or the creation of fake content. Therefore, it is important to approach the development and use of generative AI with caution and responsibility."
    <br>[{2023-05-02 retrieved} https://chat.openai.com/?model=text-davinci-002-render]
    <br>
    <br>* generates new data that is similar to data it was trained on,
    <br>* understands distribution of data and how likely a given example is,
    <br>* predict next word in a sentence,
    <br>[{2023-08-01 retrieved} https://www.cloudskillsboost.google/course_sessions/4138507/video/384243]
    <a class="clsHide" href="#idTechAiGnrvdsn"></a></p>
  <p id="idTechAiGnrvnam">name::
    <br>* McsEngl.GenAI!⇒techAiGenerative,
    <br>* McsEngl.generative-AI/jénrativ/,
    <br>* McsEngl.techAi.generative,
    <br>* McsEngl.techAiGenerative,
    <br>* McsEngl.techAiGenerative:techDl,
    <br>* McsEngl.techDl.generative-AI,
    <br>====== langoGreek:
    <br>* McsElln.γενετική-τεχνητή-νοημοσύνης!η!=techAiGenerative,
    <br>* McsElln.παραγωγική-τεχνητή-νοημοσύνης!η!=techAiGenerative,
    <a class="clsHide" href="#idTechAiGnrvnam"></a></p>
  <p id="idTechAiGnrvgtr">generic-tree-of-techAiGenerative::
    <br>* deep-learning,
    <br>** machine-learning,
    <a class="clsHide" href="#idTechAiGnrvgtr"></a></p>
  <p id="idTechAiGnrvstr">specific-tree-of-techAiGenerative::
    <br>* generative-language-model,
    <br>* generative-image-model,
    <br>===
    <br>* large-language-model,
    <br>* text-to-text,
    <br>* text-to-audio,
    <br>* text-to-video,
    <br>* text-to-task,
    <a class="clsHide" href="#idTechAiGnrvstr"></a></p>
</section>

<section id="idTchNlp">
  <h1 id="idTchNlpH1">techAi.natural-language-processing
    <a class="clsHide" href="#idTchNlpH1"></a></h1>
  <p id="idTchNlpdsn">description::
    <br>"Natural Language Processing (NLP) comprises a set of techniques to work with documents written in a natural language to achieve many different objectives. They range from simple ones that any developer can implement, to extremely complex ones that require a lot of expertise."
    <br>[{2020-09-20} https://tomassetti.me/guide-natural-language-processing/]
    <a class="clsHide" href="#idTchNlpdsn"></a></p>
  <p id="idTchNlpnam">name::
    <br>* McsEngl.NLP!⇒techNlp,
    <br>* McsEngl.NLP=natural-language-processing!⇒techNlp,
    <br>* McsEngl.techAi.natural-language-processing!⇒techNlp,
    <br>* McsEngl.techInfo.006-natural-language-processing!⇒techNlp,
    <br>* McsEngl.techInfo.natural-language-processing!⇒techNlp,
    <br>* McsEngl.techNlp,
    <a class="clsHide" href="#idTchNlpnam"></a></p>

  <section id="idTchNlp001lgml">
  <h2 id="idTchNlp001lgmlH2">language-model of techNlp
    <a class="clsHide" href="#idTchNlp001lgmlH2"></a></h2>
  <p id="idTchNlp001lgmldsn">description::
    <br>"A language model is a probability distribution over sequences of words.[1] Given any sequence of words of length m, a language model assigns a probability P(w1,...,wm) to the whole sequence. Language models generate probabilities by training on text corpora in one or many languages. Given that languages can be used to express an infinite variety of valid sentences (the property of digital infinity), language modeling faces the problem of assigning non-zero probabilities to linguistically valid sequences that may never be encountered in the training data. Several modelling approaches have been designed to surmount this problem, such as applying the Markov assumption or using neural architectures such as recurrent neural networks or transformers.
    <br>Language models are useful for a variety of problems in computational linguistics; from initial applications in speech recognition[2] to ensure nonsensical (i.e. low-probability) word sequences are not predicted, to wider use in machine translation[3] (e.g. scoring candidate translations), natural language generation (generating more human-like text), part-of-speech tagging, parsing,[3] optical character recognition, handwriting recognition,[4] grammar induction,[5] information retrieval,[6][7] and other applications."
    <br>[{2023-03-31 retrieved} https://en.wikipedia.org/wiki/Language_model]
    <a class="clsHide" href="#idTchNlp001lgmldsn"></a></p>
  <p id="idTchNlp001lgmlnam">name::
    <br>* McsEngl.language-model-techNlp!⇒techNlplm,
    <br>* McsEngl.techNlp'language-model!⇒techNlplm,
    <br>* McsEngl.techNlplm,
    <br>* McsEngl.statistical-language-model!⇒techNlplm,
    <a class="clsHide" href="#idTchNlp001lgmlnam"></a></p>

  <section id="idTchNlp001lgmlevg">
  <h3 id="idTchNlp001lgmlevgH3">evoluting of techNlplm
    <a class="clsHide" href="#idTchNlp001lgmlevgH3"></a></h3>
  <p id="idTchNlp001lgml2018">{2018}-techAi-LLM::
    <br>"Since 2018, large language models (LLMs) consisting of deep neural networks with billions of trainable parameters, trained on massive datasets of unlabelled text, have demonstrated impressive results on a wide variety of natural language processing tasks. This development has led to a shift in research focus toward the use of general-purpose LLMs."
    <br>[{2023-04-09 retrieved} https://en.wikipedia.org/wiki/Language_model]
    <br>* McsEngl.{2018}-techAi-LLM,
    <br>* McsEngl.{science'2018}-techAi-LLM,
    <a class="clsHide" href="#idTchNlp001lgml2018"></a></p>
  </section>

  <section id="idTchNlp001lgmlspc">
  <h3 id="idTchNlp001lgmlspcH3">techNlplm.SPECIFIC
    <a class="clsHide" href="#idTchNlp001lgmlspcH3"></a></h3>
  <p id="idTchNlp001lgmlspcdsn">description::
    <br>* neural-language-model,
    <br>** large-neural-language-model,
    <br>** recurent-neural-language-model,
    <br>** feedforward-neural-language-model,
    <br>** transformer-neural-language-model,
    <br>* Markov-(n-gram)-language-model,
    <a class="clsHide" href="#idTchNlp001lgmlspcdsn"></a></p>
  <p id="idTchNlp001lgmlspcnam">name::
    <br>* McsEngl.techNlplm.specific,
    <a class="clsHide" href="#idTchNlp001lgmlspcnam"></a></p>
  </section>

  <section id="idTchNlp001lgmlNrl">
  <h3 id="idTchNlp001lgmlNrlH3">techNlplm.neural-language-model
    <a class="clsHide" href="#idTchNlp001lgmlNrlH3"></a></h3>
  <p id="idTchNlp001lgmlNrldsn">description::
    <br>"Neural language models (or continuous space language models) use continuous representations or embeddings of words to make their predictions.[10] These models make use of neural networks.
    <br>Continuous space embeddings help to alleviate the curse of dimensionality in language modeling: as language models are trained on larger and larger texts, the number of unique words (the vocabulary) increases.[a] The number of possible sequences of words increases exponentially with the size of the vocabulary, causing a data sparsity problem because of the exponentially many sequences. Thus, statistics are needed to properly estimate probabilities. Neural networks avoid this problem by representing words in a distributed way, as non-linear combinations of weights in a neural net.[11] An alternate description is that a neural net approximates the language function. The neural net architecture might be feed-forward or recurrent, and while the former is simpler the latter is more common."
    <br>[{2023-04-09 retrieved} https://en.wikipedia.org/wiki/Language_model#Neural_network]
    <a class="clsHide" href="#idTchNlp001lgmlNrldsn"></a></p>
  <p id="idTchNlp001lgmlNrlnam">name::
    <br>* McsEngl.NLM-neural-language-model!⇒techNlm,
    <br>* McsEngl.neural-language-model!⇒techNlm,
    <br>* McsEngl.techNlm,
    <br>* McsEngl.techNlplm.neural-language-model!⇒techNlm,
    <a class="clsHide" href="#idTchNlp001lgmlNrlnam"></a></p>
  </section>

  <section id="idTchLlm">
  <h3 id="idTchLlmH3">techNlm.large-language-model (<a class="clsPreview" href="McsTchInf000038.last.html#idOverview">link</a>)
    <a class="clsHide" href="#idTchLlmH3"></a></h3>
  </section>

  <section id="idTchNlprtai">
  <h2 id="idTchNlprtaiH2">relation-to-techAi of techNlp
    <a class="clsHide" href="#idTchNlprtaiH2"></a></h2>
  <p id="idTchNlprtaidsn">description::
    <br>· "AI (Artificial Intelligence) and NLP (Natural Language Processing) are related but distinct fields in the realm of computer science and technology. Let's explore the differences between the two:
    <br>AI (Artificial Intelligence):
    <br>AI is a broad field that focuses on creating machines or systems that can perform tasks that typically require human intelligence. The goal of AI is to develop algorithms and models that enable computers to exhibit characteristics like learning, reasoning, problem-solving, perception, and decision-making. AI can be applied to various domains, including computer vision, robotics, speech recognition, game playing, and more.
    <br>AI encompasses several subfields, such as machine learning, deep learning, expert systems, knowledge representation, and natural language processing. NLP is one of these subfields that deals specifically with enabling computers to understand, interpret, and generate human language.
    <br>NLP (Natural Language Processing):
    <br>NLP is a subset of AI that focuses on the interaction between computers and human language. Its primary aim is to enable computers to understand, interpret, and generate natural language in a way that is meaningful to humans. NLP seeks to bridge the gap between human language and computer language, enabling machines to comprehend and respond to textual or spoken language.
    <br>Key tasks in NLP include:
    <br>* Natural Language Understanding (NLU): Extracting meaning and insights from human language.
    <br>* Natural Language Generation (NLG): Creating human-readable text or speech from structured data or information.
    <br>* Sentiment Analysis: Determining the sentiment or emotion behind a piece of text.
    <br>* Named Entity Recognition (NER): Identifying and classifying entities like names of people, places, and organizations in text.
    <br>* Machine Translation: Automatically translating text from one language to another.
    <br>* Speech Recognition: Converting spoken language into written text.
    <br>AI and NLP often intersect, as NLP techniques frequently rely on AI methods such as machine learning and deep learning to achieve their goals. For example, many state-of-the-art NLP models, such as transformers, are built using deep learning techniques like attention mechanisms.
    <br>In summary, AI is a broader field that encompasses NLP as one of its subfields. While AI deals with the creation of intelligent systems, NLP is specifically concerned with enabling computers to understand and generate human language. Both AI and NLP play crucial roles in developing technology that can interact with humans more naturally and intelligently."
    <br>[{2023-07-31 retrieved} https://chat.openai.com/?model=text-davinci-002-render-sha]
    <a class="clsHide" href="#idTchNlprtaidsn"></a></p>
  <p id="idTchNlprtainam">name::
    <br>* McsEngl.techAi'relation-to-techNlp,
    <br>* McsEngl.techNlp'relation-to-techAi,
    <a class="clsHide" href="#idTchNlprtainam"></a></p>
  </section>

  <section id="idTchNlp001">
  <h2 id="idTchNlp001H2">techNlp.SPECIFIC
    <a class="clsHide" href="#idTchNlp001H2"></a></h2>
  <p id="idTchNlp001dsn">description::
    <br>* natural-language-understanding,
    <br>* natural-language-generation,
    <br>
    <br>* finding similar documents,
    <br>* finding words with the same meaning,
    <br>* generating a summary of a text,
    <br>* generating realistic names,
    <br>* grouping similar words,
    <br>* handwriting recognition,
    <br>* identifying entities,
    <br>* <a class="clsPreview" href="#idTchNlp001">identifying the language of a text</a>,
    <br>* <a class="clsPreview" href="#idTchInf008">machine-learning</a>,
    <br>* <a class="clsPreview" href="#idTchInf019">machine-translation</a>,
    <br>* named-entity-recognition-(NER),
    <br>* optical character recognition,
    <br>* parsing,
    <br>* part-of-speech tagging,
    <br>* question answering,
    <br>* sentiment analysis,
    <br>* speech-to-text,
    <br>* text-classification,
    <br>* text-to-speech,
    <br>* <a class="clsPreview" href="#idTchInf008">translating a text</a>,
    <br>* understanding how much time it takes to read a text,
    <br>* understanding how difficult to read is a text,
    <br>* understanding the attitude expressed in a text,
    <a class="clsHide" href="#idTchNlp001dsn"></a></p>
  <p id="idTchNlp001nam">name::
    <br>* McsEngl.techNlp.specific,
    <a class="clsHide" href="#idTchNlp001nam"></a></p>
  </section>

  <section id="idTchNlp006">
  <h2 id="idTchNlp006H2">techNlp.identifying-entities
    <a class="clsHide" href="#idTchNlp006H2"></a></h2>
  <p id="idTchNlp006dsn">description::
    <br>· "Identifying and classifying names of concepts in text."
    <a class="clsHide" href="#idTchNlp006dsn"></a></p>
  <p id="idTchNlp006nam">name::
    <br>* McsEngl.NER-named-entity-recognition,
    <br>* McsEngl.named-entity-recognition,
    <br>* McsEngl.techNlp.006-identifying-entities,
    <br>* McsEngl.techNlp.identifying-entities,
    <a class="clsHide" href="#idTchNlp006nam"></a></p>
  </section>

  <section id="idTchNlp005">
  <h2 id="idTchNlp005H2">techNlp.language-understanding
    <a class="clsHide" href="#idTchNlp005H2"></a></h2>
  <p id="idTchNlp005dsn">description::
    <br>· "Natural-language understanding (NLU) or natural-language interpretation (NLI)[1] is a subtopic of natural-language processing in artificial intelligence that deals with machine reading comprehension. Natural-language understanding is considered an AI-hard problem.[2]
    <br>There is considerable commercial interest in the field because of its application to automated reasoning,[3] machine translation,[4] question answering,[5] news-gathering, text categorization, voice-activation, archiving, and large-scale content analysis."
    <br>[{2023-07-31 retrieved} https://en.wikipedia.org/wiki/Natural-language_understanding]
    <a class="clsHide" href="#idTchNlp005dsn"></a></p>
  <p id="idTchNlp005nam">name::
    <br>* McsEngl.NLU-natural-language-understanding,
    <br>* McsEngl.natural-language-understanding,
    <br>* McsEngl.techNlp.005-language-understanding,
    <br>* McsEngl.techNlp.language-understanding,
    <a class="clsHide" href="#idTchNlp005nam"></a></p>
  </section>

  <section id="idTchNlp001">
  <h2 id="idTchNlp001H2">techNlp.language-recognition
    <a class="clsHide" href="#idTchNlp001H2"></a></h2>
  <p id="idTchNlp001dsn">description::
    <br>· Identifying the language of a text.
    <a class="clsHide" href="#idTchNlp001dsn"></a></p>
  <p id="idTchNlp001nam">name::
    <br>* McsEngl.identifying-language-of-text,
    <br>* McsEngl.techNlp.001-language-recognition,
    <br>* McsEngl.techNlp.language-recognition,
    <a class="clsHide" href="#idTchNlp001nam"></a></p>
  <p id="idTchNlp001wpa">addressWpg::
    <br>* https://en.wikipedia.org/wiki/Wikipedia:Language_recognition_chart,
    <a class="clsHide" href="#idTchNlp001wpa"></a></p>
  </section>

  <section id="idTchNlp007">
  <h2 id="idTchNlp007H2">techNlp.question-answering
    <a class="clsHide" href="#idTchNlp007H2"></a></h2>
  <p id="idTchNlp007dsn">description::
    <br>· "question answering is a subfield of NLP that deals with the task of automatically answering questions posed in natural language."
    <br>[{2023-08-01 retrieved} https://www.cloudskillsboost.google/course_sessions/4141808/video/379143]
    <a class="clsHide" href="#idTchNlp007dsn"></a></p>
  <p id="idTchNlp007nam">name::
    <br>* McsEngl.QA=question-answering-system!⇒techQa,
    <br>* McsEngl.question-answering-system!⇒techQa,
    <br>* McsEngl.techNlp.question-answering!⇒techQa,
    <br>* McsEngl.techQa,
    <a class="clsHide" href="#idTchNlp007nam"></a></p>
  </section>

  <section id="idTchNlp003">
  <h2 id="idTchNlp003H2">techNlp.speech-recognition
    <a class="clsHide" href="#idTchNlp003H2"></a></h2>
  <p id="idTchNlp003dsn">description::
    <br>"Speech recognition is an interdisciplinary subfield of computer science and computational linguistics that develops methodologies and technologies that enable the recognition and translation of spoken language into text by computers with the main benefit of searchability. It is also known as automatic speech recognition (ASR), computer speech recognition or speech to text (STT). It incorporates knowledge and research in the computer science, linguistics and computer engineering fields. The reverse process is speech synthesis.
    <br>Some speech recognition systems require "training" (also called "enrollment") where an individual speaker reads text or isolated vocabulary into the system. The system analyzes the person's specific voice and uses it to fine-tune the recognition of that person's speech, resulting in increased accuracy. Systems that do not use training are called "speaker-independent"[1] systems. Systems that use training are called "speaker dependent".
    <br>Speech recognition applications include voice user interfaces such as voice dialing (e.g. "call home"), call routing (e.g. "I would like to make a collect call"), domotic appliance control, search key words (e.g. find a podcast where particular words were spoken), simple data entry (e.g., entering a credit card number), preparation of structured documents (e.g. a radiology report), determining speaker characteristics,[2] speech-to-text processing (e.g., word processors or emails), and aircraft (usually termed direct voice input).
    <br>The term voice recognition[3][4][5] or speaker identification[6][7][8] refers to identifying the speaker, rather than what they are saying. Recognizing the speaker can simplify the task of translating speech in systems that have been trained on a specific person's voice or it can be used to authenticate or verify the identity of a speaker as part of a security process.
    <br>From the technology perspective, speech recognition has a long history with several waves of major innovations. Most recently, the field has benefited from advances in deep learning and big data. The advances are evidenced not only by the surge of academic papers published in the field, but more importantly by the worldwide industry adoption of a variety of deep learning methods in designing and deploying speech recognition systems."
    <br>[{2023-04-02 retrieved} https://en.wikipedia.org/wiki/Speech_recognition]
    <a class="clsHide" href="#idTchNlp003dsn"></a></p>
  <p id="idTchNlp003nam">name::
    <br>* McsEngl.ASR-automatic-speech-recognition!⇒techSprc,
    <br>* McsEngl.STT-speech-to-text!⇒techSprc,
    <br>* McsEngl.automatic-speech-recognition!⇒techSprc,
    <br>* McsEngl.computer-speech-recognition!⇒techSprc,
    <br>* McsEngl.techNlp.003-speech-recognition!⇒techSprc,
    <br>* McsEngl.techNlp.speech-recognition!⇒techSprc,
    <br>* McsEngl.techSprc,
    <br>* McsEngl.speech-recognition!⇒techSprc,
    <br>* McsEngl.speech-to-text!⇒techSprc,
    <a class="clsHide" href="#idTchNlp003nam"></a></p>
  </section>

  <section id="idTchNlp004">
  <h2 id="idTchNlp004H2">techNlp.speech-synthesis
    <a class="clsHide" href="#idTchNlp004H2"></a></h2>
  <p id="idTchNlp004dsn">description::
    <br>"Speech synthesis is the artificial production of human speech. A computer system used for this purpose is called a speech synthesizer, and can be implemented in software or hardware products. A text-to-speech (TTS) system converts normal language text into speech; other systems render symbolic linguistic representations like phonetic transcriptions into speech.[1] The reverse process is speech recognition."
    <br>[{2023-04-02 retrieved} https://en.wikipedia.org/wiki/Speech_synthesis]
    <a class="clsHide" href="#idTchNlp004dsn"></a></p>
  <p id="idTchNlp004nam">name::
    <br>* McsEngl.TTS-text-to-speech!⇒techSpsn,
    <br>* McsEngl.techNlp.004-speech-synthesis!⇒techSpsn,
    <br>* McsEngl.techNlp.speech-synthesis!⇒techSpsn,
    <br>* McsEngl.techSpsn,
    <br>* McsEngl.text-to-speech!⇒techSpsn,
    <br>* McsEngl.speech-synthesis!⇒techSpsn,
    <a class="clsHide" href="#idTchNlp004nam"></a></p>
  </section>
</section>

<section id="idTchInf022">
  <h1 id="idTchInf022H1">techAi.machine-translation
    <a class="clsHide" href="#idTchInf022H1"></a></h1>
  <p id="idTchInf022dsn">description::
    <br>"Machine translation, sometimes referred to by the abbreviation MT[1] (not to be confused with computer-aided translation, machine-aided human translation or interactive translation), is a sub-field of computational linguistics that investigates the use of software to translate text or speech from one language to another."
    <br>[{2021-02-06} https://en.wikipedia.org/wiki/Machine_translation]
    <a class="clsHide" href="#idTchInf022dsn"></a></p>
  <p id="idTchInf022nam">name::
    <br>* McsEngl.machine-translation-tech!⇒techTrln,
    <br>* McsEngl.techAi.machine-translation!⇒techTrln,
    <br>* McsEngl.techInfo.022-machine-translation!⇒techTrln,
    <br>* McsEngl.techInfo.machine-translation!⇒techTrln,
    <br>* McsEngl.techTrln,
    <br>* McsEngl.techTrln!=machine-translation,
    <br>* McsEngl.techNlp.002-machine-translation!⇒techTrln,
    <br>* McsEngl.techNlp.machine-translation!⇒techTrln,
    <br>* McsEngl.translation.machine!⇒techTrln,
    <a class="clsHide" href="#idTchInf022nam"></a></p>
</section>

<section id="idTchInf008">
  <h1 id="idTchInf008H1">techAi.machine-learning
    <a class="clsHide" href="#idTchInf008H1"></a></h1>
  <p id="idTchInf008dsn">description::
    <br>"We call machines programmed to learn from examples “neural networks.” "
    <br>[{2023-07-30 retrieved} https://blog.google/inside-google/googlers/ask-a-techspert/what-is-generative-ai/]
    <br>===
    <br>"A machine learning algorithm is an algorithm that is able to learn from data.But what do we mean by learning? Mitchell (1997) provides a succinct deﬁnition:“A computer program is said to learn from experienceEwith respect to someclass of tasksTand performance measureP, if its performance at tasks inT, asmeasured byP, improves with experienceE.” "
    <br>[{2022-12-06 retrieved} https://www.deeplearningbook.org/contents/ml.html]
    <br>"A system is said to learn if it is capable of acquiring new knowledge from its environment.
    <br>Learning may also enable the ability to perform new tasks without having to be redesigned or reprogrammed, especially when accompanied by generalization.
    <br>Learning is most readily accomplished in a system that supports symbolic abstraction, though such a property is not exclusive (reinforcement strategies, for example, do not necessarily require symbolic representation).
    <br>This type of learning is separated from the acquisition of knowledge through direct programming by the designer, which is referred to throughout this document as the Ability to Add New Knowledge."
    [{1998-02-16} http://krusty.eecs.umich.edu/cogarch4/toc_defs/defs_capa/defs_lear.html]
    <a class="clsHide" href="#idTchInf008dsn"></a></p>
  <p id="idTchInf008nam">name::
    <br>* McsEngl.ML!=Machine-Learning,
    <br>* McsEngl.Machine-Learning!⇒techMl,
    <br>* McsEngl.techInfo.008-Machine-Learning!⇒techMl,
    <br>* McsEngl.techInfo.Machine-Learning!⇒techMl,
    <br>* McsEngl.techAi.techMl!⇒techMl,
    <br>* McsEngl.techNlp.techMl!⇒techMl,
    <br>* McsEngl.techMl,
    <br>* McsEngl.techMachine-learning!⇒techMl,
    <a class="clsHide" href="#idTchInf008nam"></a></p>
  <p id="idTchInf008dsnL">descriptionLong::
    <br>"Machine learning (ML) is the study of computer algorithms that improve automatically through experience.[1][2] It is seen as a subset of artificial intelligence. Machine learning algorithms build a mathematical model based on sample data, known as "training data", in order to make predictions or decisions without being explicitly programmed to do so.[3] Machine learning algorithms are used in a wide variety of applications, such as email filtering and computer vision, where it is difficult or infeasible to develop conventional algorithms to perform the needed tasks.
    <br>Machine learning is closely related to computational statistics, which focuses on making predictions using computers. The study of mathematical optimization delivers methods, theory and application domains to the field of machine learning. Data mining is a related field of study, focusing on exploratory data analysis through unsupervised learning.[5][6] In its application across business problems, machine learning is also referred to as predictive analytics."
    <br>[{2020-09-22} https://en.wikipedia.org/wiki/Machine_learning]
    <a class="clsHide" href="#idTchInf008dsnL"></a></p>

  <section id="idTchInf008stcd">
  <h2 id="idTchInf008stcdH2">Softcode of techMl
    <a class="clsHide" href="#idTchInf008stcdH2"></a></h2>
  <p id="idTchInf008stcddsn">description::
    <br>·
    <a class="clsHide" href="#idTchInf008stcddsn"></a></p>
  <p id="idTchInf008stcdnam">name::
    <br>* McsEngl.Mlsoftcode,
    <br>* McsEngl.techMl'Softcode!⇒Mlsoftcode,
    <a class="clsHide" href="#idTchInf008stcdnam"></a></p>

  <section id="idTchInf008stcdPtrc">
  <h3 id="idTchInf008stcdPtrcH3">Mlsoftcode.PyTorch
    <a class="clsHide" href="#idTchInf008stcdPtrcH3"></a></h3>
  <p id="idTchInf008stcdPtrcdsn">description::
    <br>· "PyTorch is a machine learning framework based on the Torch library,[4][5][6] used for applications such as computer vision and natural language processing,[7] originally developed by Meta AI and now part of the Linux Foundation umbrella.[8][9][10][11] It is free and open-source software released under the modified BSD license. Although the Python interface is more polished and the primary focus of development, PyTorch also has a C++ interface.[12]
    <br>A number of pieces of deep learning software are built on top of PyTorch, including Tesla Autopilot,[13] Uber's Pyro,[14] Hugging Face's Transformers,[15] PyTorch Lightning,[16][17] and Catalyst.[18][19]
    <br>PyTorch provides two high-level features:[20]
    <br>* Tensor computing (like NumPy) with strong acceleration via graphics processing units (GPU)
    <br>* Deep neural networks built on a tape-based automatic differentiation system"
    <br>[{2023-08-15 retrieved} https://en.wikipedia.org/wiki/PyTorch]
    <a class="clsHide" href="#idTchInf008stcdPtrcdsn"></a></p>
  <p id="idTchInf008stcdPtrcnam">name::
    <br>* McsEngl.Mlsoftcode.PyTorch,
    <br>* McsEngl.PyTorch-Mlsoftcode,
    <a class="clsHide" href="#idTchInf008stcdPtrcnam"></a></p>
  </section>
  </section>

  <section id="idTchInf008engnr">
  <h2 id="idTchInf008engnrH2">engineer of techMl
    <a class="clsHide" href="#idTchInf008engnrH2"></a></h2>
  <p id="idTchInf008engnrdsn">description::
    <br>"A Machine Learning Engineer creates, edits, analyzes, debugs, models, and supervises the development of machine learning models using programming languages such as Python or C++ and machine learning libraries such as Keras or TensorFlow.
    <br>... The average annual income of a Machine Learning Engineer in the United States is between $112,000 and $157,000 with a median of $131,000 per year according to multiple data sources such as Indeed, Glassdoor, Salary.com, and Payscale."
    <br>[{2023-03-29 retrieved} https://blog.finxter.com/machine-learning-engineer-income-and-opportunity/]
    <a class="clsHide" href="#idTchInf008engnrdsn"></a></p>
  <p id="idTchInf008engnrnam">name::
    <br>* McsEngl.techMl'engineer,
    <a class="clsHide" href="#idTchInf008engnrnam"></a></p>
  </section>

  <section id="idTchInf008ozn">
  <h2 id="idTchInf008oznH2">organization of techMl
    <a class="clsHide" href="#idTchInf008oznH2"></a></h2>
  <p id="idTchInf008ozndsn">description::
    <br>* <a class="clsPreview" href="https://huggingface.co/">Hugging-Face</a>,
    <a class="clsHide" href="#idTchInf008ozndsn"></a></p>
  <p id="idTchInf008oznnam">name::
    <br>* McsEngl.techMl'organization,
    <a class="clsHide" href="#idTchInf008oznnam"></a></p>
  </section>

  <section id="idTchInf008rscF">
  <h2 id="idTchInf008rscFH2">info-resource of techMl
    <a class="clsHide" href="#idTchInf008rscFH2"></a></h2>
  <p id="idTchInf008rscFdsn">description::
    <br>* https://huggingface.co/docs/hub/index,
    <br>* https://github.com/dair-ai/ML-YouTube-Courses,
    <br>* https://machinelearningmastery.com/,
    <br>* https://www.deeplearning.ai/,
    <br>
    <br>* Practical Deep Learning for Coders: https://course.fast.ai/Lessons/lesson1.html,
    <a class="clsHide" href="#idTchInf008rscFdsn"></a></p>
  <p id="idTchInf008rscFnam">name::
    <br>* McsEngl.techMl'Infrsc,
    <a class="clsHide" href="#idTchInf008rscFnam"></a></p>
  </section>

  <section id="idTchInf008Spc">
  <h2 id="idTchInf008SpcH2">techMl.SPECIFIC
    <a class="clsHide" href="#idTchInf008SpcH2"></a></h2>
  <p id="idTchInf008Spcdsn">description::
    <br>* generative-AI,
    <br>* deep-learning,
    <br>* Supervised Learning (e.g., Linear Regression, Support Vector Machines)
    <br>* Unsupervised Learning (e.g., Clustering, Dimensionality Reduction)
    <br>* Reinforcement Learning (e.g., Q-Learning, Policy Gradient)
    <a class="clsHide" href="#idTchInf008Spcdsn"></a></p>
  <p id="idTchInf008Spcnam">name::
    <br>* McsEngl.techMl.specific,
    <a class="clsHide" href="#idTchInf008Spcnam"></a></p>
  </section>

  <section id="idTchInf008Dplr">
  <h2 id="idTchInf008DplrH2">techMl.deep-learning
    <a class="clsHide" href="#idTchInf008DplrH2"></a></h2>
  <p id="idTchInf008Dplrdsn">description::
    <br>"Deep learning is part of a broader family of machine learning methods based on artificial neural networks with representation learning. Learning can be supervised, semi-supervised or unsupervised.[2]
    <br>Deep-learning architectures such as deep neural networks, deep belief networks, deep reinforcement learning, recurrent neural networks, convolutional neural networks and transformers have been applied to fields including computer vision, speech recognition, natural language processing, machine translation, bioinformatics, drug design, medical image analysis, climate science, material inspection and board game programs, where they have produced results comparable to and in some cases surpassing human expert performance.[3][4][5]
    <br>Artificial neural networks (ANNs) were inspired by information processing and distributed communication nodes in biological systems. ANNs have various differences from biological brains. Specifically, artificial neural networks tend to be static and symbolic, while the biological brain of most living organisms is dynamic (plastic) and analog.[6][7]
    <br>The adjective "deep" in deep learning refers to the use of multiple layers in the network. Early work showed that a linear perceptron cannot be a universal classifier, but that a network with a nonpolynomial activation function with one hidden layer of unbounded width can. Deep learning is a modern variation that is concerned with an unbounded number of layers of bounded size, which permits practical application and optimized implementation, while retaining theoretical universality under mild conditions. In deep learning the layers are also permitted to be heterogeneous and to deviate widely from biologically informed connectionist models, for the sake of efficiency, trainability and understandability."
    <br>[{2023-04-01 retrieved} https://en.wikipedia.org/wiki/Deep_learning#]
    <a class="clsHide" href="#idTchInf008Dplrdsn"></a></p>
  <p id="idTchInf008Dplrnam">name::
    <br>* McsEngl.DL-deep-learning!⇒techDl,
    <br>* McsEngl.deep-learning!⇒techDl,
    <br>* McsEngl.techDl,
    <br>* McsEngl.techDl-deep-learning!⇒techDl,
    <a class="clsHide" href="#idTchInf008Dplrnam"></a></p>

  <section id="idTchInf008DplrSpc">
  <h3 id="idTchInf008DplrSpcH3">techDl.SPECIFIC
    <a class="clsHide" href="#idTchInf008DplrSpcH3"></a></h3>
  <p id="idTchInf008DplrSpcdsn">description::
    <br>* discriminative,
    <br>* <a class="clsPreview" href="#idTechAiGnrv">generative</a>,
    <br>===
    <br>* <a class="clsPreview" href="#idTchNnDeep">deep neural networks</a>,
    <br>* deep belief networks,
    <br>* deep reinforcement learning,
    <br>* <a class="clsPreview" href="#idTchNnRtnn">recurrent neural networks</a>,
    <br>* <a class="clsPreview" href="#idTchNnCvnl">convolutional neural networks</a>,
    <br>* <a class="clsPreview" href="McsTchInf000038.last.html#idTchLlmTrfm">transformers</a>,
    <a class="clsHide" href="#idTchInf008DplrSpcdsn"></a></p>
  <p id="idTchInf008DplrSpcnam">name::
    <br>* McsEngl.techDl.specific,
    <a class="clsHide" href="#idTchInf008DplrSpcnam"></a></p>
  </section>

  <section id="idTchInf008DplrDrmv">
  <h3 id="idTchInf008DplrDrmvH3">techDl.discriminative
    <a class="clsHide" href="#idTchInf008DplrDrmvH3"></a></h3>
  <p id="idTchInf008DplrDrmvdsn">description::
    <br>* used to classify or predict,
    <br>* typically trained on a dataset of labeled data,
    <br>* learns the relationship between the features of the data points and the labels.
    <br>[{2023-08-01 retrieved} https://www.cloudskillsboost.google/course_sessions/4138507/video/384243]
    <a class="clsHide" href="#idTchInf008DplrDrmvdsn"></a></p>
  <p id="idTchInf008DplrDrmvnam">name::
    <br>* McsEngl.techDl.discriminative,
    <a class="clsHide" href="#idTchInf008DplrDrmvnam"></a></p>
  </section>
  </section>

  <section id="idTchInf008Spvd">
  <h2 id="idTchInf008SpvdH2">techMl.supervised-learning
    <a class="clsHide" href="#idTchInf008SpvdH2"></a></h2>
  <p id="idTchInf008Spvddsn">description::
    <br>"Supervised learning (SL) is a machine learning paradigm for problems where the available data consists of labeled examples, meaning that each data point contains features (covariates) and an associated label. The goal of supervised learning algorithms is learning a function that maps feature vectors (inputs) to labels (output), based on example input-output pairs.[1] It infers a function from labeled training data consisting of a set of training examples.[2] In supervised learning, each example is a pair consisting of an input object (typically a vector) and a desired output value (also called the supervisory signal). A supervised learning algorithm analyzes the training data and produces an inferred function, which can be used for mapping new examples. An optimal scenario will allow for the algorithm to correctly determine the class labels for unseen instances. This requires the learning algorithm to generalize from the training data to unseen situations in a "reasonable" way (see inductive bias). This statistical quality of an algorithm is measured through the so-called generalization error."
    <br>[{2023-04-04 retrieved} https://en.wikipedia.org/wiki/Supervised_learning]
    <a class="clsHide" href="#idTchInf008Spvddsn"></a></p>
  <p id="idTchInf008Spvdnam">name::
    <br>* McsEngl.techMl.supervised-learning,
    <br>* McsEngl.supervised-learning,
    <a class="clsHide" href="#idTchInf008Spvdnam"></a></p>
  </section>

  <section id="idTchInf008Uspd">
  <h2 id="idTchInf008UspdH2">techMl.unsupervised-learning
    <a class="clsHide" href="#idTchInf008UspdH2"></a></h2>
  <p id="idTchInf008Uspddsn">description::
    <br>"Unsupervised learning is a type of algorithm that learns patterns from untagged data. The goal is that through mimicry, which is an important mode of learning in people, the machine is forced to build a concise representation of its world and then generate imaginative content from it.
    <br>In contrast to supervised learning where data is tagged by an expert, e.g. tagged as a "ball" or "fish", unsupervised methods exhibit self-organization that captures patterns as probability densities[1] or a combination of neural feature preferences encoded in the machine's weights and activations. The other levels in the supervision spectrum are reinforcement learning where the machine is given only a numerical performance score as guidance, and semi-supervised learning where a small portion of the data is tagged."
    <br>[{2023-04-04 retrieved} https://en.wikipedia.org/wiki/Unsupervised_learning#]
    <a class="clsHide" href="#idTchInf008Uspddsn"></a></p>
  <p id="idTchInf008Uspdnam">name::
    <br>* McsEngl.techMl.unsupervised-learning,
    <br>* McsEngl.unsupervised-learning,
    <a class="clsHide" href="#idTchInf008Uspdnam"></a></p>
  </section>

  <section id="idTchInf008Sspd">
  <h2 id="idTchInf008SspdH2">techMl.semisupervised-learning
    <a class="clsHide" href="#idTchInf008SspdH2"></a></h2>
  <p id="idTchInf008Sspddsn">description::
    <br>"Weak supervision, also called semi-supervised learning, is a branch of machine learning that combines a small amount of labeled data with a large amount of unlabeled data during training. Semi-supervised learning falls between unsupervised learning (with no labeled training data) and supervised learning (with only labeled training data). Semi-supervised learning aims to alleviate the issue of having limited amounts of labeled data available for training.
    <br>Semi-supervised learning is motivated by problem settings where unlabeled data is abundant and obtaining labeled data is expensive. Other branches of machine learning that share the same motivation but follow different assumptions and methodologies are active learning and weak supervision. Unlabeled data, when used in conjunction with a small amount of labeled data, can produce considerable improvement in learning accuracy. The acquisition of labeled data for a learning problem often requires a skilled human agent (e.g. to transcribe an audio segment) or a physical experiment (e.g. determining the 3D structure of a protein or determining whether there is oil at a particular location). The cost associated with the labeling process thus may render large, fully labeled training sets infeasible, whereas acquisition of unlabeled data is relatively inexpensive. In such situations, semi-supervised learning can be of great practical value. Semi-supervised learning is also of theoretical interest in machine learning and as a model for human learning."
    <br>[{2023-04-04 retrieved} https://en.wikipedia.org/wiki/Weak_supervision#Semi-supervised_learning]
    <a class="clsHide" href="#idTchInf008Sspddsn"></a></p>
  <p id="idTchInf008Sspdnam">name::
    <br>* McsEngl.semisupervised-learning,
    <br>* McsEngl.techMl.semisupervised-learning,
    <br>* McsEngl.weak-semisupervised-learning,
    <a class="clsHide" href="#idTchInf008Sspdnam"></a></p>
  </section>

  <section id="idTchInf008Rfmt">
  <h2 id="idTchInf008RfmtH2">techMl.reinforcement-learning
    <a class="clsHide" href="#idTchInf008RfmtH2"></a></h2>
  <p id="idTchInf008Rfmtdsn">description::
    <br>"Reinforcement learning (RL) is an area of machine learning concerned with how intelligent agents ought to take actions in an environment in order to maximize the notion of cumulative reward. Reinforcement learning is one of three basic machine learning paradigms, alongside supervised learning and unsupervised learning.
    <br>Reinforcement learning differs from supervised learning in not needing labelled input/output pairs to be presented, and in not needing sub-optimal actions to be explicitly corrected. Instead the focus is on finding a balance between exploration (of uncharted territory) and exploitation (of current knowledge).[1]
    <br>The environment is typically stated in the form of a Markov decision process (MDP), because many reinforcement learning algorithms for this context use dynamic programming techniques.[2] The main difference between the classical dynamic programming methods and reinforcement learning algorithms is that the latter do not assume knowledge of an exact mathematical model of the MDP and they target large MDPs where exact methods become infeasible."
    <br>[{2023-04-04 retrieved} https://en.wikipedia.org/wiki/Reinforcement_learning]
    <br>===
    <br>"Reinforcement learning (RL) is learning by interacting with an environment. An RL agent learns from the consequences of its actions, rather than from being explicitly taught and it selects its actions on basis of its past experiences (exploitation) and also by new choices (exploration), which is essentially trial and error learning. The reinforcement signal that the RL-agent receives is a numerical reward, which encodes the success of an action's outcome, and the agent seeks to learn to select actions that maximize the accumulated reward over time. (The use of the term reward is used here in a neutral fashion and does not imply any pleasure, hedonic impact or other psychological interpretations.)"
    <br>[{2023-04-05 retrieved} http://www.scholarpedia.org/article/Reinforcement_learning]
    <a class="clsHide" href="#idTchInf008Rfmtdsn"></a></p>
  <p id="idTchInf008Rfmtnam">name::
    <br>* McsEngl.reinforecment-learning,
    <br>* McsEngl.techMl.reinforecment-learning,
    <a class="clsHide" href="#idTchInf008Rfmtnam"></a></p>
  </section>
</section>

<section id="idTchInf016">
  <h1 id="idTchInf016H1">techAi.machine-reasoning
    <a class="clsHide" href="#idTchInf016H1"></a></h1>
  <p id="idTchInf016dsn">description::
    <br>"In computer science, in particular in knowledge representation and reasoning and metalogic, the area of automated reasoning is dedicated to understanding different aspects of reasoning. The study of automated reasoning helps produce computer programs that allow computers to reason completely, or nearly completely, automatically. Although automated reasoning is considered a sub-field of artificial intelligence, it also has connections with theoretical computer science and philosophy.
    <br>The most developed subareas of automated reasoning are automated theorem proving (and the less automated but more pragmatic subfield of interactive theorem proving) and automated proof checking (viewed as guaranteed correct reasoning under fixed assumptions).[citation needed] Extensive work has also been done in reasoning by analogy using induction and abduction.[1]
    <br>Other important topics include reasoning under uncertainty and non-monotonic reasoning. An important part of the uncertainty field is that of argumentation, where further constraints of minimality and consistency are applied on top of the more standard automated deduction. John Pollock's OSCAR system[2] is an example of an automated argumentation system that is more specific than being just an automated theorem prover.
    <br>Tools and techniques of automated reasoning include the classical logics and calculi, fuzzy logic, Bayesian inference, reasoning with maximal entropy and many less formal ad hoc techniques."
    <br>[{2023-04-03 retrieved} https://en.wikipedia.org/wiki/Automated_reasoning]
    <a class="clsHide" href="#idTchInf016dsn"></a></p>
  <p id="idTchInf016nam">name::
    <br>* McsEngl.automated-reasoning!⇒techMr,
    <br>* McsEngl.machine-reasoning!⇒techMr,
    <br>* McsEngl.techInfo.016-machine-reasoning!⇒techMr,
    <br>* McsEngl.techMr,
    <br>====== langoGreek:
    <br>* McsElln.μηχανικός-συλλογισμός,
    <a class="clsHide" href="#idTchInf016nam"></a></p>
</section>

<section id="idTchInf010">
  <h1 id="idTchInf010H1">techAi.computer-vision
    <a class="clsHide" href="#idTchInf010H1"></a></h1>
  <p id="idTchInf010dsn">description::
    <br>"Computer vision tasks include methods for acquiring, processing, analyzing and understanding digital images, and extraction of high-dimensional data from the real world in order to produce numerical or symbolic information, e.g. in the forms of decisions.[1][2][3][4] Understanding in this context means the transformation of visual images (the input of the retina) into descriptions of the world that make sense to thought processes and can elicit appropriate action. This image understanding can be seen as the disentangling of symbolic information from image data using models constructed with the aid of geometry, physics, statistics, and learning theory.
    <br>The scientific discipline of computer vision is concerned with the theory behind artificial systems that extract information from images. The image data can take many forms, such as video sequences, views from multiple cameras, multi-dimensional data from a 3D scanner, or medical scanning devices. The technological discipline of computer vision seeks to apply its theories and models to the construction of computer vision systems.
    <br>Sub-domains of computer vision include scene reconstruction, object detection, event detection, video tracking, object recognition, 3D pose estimation, learning, indexing, motion estimation, visual servoing, 3D scene modeling, image generation, and image restoration.
    <br>Adopting computer vision technology might be painstaking for organizations as there is no single point solution for it. There are very few companies that provide a unified and distributed platform or an Operating System where computer vision applications can be easily deployed and managed."
    <br>[{2023-03-31 retrieved} https://en.wikipedia.org/wiki/Computer_vision#]
    <a class="clsHide" href="#idTchInf010dsn"></a></p>
  <p id="idTchInf010nam">name::
    <br>* McsEngl.computer-vision!⇒techCmrv,
    <br>* McsEngl.techInfo.010-computer-vision!⇒techCmrv,
    <br>* McsEngl.techInfo.computer-vision!⇒techCmrv,
    <a class="clsHide" href="#idTchInf010nam"></a></p>
</section>

<section id="idTchNn">
  <h1 id="idTchNnH1">techAi.Artificial-Neural-Network
    <a class="clsHide" href="#idTchNnH1"></a></h1>
  <p id="idTchNndsn">description::
    <br>"Artificial neural networks (ANNs), usually simply called neural networks (NNs) or neural nets,[1] are computing systems inspired by the biological neural networks that constitute animal brains.[2]
    <br>An ANN is based on a collection of connected units or nodes called artificial neurons, which loosely model the neurons in a biological brain. Each connection, like the synapses in a biological brain, can transmit a signal to other neurons. An artificial neuron receives signals then processes them and can signal neurons connected to it. The "signal" at a connection is a real number, and the output of each neuron is computed by some non-linear function of the sum of its inputs. The connections are called edges. Neurons and edges typically have a weight that adjusts as learning proceeds. The weight increases or decreases the strength of the signal at a connection. Neurons may have a threshold such that a signal is sent only if the aggregate signal crosses that threshold.
    <br>Typically, neurons are aggregated into layers. Different layers may perform different transformations on their inputs. Signals travel from the first layer (the input layer), to the last layer (the output layer), possibly after traversing the layers multiple times."
    <br>[{2023-03-29 retrieved} https://en.wikipedia.org/wiki/Artificial_neural_network]
    <a class="clsHide" href="#idTchNndsn"></a></p>
  <p id="idTchNnnam">name::
    <br>* McsEngl.ANN-artificial-neural-network!⇒techNn,
    <br>* McsEngl.artificial-neural-network!⇒techNn,
    <br>* McsEngl.neural-network!⇒techNn,
    <br>* McsEngl.techInfo.009-Artificial-Neural-Network!⇒techNn,
    <br>* McsEngl.techAi.neural-network!⇒techNn,
    <br>* McsEngl.techNn,
    <br>====== langoGreek:
    <br>* McsElln.τεχνητό-νευρωνικό-δίκτυο!το!=techNn,
    <a class="clsHide" href="#idTchNnnam"></a></p>

  <section id="idTchNnnurn">
  <h2 id="idTchNnnurnH2">artificial-neuron of techNn
    <a class="clsHide" href="#idTchNnnurnH2"></a></h2>
  <p id="idTchNnnurndsn">description::
    <br>"An artificial neuron is a mathematical function conceived as a model of biological neurons, a neural network. Artificial neurons are elementary units in an artificial neural network.[1] The artificial neuron receives one or more inputs (representing excitatory postsynaptic potentials and inhibitory postsynaptic potentials at neural dendrites) and sums them to produce an output (or activation, representing a neuron's action potential which is transmitted along its axon). Usually each input is separately weighted, and the sum is passed through a non-linear function known as an activation function or transfer function[clarification needed]. The transfer functions usually have a sigmoid shape, but they may also take the form of other non-linear functions, piecewise linear functions, or step functions. They are also often monotonically increasing, continuous, differentiable and bounded. Non-monotonic, unbounded and oscillating activation functions with multiple zeros that outperform sigmoidal and ReLU like activation functions on many tasks have also been recently explored. The thresholding function has inspired building logic gates referred to as threshold logic; applicable to building logic circuits resembling brain processing. For example, new devices such as memristors have been extensively used to develop such logic in recent times.[2]
    <br>The artificial neuron transfer function should not be confused with a linear system's transfer function.
    <br>Artificial neurons can also refer to artificial cells in neuromorphic engineering (see below) that are similar to natural physical neurons."
    <br>[{2023-04-05 retrieved} https://en.wikipedia.org/wiki/Artificial_neuron]
    <br>· syntheticNo-neuron is <a class="clsPreview" href="../dirNtr/McsNtr000010.last.html#idBrnBioatt002NtrN">a-naturalNo-neuronBio</a> that DOES NOT look like a-natural-neuron.
    <a class="clsHide" href="#idTchNnnurndsn"></a></p>
  <p id="idTchNnnurnnam">name::
    <br>* McsEngl.artificial-neuron,
    <br>* McsEngl.neuronBio.syntheticNo,
    <br>* McsEngl.techNn'neuron,
    <a class="clsHide" href="#idTchNnnurnnam"></a></p>
  </section>

  <section id="idTchNnappl">
  <h2 id="idTchNnapplH2">application of techNn
    <a class="clsHide" href="#idTchNnapplH2"></a></h2>
  <p id="idTchNnappldsn">description::
    <br>· "Neural networks have been used on a variety of tasks, including computer vision, speech recognition, machine translation, social network filtering, playing board and video games and medical diagnosis."
    <br>[{2023-07-31 retrieved} https://en.wikipedia.org/wiki/Deep_learning#Neural_networks]
    <a class="clsHide" href="#idTchNnappldsn"></a></p>
  <p id="idTchNnapplnam">name::
    <br>* McsEngl.techNn'application,
    <a class="clsHide" href="#idTchNnapplnam"></a></p>
  </section>

  <section id="idTchNnlbr">
  <h2 id="idTchNnlbrH2">library of techNn
    <a class="clsHide" href="#idTchNnlbrH2"></a></h2>
  <p id="idTchNnlbrdsn">description::
    <br>·
    <a class="clsHide" href="#idTchNnlbrdsn"></a></p>
  <p id="idTchNnlbrnam">name::
    <br>* McsEngl.techNn'framework,
    <br>* McsEngl.techNn'library,
    <a class="clsHide" href="#idTchNnlbrnam"></a></p>
  </section>

  <section id="idTchNneval">
  <h2 id="idTchNnevalH2">evaluation of techNn
    <a class="clsHide" href="#idTchNnevalH2"></a></h2>
  <p id="idTchNnevaldsn">description::
    <br>·
    <a class="clsHide" href="#idTchNnevaldsn"></a></p>
  <p id="idTchNnevalnam">name::
    <br>* McsEngl.techNn'evaluation,
    <a class="clsHide" href="#idTchNnevalnam"></a></p>
  </section>

  <section id="idTchNnrsc">
  <h2 id="idTchNnrscH2">info-resource of techNn
    <a class="clsHide" href="#idTchNnrscH2"></a></h2>
  <p id="idTchNnrscdsn">description::
    <br>* https://en.wikipedia.org/wiki/Types_of_artificial_neural_networks,
    <a class="clsHide" href="#idTchNnrscdsn"></a></p>
  <p id="idTchNnrscnam">name::
    <br>* McsEngl.techNn'Infrsc,
    <a class="clsHide" href="#idTchNnrscnam"></a></p>
  </section>

  <section id="idTchNnSpc">
  <h2 id="idTchNnSpcH2">techNn.SPECIFIC
    <a class="clsHide" href="#idTchNnSpcH2"></a></h2>
  <p id="idTchNnSpcdsn">description::
    <br>* convolutional-neural-network,
    <br>* <a class="clsPreview" href="#idTchNnFdfd">feedforward-neural-network</a>,
    <br>* <a class="clsPreview" href="#idTchNnLstm">long-short-term-memory-neural-network</a>,
    <br>* <a class="clsPreview" href="#idTchNnRtnn">recurrent-neural-network</a>,
    <br>* recursive-neural-network,
    <br>* <a class="clsPreview" href="McsTchInf000038.last.html#idTchLlmTrfm">transformer-neural-network</a>,
    <a class="clsHide" href="#idTchNnSpcdsn"></a></p>
  <p id="idTchNnSpcnam">name::
    <br>* McsEngl.techNn.specific,
    <a class="clsHide" href="#idTchNnSpcnam"></a></p>
  </section>

  <section id="idTchNnCvnl">
  <h2 id="idTchNnCvnlH2">techNn.convolutional
    <a class="clsHide" href="#idTchNnCvnlH2"></a></h2>
  <p id="idTchNnCvnldsn">description::
    <br>· "In deep learning, a convolutional neural network (CNN) is a class of artificial neural network most commonly applied to analyze visual imagery.[1] CNNs use a mathematical operation called convolution in place of general matrix multiplication in at least one of their layers.[2] They are specifically designed to process pixel data and are used in image recognition and processing. They have applications in:
    <br>* image and video recognition,
    <br>* recommender systems,[3]
    <br>* image classification,
    <br>* image segmentation,
    <br>* medical image analysis,
    <br>* natural language processing,[4]
    <br>* brain–computer interfaces,[5] and
    <br>* financial time series.[6]
    <br>CNNs are also known as Shift Invariant or Space Invariant Artificial Neural Networks (SIANN), based on the shared-weight architecture of the convolution kernels or filters that slide along input features and provide translation-equivariant responses known as feature maps.[7][8] Counter-intuitively, most convolutional neural networks are not invariant to translation, due to the downsampling operation they apply to the input.[9]"
    <br>[{2023-07-31 retrieved} https://en.wikipedia.org/wiki/Convolutional_neural_network]
    <a class="clsHide" href="#idTchNnCvnldsn"></a></p>
  <p id="idTchNnCvnlnam">name::
    <br>* McsEngl.CNN-convolutional-neural-network!⇒techNnCv,
    <br>* McsEngl.convolutional-neural-network!⇒techNnCv,
    <br>* McsEngl.techDl.convolutional-neural-network!⇒techNnCv,
    <br>* McsEngl.techNn.convolutional!⇒techNnCv,
    <br>* McsEngl.techNnCv,
    <a class="clsHide" href="#idTchNnCvnlnam"></a></p>
  </section>

  <section id="idTchNnFdfd">
  <h2 id="idTchNnFdfdH2">techNn.feedforward
    <a class="clsHide" href="#idTchNnFdfdH2"></a></h2>
  <p id="idTchNnFdfddsn">description::
    <br>"A feedforward neural network (FNN) is an artificial neural network wherein connections between the nodes do not form a cycle.[1] As such, it is different from its descendant: recurrent neural networks.
    <br>The feedforward neural network was the first and simplest type of artificial neural network devised.[2] In this network, the information moves in only one direction—forward—from the input nodes, through the hidden nodes (if any) and to the output nodes. There are no cycles or loops in the network."
    <br>[{2023-04-05 retrieved} https://en.wikipedia.org/wiki/Feedforward_neural_network]
    <a class="clsHide" href="#idTchNnFdfddsn"></a></p>
  <p id="idTchNnFdfdnam">name::
    <br>* McsEngl.FNN-feedforward-neural-network,
    <br>* McsEngl.feedforward-neural-network,
    <br>* McsEngl.techNn.feedforward,
    <br>* McsEngl.techNnFf,
    <a class="clsHide" href="#idTchNnFdfdnam"></a></p>
  </section>

  <section id="idTchNnDeep">
  <h2 id="idTchNnDeepH2">techNn.deep
    <a class="clsHide" href="#idTchNnDeepH2"></a></h2>
  <p id="idTchNnDeepdsn">description::
    <br>· "A deep neural network (DNN) is an artificial neural network (ANN) with multiple layers between the input and output layers.[10][13] There are different types of neural networks but they always consist of the same components: neurons, synapses, weights, biases, and functions.[137] These components as a whole function similarly to a human brain, and can be trained like any other ML algorithm.[citation needed]
    <br>For example, a DNN that is trained to recognize dog breeds will go over the given image and calculate the probability that the dog in the image is a certain breed. The user can review the results and select which probabilities the network should display (above a certain threshold, etc.) and return the proposed label. Each mathematical manipulation as such is considered a layer,[citation needed] and complex DNN have many layers, hence the name "deep" networks.
    <br>DNNs can model complex non-linear relationships. DNN architectures generate compositional models where the object is expressed as a layered composition of primitives.[138] The extra layers enable composition of features from lower layers, potentially modeling complex data with fewer units than a similarly performing shallow network.[10] For instance, it was proved that sparse multivariate polynomials are exponentially easier to approximate with DNNs than with shallow networks.[139]
    <br>Deep architectures include many variants of a few basic approaches. Each architecture has found success in specific domains. It is not always possible to compare the performance of multiple architectures, unless they have been evaluated on the same data sets.
    <br>DNNs are typically feedforward networks in which data flows from the input layer to the output layer without looping back. At first, the DNN creates a map of virtual neurons and assigns random numerical values, or "weights", to connections between them. The weights and inputs are multiplied and return an output between 0 and 1. If the network did not accurately recognize a particular pattern, an algorithm would adjust the weights.[140] That way the algorithm can make certain parameters more influential, until it determines the correct mathematical manipulation to fully process the data.
    <br>Recurrent neural networks (RNNs), in which data can flow in any direction, are used for applications such as language modeling.[141][142][143][144][145] Long short-term memory is particularly effective for this use.[75][146]
    <br>Convolutional deep neural networks (CNNs) are used in computer vision.[147] CNNs also have been applied to acoustic modeling for automatic speech recognition (ASR).[148]"
    <br>[{2023-07-31 retrieved} https://en.wikipedia.org/wiki/Deep_learning#Deep_neural_networks]
    <a class="clsHide" href="#idTchNnDeepdsn"></a></p>
  <p id="idTchNnDeepnam">name::
    <br>* McsEngl.DNN-deep-neural-network!⇒techNnD,
    <br>* McsEngl.deep-neural-network!⇒techNnD,
    <br>* McsEngl.techDl.deep-neural-net!⇒techNnD,
    <br>* McsEngl.techNn.deep!⇒techNnD,
    <br>* McsEngl.techNnD,
    <a class="clsHide" href="#idTchNnDeepnam"></a></p>
  </section>

  <section id="idTchNnDpbf">
  <h2 id="idTchNnDpbfH2">techNn.deep-belief
    <a class="clsHide" href="#idTchNnDpbfH2"></a></h2>
  <p id="idTchNnDpbfdsn">description::
    <br>· "In machine learning, a deep belief network (DBN) is a generative graphical model, or alternatively a class of deep neural network, composed of multiple layers of latent variables ("hidden units"), with connections between the layers but not between units within each layer.[1]
    <br>When trained on a set of examples without supervision, a DBN can learn to probabilistically reconstruct its inputs. The layers then act as feature detectors.[1] After this learning step, a DBN can be further trained with supervision to perform classification.[2]
    <br>DBNs can be viewed as a composition of simple, unsupervised networks such as restricted Boltzmann machines (RBMs)[1] or autoencoders,[3] where each sub-network's hidden layer serves as the visible layer for the next. An RBM is an undirected, generative energy-based model with a "visible" input layer and a hidden layer and connections between but not within layers. This composition leads to a fast, layer-by-layer unsupervised training procedure, where contrastive divergence is applied to each sub-network in turn, starting from the "lowest" pair of layers (the lowest visible layer is a training set).
    <br>The observation[2] that DBNs can be trained greedily, one layer at a time, led to one of the first effective deep learning algorithms.[4]: 6  Overall, there are many attractive implementations and uses of DBNs in real-life applications and scenarios (e.g., electroencephalography,[5] drug discovery[6][7][8])."
    <br>[{2023-08-01 retrieved} https://en.wikipedia.org/wiki/Deep_belief_network]
    <a class="clsHide" href="#idTchNnDpbfdsn"></a></p>
  <p id="idTchNnDpbfnam">name::
    <br>* McsEngl.DBN=deep-belief-network!⇒techNnDb,
    <br>* McsEngl.deep-belief-network!⇒techNnDb,
    <br>* McsEngl.techDl.deep-belief-net!⇒techNnDb,
    <br>* McsEngl.techNn.deep-belief!⇒techNnDb,
    <br>* McsEngl.techNnDb,
    <a class="clsHide" href="#idTchNnDpbfnam"></a></p>
  </section>

  <section id="idTchNnLgmd">
  <h2 id="idTchNnLgmdH2">techNn.language-model (<a class="clsPreview" href="#idTchNlp001lgmlNrl">link</a>)
    <a class="clsHide" href="#idTchNnLgmdH2"></a></h2>
  </section>

  <section id="idTchNnLstm">
  <h2 id="idTchNnLstmH2">techNn.long-short-term-memory
    <a class="clsHide" href="#idTchNnLstmH2"></a></h2>
  <p id="idTchNnLstmdsn">description::
    <br>"Long short-term memory (LSTM)[1] is an artificial neural network used in the fields of artificial intelligence and deep learning. Unlike standard feedforward neural networks, LSTM has feedback connections. Such a recurrent neural network (RNN) can process not only single data points (such as images), but also entire sequences of data (such as speech or video). This characteristic makes LSTM networks ideal for processing and predicting data. For example, LSTM is applicable to tasks such as unsegmented, connected handwriting recognition,[2] speech recognition,[3][4] machine translation,[5][6] speech activity detection,[7] robot control,[8][9] video games,[10][11] and healthcare.[12]
    <br>The name of LSTM refers to the analogy that a standard RNN has both "long-term memory" and "short-term memory". The connection weights and biases in the network change once per episode of training, analogous to how physiological changes in synaptic strengths store long-term memories; the activation patterns in the network change once per time-step, analogous to how the moment-to-moment change in electric firing patterns in the brain store short-term memories.[13] The LSTM architecture aims to provide a short-term memory for RNN that can last thousands of timesteps, thus "long short-term memory".[1]
    <br>A common LSTM unit is composed of a cell, an input gate, an output gate[14] and a forget gate.[15] The cell remembers values over arbitrary time intervals and the three gates regulate the flow of information into and out of the cell. Forget gates decide what information to discard from a previous state by assigning a previous state, compared to a current input, a value between 0 and 1. A (rounded) value of 1 means to keep the information, and a value of 0 means to discard it. Input gates decide which pieces of new information to store in the current state, using the same system as forget gates. Output gates control which pieces of information in the current state to output by assigning a value from 0 to 1 to the information, considering the previous and current states. Selectively outputting relevant information from the current state allows the LSTM network to maintain useful, long-term dependencies to make predictions, both in current and future time-steps.
    <br>LSTM networks are well-suited to classifying, processing and making predictions based on time series data, since there can be lags of unknown duration between important events in a time series. LSTMs were developed to deal with the vanishing gradient problem[16] that can be encountered when training traditional RNNs. Relative insensitivity to gap length is an advantage of LSTM over RNNs, hidden Markov models and other sequence learning methods in numerous applications."
    <br>[{2023-04-02 retrieved} https://en.wikipedia.org/wiki/Long_short-term_memory]
    <a class="clsHide" href="#idTchNnLstmdsn"></a></p>
  <p id="idTchNnLstmnam">name::
    <br>* McsEngl.LSTM-long-short-term-memory!⇒techLstm,
    <br>* McsEngl.long-short-term-memory!⇒techLstm,
    <br>* McsEngl.techLstm,
    <br>* McsEngl.techNn.long-short-term-memory!⇒techLstm,
    <a class="clsHide" href="#idTchNnLstmnam"></a></p>
  </section>

  <section id="idTchNnMlp">
  <h2 id="idTchNnMlpH2">techNn.multilayer-perceptron
    <a class="clsHide" href="#idTchNnMlpH2"></a></h2>
  <p id="idTchNnMlpdsn">description::
    <br>· "Multilayer Perceptron: In the context of machine learning, an MLP is a type of artificial neural network consisting of multiple layers of interconnected nodes (neurons). Each node in one layer is connected to every node in the subsequent layer. MLPs are commonly used for tasks such as classification and regression."
    <br>[{2023-08-09 retrieved} https://chat.openai.com/?model=text-davinci-002-render-sha]
    <a class="clsHide" href="#idTchNnMlpdsn"></a></p>
  <p id="idTchNnMlpnam">name::
    <br>* McsEngl.MLP=multilayer-perceptron,
    <br>* McsEngl.multilayer-perceptron-techNn,
    <br>* McsEngl.techNn.multilayer-perceptron,
    <a class="clsHide" href="#idTchNnMlpnam"></a></p>
  </section>

  <section id="idTchNnRtnn">
  <h2 id="idTchNnRtnnH2">techNn.recurrent
    <a class="clsHide" href="#idTchNnRtnnH2"></a></h2>
  <p id="idTchNnRtnndsn">description::
    <br>"A recurrent neural network (RNN) is a class of artificial neural networks where connections between nodes can create a cycle, allowing output from some nodes to affect subsequent input to the same nodes. This allows it to exhibit temporal dynamic behavior. Derived from feedforward neural networks, RNNs can use their internal state (memory) to process variable length sequences of inputs.[1][2][3] This makes them applicable to tasks such as unsegmented, connected handwriting recognition[4] or speech recognition.[5][6] Recurrent neural networks are theoretically Turing complete and can run arbitrary programs to process arbitrary sequences of inputs.[7]
    <br>The term "recurrent neural network" is used to refer to the class of networks with an infinite impulse response, whereas "convolutional neural network" refers to the class of finite impulse response. Both classes of networks exhibit temporal dynamic behavior.[8] A finite impulse recurrent network is a directed acyclic graph that can be unrolled and replaced with a strictly feedforward neural network, while an infinite impulse recurrent network is a directed cyclic graph that can not be unrolled.
    <br>Both finite impulse and infinite impulse recurrent networks can have additional stored states, and the storage can be under direct control by the neural network. The storage can also be replaced by another network or graph if that incorporates time delays or has feedback loops. Such controlled states are referred to as gated state or gated memory, and are part of long short-term memory networks (LSTMs) and gated recurrent units. This is also called Feedback Neural Network (FNN)."
    <br>[{2023-04-02 retrieved} https://en.wikipedia.org/wiki/Recurrent_neural_network]
    <a class="clsHide" href="#idTchNnRtnndsn"></a></p>
  <p id="idTchNnRtnnnam">name::
    <br>* McsEngl.RNN-recurrent-neural-network!⇒techNnRt,
    <br>* McsEngl.recurrent-neural-network!⇒techNnRt,
    <br>* McsEngl.techDl.recurrent-neural-net!⇒techNnRt,
    <br>* McsEngl.techNn.recurrent!⇒techNnRt,
    <br>* McsEngl.techNnFf;techNnRt,
    <br>* McsEngl.techNnRt,
    <br>* McsEngl.techNnRt;;techNnFf,
    <a class="clsHide" href="#idTchNnRtnnnam"></a></p>
  </section>

  <section id="idTchNnRcsv">
  <h2 id="idTchNnRcsvH2">techNn.recursive
    <a class="clsHide" href="#idTchNnRcsvH2"></a></h2>
  <p id="idTchNnRcsvdsn">description::
    <br>· "A recursive neural network is a kind of deep neural network created by applying the same set of weights recursively over a structured input, to produce a structured prediction over variable-size input structures, or a scalar prediction on it, by traversing a given structure in topological order. Recursive neural networks, sometimes abbreviated as RvNNs, have been successful, for instance, in learning sequence and tree structures in natural language processing, mainly phrase and sentence continuous representations based on word embedding. RvNNs have first been introduced to learn distributed representations of structure, such as logical terms.[1] Models and general frameworks have been developed in further works since the 1990s.[2][3]"
    <br>[{2023-07-31 retrieved} https://en.wikipedia.org/wiki/Recursive_neural_network]
    <a class="clsHide" href="#idTchNnRcsvdsn"></a></p>
  <p id="idTchNnRcsvnam">name::
    <br>* McsEngl.RNN-recursive-neural-network!⇒techNnRv,
    <br>* McsEngl.recursive-neural-network!⇒techNnRv,
    <br>* McsEngl.techNn.recursive!⇒techNnRv,
    <a class="clsHide" href="#idTchNnRcsvnam"></a></p>
  </section>

  <section id="idTchInf011">
  <h2 id="idTchInf011H2">techNn.transformer (<a class="clsPreview" href="McsTchInf000038.last.html#idTchLlmTrfm">link</a>)
    <a class="clsHide" href="#idTchInf011H2"></a></h2>
  </section>

  <section id="idTchNnQntm">
  <h2 id="idTchNnQntmH2">techNn.quantum
    <a class="clsHide" href="#idTchNnQntmH2"></a></h2>
  <p id="idTchNnQntmdsn">description::
    <br>· "Quantum neural networks are computational neural network models which are based on the principles of quantum mechanics. The first ideas on quantum neural computation were published independently in 1995 by Subhash Kak and Ron Chrisley,[1][2] engaging with the theory of quantum mind, which posits that quantum effects play a role in cognitive function. However, typical research in quantum neural networks involves combining classical artificial neural network models (which are widely used in machine learning for the important task of pattern recognition) with the advantages of quantum information in order to develop more efficient algorithms.[3][4][5] One important motivation for these investigations is the difficulty to train classical neural networks, especially in big data applications. The hope is that features of quantum computing such as quantum parallelism or the effects of interference and entanglement can be used as resources. Since the technological implementation of a quantum computer is still in a premature stage, such quantum neural network models are mostly theoretical proposals that await their full implementation in physical experiments.
    <br>Most Quantum neural networks are developed as feed-forward networks. Similar to their classical counterparts, this structure intakes input from one layer of qubits, and passes that input onto another layer of qubits. This layer of qubits evaluates this information and passes on the output to the next layer. Eventually the path leads to the final layer of qubits.[6][7] The layers do not have to be of the same width, meaning they don't have to have the same number of qubits as the layer before or after it. This structure is trained on which path to take similar to classical artificial neural networks. This is discussed in a lower section. Quantum neural networks refer to three different categories: Quantum computer with classical data, classical computer with quantum data, and quantum computer with quantum data.[6]"
    <br>[{2023-07-31 retrieved} https://en.wikipedia.org/wiki/Quantum_neural_network]
    <a class="clsHide" href="#idTchNnQntmdsn"></a></p>
  <p id="idTchNnQntmnam">name::
    <br>* McsEngl.quantum-neural-network!⇒techNnQ,
    <br>* McsEngl.techNn.quantum!⇒techNnQ,
    <br>* McsEngl.techNnQ,
    <a class="clsHide" href="#idTchNnQntmnam"></a></p>
  </section>
</section>

<section id="idMeta">
  <h1 id="idMetaH1">meta-info
    <a class="clsHide" href="#idMetaH1"></a></h1>
  <p id="idMetaCounter" class="clsCenter">this page was-visited
    <span class="clsColorRed">
    <script src="../../dirPgm/dirCntr/counter.php?page=McsTchInf000036"></script>
    </span>
    times since {2023-07-30}</p>
  <!-- the content of page-path paragraph is displayed as it is on top of toc -->
  <p id="idMetaWebpage_path"><span class="clsB clsColorGreen">page-wholepath</span>:
    <a class="clsPreview" href="../../#idOverview">synagonism.net</a> /
    <a class="clsPreview" href="../Mcs000000.last.html#idOverview">worldviewSngo</a> /
    <a class="clsPreview" href="McsTchInf000000.last.html#idOverview">dirTchInf</a> /
    techAi
    </p>
  <p id="idMetaP1">SEARCH::
    <br>· this page uses '<span class="clsColorRed">locator-names</span>', names that when you find them, you find the-LOCATION of the-concept they denote.
    <br>⊛ <strong>GLOBAL-SEARCH</strong>:
    <br>· clicking on <span class="clsColorGreenBg">the-green-BAR of a-page</span> you have access to the-global--locator-names of my-site.
    <br>· use the-prefix '<span class="clsColorRed">techAi</span>' for <a class="clsPreview" href="../dirCor/McsCor000002.last.html#idOverview">senso-concepts</a> related to current concept 'artificial-intelligence'.
    <br>⊛ <strong>LOCAL-SEARCH</strong>:
    <br>· TYPE <span class="clsColorRed">CTRL+F "McsLang.words-of-concept's-name"</span>, to go to the-LOCATION of the-concept.
    <br>· a-preview of the-description of a-global-name makes reading fast.
    <a class="clsHide" href="#idMetaP1"></a></p>
  <p id="idFooterP1">footer::
    <br>• author: <a class="clsPreview" href="../dirHmn/McsHmn000003.last.html#idOverview">Kaseluris.Nikos.1959</a>
    <br>• email:
    <br> &nbsp;<img src="../../dirRsc/dirImg/mail.png">
    <br>• edit on github: https://github.com/synagonism/McsWorld/blob/master/dirMcs/dirTchInf/McsTchInf000036.last.html,
    <br>• comments on <a class="clsPreview" href="McsTchInf000000.last.html#idComment">Disqus</a>,
    <br>• twitter: <a href="https://twitter.com/synagonism">@synagonism</a>,
    <a class="clsHide" href="#idFooterP1"></a></p>
  <p id="idMetaVersion">webpage-versions::
    <br>• version.last.dynamic: <a lass="clsPreview" href="McsTchInf000036.last.html">McsTchInf000036.last.html</a>,
    <br>• version.draft.creation: McsTchInf000036.0-1-0.2023-07-30.last.html,
    <a class="clsHide" href="#idMetaVersion"></a></p>
</section>

<section id="idSupport">
  <h1 id="idSupportH1">support (<a class="clsPreview" href="../../#idSupport">link</a>)</h1>
  <p></p>
</section>

<script type="module">
  import * as omMcsh from '../Mcsmgr/mMcsh.js'
</script>
<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-19285371-5"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());
  gtag('config', 'UA-19285371-5');
</script>
<!--  -->
</body>
</html>